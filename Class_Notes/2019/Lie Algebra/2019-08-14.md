---
title: "Lie Algebras"
---

# Lecture 1

> The material for this class will roughly come from Humphrey, Chapters 1 to 5. There is also a useful appendix which has been uploaded to the ELC system online.

## Overview
Here is a short overview of the topics we expect to cover:

### Chapter 2
- Ideals, solvability, and nilpotency
- Semisimple Lie algebras
  - These have a particularly nice structure and representation theory
- Determining if a Lie algebra is semisimple using Killing forms
- Weyl's theorem for complete reducibility for finite dimensional representations
- Root space decompositions

### Chapter 3-4
We will describe the following series of correspondences:

\begin{tikzcd}
\text{Semisimple algebras} \arrow[rr, Leftrightarrow] &  & \text{Root systems} \arrow[rr, Leftrightarrow] & & \text{Dynkin diagrams} \\
& & & & \\
\text{Simple algebras over } \CC \arrow[uu, Rightarrow, "\bigoplus"] \arrow[rr, Leftrightarrow] & & \text{Irreducible root systems} \arrow[uu, Rightarrow, "\coprod"] \arrow[rr, Leftrightarrow] & & \arrow[uu, Rightarrow, "\coprod"] \text{Connected Dynkin diagrams}
\end{tikzcd}

## Classification
The classical Lie algebras can be essentially classified by certain classes of diagrams:

\begin{tikzpicture}
\tikzset{vertex/.style = {shape=circle,draw,minimum size=0.2em,fill=black!60}}
\tikzset{edge/.style = {->,> = latex'}}
\node at (-1,0) {$A_\ell:$};
\node[vertex] (a) at  (0,0) {};
\node[vertex] (b) at  (2,0) {};
\node[vertex] (c) at  (4,0) {};
\node[label=$\ell$, vertex] (d) at  (6,0) {};
\draw[edge] (a) to (b);
\draw[edge] (c) to (d);
\node at ($(b)!0.5!(c)$) {$\cdots$};
\end{tikzpicture}

\begin{tikzpicture}
\tikzset{vertex/.style = {shape=circle,draw,minimum size=0.2em,fill=black!60}}
\tikzset{edge/.style = {->,> = latex'}}
\node at (-1,0) {$B_\ell:$};
\node[vertex] (a) at  (0,0) {};
\node[vertex] (b) at  (2,0) {};
\node[vertex] (c) at  (4,0) {};
\node[label=$\ell$, vertex] (d) at  (6,0) {};
\draw[edge] (a) to (b);
\draw[edge, bend left] (c) to (d);
\draw[edge, bend right] (c) to (d);
\node at ($(b)!0.5!(c)$) {$\cdots$};
\end{tikzpicture}

\begin{tikzpicture}
\tikzset{vertex/.style = {shape=circle,draw,minimum size=0.2em,fill=black!60}}
\tikzset{edge/.style = {->,> = latex'}}
\node at (-1,0) {$C_\ell:$};
\node[vertex] (a) at  (0,0) {};
\node[vertex] (b) at  (2,0) {};
\node[vertex] (c) at  (4,0) {};
\node[label=$\ell$, vertex] (d) at  (6,0) {};
\draw[edge] (a) to (b);
\draw[edge, bend left] (d) to (c);
\draw[edge, bend right] (d) to (c);
\node at ($(b)!0.5!(c)$) {$\cdots$};
\end{tikzpicture}

\begin{tikzpicture}
\tikzset{vertex/.style = {shape=circle,draw,minimum size=0.2em,fill=black!60}}
\tikzset{edge/.style = {->,> = latex'}}
\node at (-1,0) {$D_\ell:$};
\node[vertex] (a) at  (0,0) {};
\node[vertex] (b) at  (2,0) {};
\node[vertex] (c) at  (4,0) {};
\node[label=$\ell$, vertex] (d) at  (6,2) {};
\node[vertex] (e) at  (6,-2) {};
\draw[edge] (a) to (b);
\draw[edge] (c) to (d);
\draw[edge] (c) to (e);
\node at ($(b)!0.5!(c)$) {$\cdots$};
\end{tikzpicture}

\begin{tikzpicture}
\tikzset{vertex/.style = {shape=circle,draw,minimum size=0.2em,fill=black!60}}
\tikzset{edge/.style = {->,> = latex'}}
\node at (-2,0) {$E_6, E_7, E_8:$};
\node[vertex] (a) at  (0,0) {};
\node[vertex] (b) at  (2,0) {};
\node[vertex] (c) at  (4,0) {};
\node[vertex] (cp) at  (4,2) {};
\node[vertex] (d) at  (6,0) {};
\node[label=$\ell$, vertex] (z) at  (8,0) {};
\draw[edge] (d) to (z);
\draw[edge] (a) to (b);
\draw[edge] (b) to (c);
\draw[edge] (c) to (cp);
%\draw[edge] (d) to (c);
\node at ($(c)!0.5!(d)$) {$\cdots$};
\end{tikzpicture}

\begin{tikzpicture}
\tikzset{vertex/.style = {shape=circle,draw,minimum size=0.2em,fill=black!60}}
\tikzset{edge/.style = {->,> = latex'}}
\node at (-1,0) {$F_4$:};
\node[vertex] (a) at  (0,0) {};
\node[vertex] (b) at  (2,0) {};
\node[vertex] (c) at  (4,0) {};
\node[vertex] (d) at  (6,0) {};
\draw[edge] (a) to (b);
\draw[edge, bend left] (b) to (c);
\draw[edge, bend right] (b) to (c);
\draw[edge] (c) to (d);
\end{tikzpicture}

## Chapters 4-5
These cover the following topics:

- Conjugacy classes of Cartan subalgebras
- The PBW theorem for the universal enveloping algebra
- Serre relations

### Chapter 6

Some import topics include:

- Weight space decompositions
- Finite dimensional modules
- Character and the Harish-Chandra theorem
- The Weyl character formula
  - This will be computed for the specific Lie algebras seen earlier

We will also see the type $A_{\ell}$ algebra used for the first time; however, it differs from the other types in several important/significant ways.

### Chapter 7
Skip!

### Topics

Time permitting, we may also cover the following extra topics:

- Infinite dimensional Lie algebras [Carter 05]
- BGG Cat$\dash\mathcal O$ [Humphrey 08]

## Content

Fix $F$ a field of characteristic zero -- note that prime characteristic is closer to a research topic.

\wrapenv{\Begin{definition}}
A **Lie Algebra** $\lieg$ over $F$ is an $F\dash$vector space with an operation denoted the Lie bracket,

\[
\begin{align*}
[\wait, \wait]: \lieg \cross \lieg \to \lieg\\
(x,y) \mapsto [x, y]
.\end{align*}
\]

satisfying the following properties:

- $[\wait, \wait]$ is bilinear
- $[x, x] = 0$
- The Jacobi identity:

\[
\begin{align*}
[x, [y, z]] + [y, [x,z]] + [z, [x, y]] = \vector 0
.\end{align*}
\]

\wrapenv{\End{definition}}

\wrapenv{\Begin{exercise}}
Show that $[x, y] = -[y,x]$.
\wrapenv{\End{exercise}}

\wrapenv{\Begin{definition}}
Two Lie algebras $\lieg, \lieg'$ are said to be isomorphic if $\varphi([x, y]) = [\varphi(x), \varphi(y)]$.
\wrapenv{\End{definition}}

## Linear Lie Algebras

Let $V = \FF^{n}$, and define $\mathrm{End}(V) = \theset{f: V \to V \suchthat V \text{ is linear}}$. We can then define $\liegl(n, V)$ by setting $[x, y] = (x\circ y) - (y\circ x)$.

\wrapenv{\Begin{exercise}}
Verify that $V$ is a Lie algebra.
\wrapenv{\End{exercise}}

\wrapenv{\Begin{definition}}
Define $$\liesl(n, V) = \theset{f \in \liegl(n, V) \suchthat \Tr(f) = 0}.$$
(Note the different in definition compared to the lie *group* $\SL(n, V)$.).
\wrapenv{\End{definition}}

\wrapenv{\Begin{definition}}
A *subalgebra* of a Lie algebra is a vector subspace that is closed under the bracket.
\wrapenv{\End{definition}}

\wrapenv{\Begin{definition}}
The symplectic algebra
\[
\begin{align*}
\liesp(2\ell, F) = \theset{A \in \liegl(2\ell, F)\suchthat MA-A^{T}M = 0} \text{ where }
M = \left(\begin{array}{c|c}{0} & {I_{n}} \\ \hline {-I_{n}} & {0}\end{array}\right)
.\end{align*}
\]
\wrapenv{\End{definition}}

\wrapenv{\Begin{definition}}

The orthogonal algebra
\[
\begin{align*}
  \lieso(2\ell, F) = \theset{A \in \liegl(2\ell, F)\suchthat MA-A^{T}M = 0} \text{ where } \\
  M = \begin{cases}
  \left(\begin{array}{l|l}
  {1} & {0} \\ \hline
  {0} & {\begin{array}{c|c}{0} & {I_{n}} \\\hline {-I_{n}} & {0}\end{array}}
  \end{array}\right) & n=2\ell + 1 \text{ odd},\\ \\
  \left(\begin{array}{c|c}{0} & {I_{n}} \\ \hline {-I_{n}} & {0}\end{array}\right) & \text{ else}.
  \end{cases}
\end{align*}
\]
\wrapenv{\End{definition}}

\wrapenv{\Begin{proposition}} The dimensions of these algebras can be computed;

- The dimension of $\liegl(n, \FF)$ is $n^{2}$, and has basis ${\theset{e_{{i,j}}}}$ the matrices if a 1 in the $i,j$ position and zero elsewhere.
![$x$ is determined to force the trace to be zero](figures/2019-08-17-01:40.png)

- For type $A_{\ell}$, we have $\dim \liesl(n, \FF) = (\ell+1)^{2} - 1$.

- For type $C_{\ell}$, we have $\abs{}{\liesp(n, \FF)} = \ell^{2} + 2\left(\frac{\ell(\ell+1)}{2} \right )$, and so elements here
\[
\begin{align*}
\left(\begin{array}{ll}{A} & {B=B^{t}} \\ {C = C^{t}} & {A^{t}}\end{array}\right)
.\end{align*}
\]

\wrapenv{\End{proposition}}

- For type $D_{\ell}$ we have
\[
\begin{align*}
\abs{}{\lieso(2\ell, \FF)}
= \dim\theset{ \left(\begin{array}{ll}{A} & {B=-B^{t}} \\ {C = -C^{t}} & {-A^{t}}\end{array}\right)}
,\end{align*}
\]
which turns out to be $2\ell^{2}-\ell$.

- For type $B_{\ell}$, we have $\dim{\lieso}(2\ell, \FF) = 2\ell^{2} -\ell+2\ell = 2\ell^{2} + \ell$, with elements of the form
\[
\begin{align*}
\left(\begin{array}{c|cc}
0 & M & N \\ \hline
-N^{t} & A & C=C^{t} \\
-M^{t} & B=B^{t} & -A^{t}
\end{array}\right)
.\end{align*}
\]
\wrapenv{\Begin{exercise}}
Use the relation $MA =  A^{tM}$ to reduce restrictions on the blocks.
\wrapenv{\End{exercise}}
\[
\begin{tikzcd}
&&  & \lieso(6)             &  &                       \\
&&  & \lieso(5) \arrow[rrd] &  &                       \\
\liesl(4) \arrow[rrruu] & \liesl(2)^2 \arrow[rr] &  & \lieso(4)             &  & \liesp(4)             \\
&&  & \lieso(3)             &  &                       \\
& \liesl(2) \arrow[rru]  &  &                       &  & \liesp(2) \arrow[llu]
\end{tikzcd}
\]

\wrapenv{\Begin{theorem}}
These are *all* of the isomorphisms between any of these types of algebras, in any dimension.
\wrapenv{\End{theorem}}

# Lecture 2

Recall from last time that a Lie Algebra is a vector space with a bilinear bracket, which importantly satisfies the Jacobi identity:
\[
\begin{align*}
[x, [y, z]] + [y, [x,z]] + [z, [x, y]] = \vector 0
.\end{align*}
\]

Also recall the examples from last time:

- $A_\ell \iff \liesl(\ell + 1, F)$
- $B_\ell \iff \lieso(2\ell + 1, F)$
- $C_\ell \iff \liesp(2\ell, F)$
- $D_\ell \iff \lieso(2\ell, F)$

\wrapenv{\Begin{exercise}}
Characterize these matrix subalgebras in terms of basis elements, and compute their dimensions.
\wrapenv{\End{exercise}}

## Lie Algebras of Derivations

\wrapenv{\Begin{definition}}
An **$F\dash$algebra** $A$ is an $F\dash$vector space endowed with a bilinear map
$A^2 \to A,~ (x,y) \mapsto xy$.
\wrapenv{\End{definition}}

\wrapenv{\Begin{definition}}
An algebra is **associative** if $x(yz) = (xy)z$.
\wrapenv{\End{definition}}

> Modern interest: simple Lie algebras, which have a good representation theory. Take a look a Erdmann-Wildon (Springer) for an introductory look at 3-dimensional algebras.

\wrapenv{\Begin{definition}}
Any map $\delta: A^2 \to A$ that satisfies the Leibniz rule is called a **derivation** of $A$, where the rule is given by $\delta(xy) = \delta(x)y + x\delta(y)$.
\wrapenv{\End{definition}}

\wrapenv{\Begin{definition}}
We define $\mathrm{Der}(A) = \theset{\delta \suchthat \delta\text{ is a derivation }}$.
\wrapenv{\End{definition}}

Any Lie algebra $\lieg$ is an $F\dash$algebra, since $[\wait, \wait]$ is bilinear. Moreover, $\lieg$ is associative iff $[x, [y,z]] = 0$.

\wrapenv{\Begin{exercise}}
Show that $\mathrm{Der} \lieg \leq \liegl(\lieg)$ is a Lie subalgebra. One needs to check that $\delta_1, \delta_2 \in \lieg \implies [\delta_1, \delta_2] \in \lieg$.
\wrapenv{\End{exercise}}

\wrapenv{\Begin{exercise}[Turn in]}
Define the adjoint by $\ad_x: \lieg\selfmap,~ y \mapsto [x, y]$. Show that $\ad_x \in \mathrm{Der}(\lieg)$.
\wrapenv{\End{exercise}}

## Abstract Lie Algebras

Fact: Every finite-dimensional Lie algebra is isomorphic to a linear Lie algebra, i.e. a subalgebra of $\liegl(V)$. Each isomorphism type can be specified by certain *structure constants* for the Lie bracket.
\wrapenv{\Begin{example}}
Any $F\dash$vector space can be made into a Lie algebra by setting $[x,y] = 0$; such algebras are referred to as *abelian*.
\wrapenv{\End{example}}

Attempting to classify Lie algebras of dimension at most 2.

- 1 dimensional: We can write $\lieg = Fx$, and so $[x, x] = 0 \implies [\wait, \wait] = 0$. So every bracket must be zero, and thus every Lie algebra is abelian.
- 2 dimensional: Write $\lieg = Fx \oplus Fy$, the only nontrivial bracket here is $[x, y]$. Some cases:
  - $[x, y] = 0 \implies \lieg$ is abelian.
  - $[x, y] = ax + by \neq 0$. Assume $a\neq 0$ and set $x' = ax+by, y' = \frac y a$. Now compute $[x', y'] = [ax+by, \frac y a] = [x,y] = ax+by = x'$. Punchline: $\lieg \cong Fx' \oplus Fy', [x', y'] = x'$.

We can fill in a table with all of the various combinations of brackets:

\begin{center}
\begin{tabular}{l|ll}
$[\wait, \wait]$ & $x'$  & $y'$ \\ \hline
$x'$                                               & $0$   & $x'$ \\
$y'$                                               & $-x'$ & $0$
\end{tabular}
\end{center}

\wrapenv{\Begin{example}}
Let $V = \RR^3$, and define $[a,b] = a\cross b$ to be the usual cross product.
\wrapenv{\End{example}}

\wrapenv{\Begin{exercise}}
Look at notes for basis elements of $\liesl(2, F)$,
\[
\begin{align*}
e=\left[\begin{array}{ll}{0} & {1} \\ {0} & {0}\end{array}\right],\\
h=\left[\begin{array}{cc}{1} & {0} \\ {0} & {-1}\end{array}\right],\\
f=\left[\begin{array}{ll}{0} & {0} \\ {1} & {0}\end{array}\right]
.\end{align*}
\]

Compute the matrices of $\ad(e), \ad(h), \ad(g)$ with respect to this basis.
\wrapenv{\End{exercise}}

## Ideals

\wrapenv{\Begin{definition}}
A subspace $I \subseteq \lieg$ is called an **ideal**, and we write $I \normal \lieg$, if $x,y \in I \implies [x,y]\in I$.
\wrapenv{\End{definition}}

Note that there is no need to distinguish right, left, or two-sided ideals. This can be shown using $[x,y] = [-y, x]$.

\wrapenv{\Begin{exercise}}
Check that the following are all ideals of $\lieg$:

- $\theset 0, \lieg$.
- $\mathfrak z (\lieg) = \theset{z\in \lieg \suchthat [x, z] = 0\quad \forall x\in \lieg}$
- The commutator (or derived) algebra $[\lieg, \lieg] = \theset{\sum_i [x_i, y_i] \suchthat x_i, y_i \in \lieg}$.
  - Moreover, $[\liegl(n, F),\liegl(n, F) ] = \liesl(n, F)$.

\wrapenv{\End{exercise}}


Fact: If $I, J \normal \lieg$, then

- $I+J = \theset{x+y\suchthat x\in I, y\in J} \normal \lieg$
- $I \intersect J \normal \lieg$
- $[I, J] = \theset{\sum_i [x_i, y_i] \suchthat x_i \in I, y_i \in J} \normal \lieg$

\wrapenv{\Begin{definition}}
A Lie algebra is **simple** if $[\lieg, \lieg] \neq 0$ (i.e. when $\lieg$ is not abelian) and has no non-trivial ideals. Note that this implies that $[\lieg, \lieg] = \lieg$.
\wrapenv{\End{definition}}

\wrapenv{\Begin{theorem}}
Suppose that $\ch F \neq 2$, then $\liesl(2, F)$ is not simple.
\wrapenv{\End{theorem}}

\wrapenv{\Begin{proof}}

Recall that we have a basis of $\liesl(2, F)$ given by $B = \theset{e, h, f}$ where

- $[e, f] = h$,
- $[h, e] = 2e$,
- $[h, f] = -2f$.

So think of $[h, e] = \ad_h$, so $h$ is an eigenvector of this map with eigenvalues $\theset{0, \pm 2}$. Since $\ch F \neq 2$, these are all distinct. Suppose $\liesl(2, F)$ has a nontrivial ideal $I$; then pick $x = ae + bh + cf \in I$. Then $[e, x] = 0 - 2be + ch$, and $[e, [e,x]] = 0 - 0 + 2ce$. Again since $\ch F \neq 2$, then if $c\neq 0$ then $e\in I$. Now you can show that $h\in I$ and $f\in I$, but then $I = \liesl(2, F)$, a contradiction. So $c=0$.

Then $x = bh \neq 0$, so $h\in I$, and we can compute

\begin{align*}
2e = [h, e] \in I \implies e \in I, \\
2f = [h, -f] \in I \implies f \in I
.\end{align*}

which implies that $I = \liesl(2, F)$ and thus it is simple.

\wrapenv{\End{proof}}

> Note that there is a homework coming due next Monday, about 4 questions.

# Lecture 3

Last time, we looked at ideals such as $0, \lieg, Z(\lieg),$ and $[\lieg, \lieg]$.

Definition:
If $I \normal \lieg$ is an ideal, then the quotient $\lieg/I$ also yields a Lie algebra with the bracket given by $[x+I, y+I] = [x,y] + I$.

Exercise:
Check that this is well-defined, so that if $x + I = x' + I$ and $y+I = y' + I$ then $[x,y] + I = [x', y'] + I$.

## Homomorphisms and Representations

\wrapenv{\Begin{definition}}
A linear map $\phi: \lieg_{1} \to \lieg_{2}$ is a *Lie homomorphism* if $\phi[x,y] = [\phi(x), \phi()]$.
\wrapenv{\End{definition}}

\wrapenv{\Begin{remark}}
$\ker \phi \normal \lieg_{1}$ and $\im\phi \leq \lieg_{2}$ is a subalgebra.
\wrapenv{\End{remark}}

Fact: There is a canonical way to set up a 1-to-1 correspondence $\theset{I \normal \lieg} \iff \theset{\hom \phi: \lieg \to \lieg'}$ where $I \mapsto (x \mapsto x + I)$ and the inverse is given by $\phi \mapsto \ker \phi$.

Theorem (Isomorphism theorem for Lie algebras):

- If $\phi: \lieg_{1} \to \lieg_{2}$ is a Lie algebra homomorphism, then $\lieg/\ker\phi \cong \im \phi$
- If $I,J \normal \lieg$ are ideals and $I \subset J$ then $J/I \normal \lieg g/I$ and $(\lieg/I)/(J/I) \cong \lieg/J$.
- If $I, J \normal \lieg$ then $(I+J)/J \cong I/(I\intersect J)$.

Definition:
A *representation* of a Lie algebra $\lieg$ is a Lie algebra homomorphism $\phi:\lieg \to \liegl(V)$ into a linear Lie algebra for some vector space $V$.

We call $V$ a $\lieg\dash$module with action $g\cdot v = \phi(g)(v)$.

Example: The *adjoint representation*:
\[
\begin{align*}
\ad: \lieg \to \liegl(\lieg) \\
x \mapsto [x, \wait]
.\end{align*}
\]

\wrapenv{\Begin{corollary}}
Any simple Lie algebra is isomorphic to a linear Lie algebra.
\wrapenv{\End{corollary}}

Proof: Since $\lieg$ is simple, the center $Z(\lieg) = 0$. We can rewrite the center as
\[
\begin{align*}
Z(\lieg) = \theset{x\in\lieg \suchthat \ad_{x(y)} = 0 \quad \forall y\in\lieg} \\
= \ker \ad_{x}
.\end{align*}
\]
Using the first isomorphism theorem, we have $\lieg/Z(\lieg) \cong \im \ad \subseteq \liegl (\lieg)$. But $\lieg/Z(\lieg) = \lieg$ here, so we are done.

## Automorphisms

Definition: An automorphism of $\lieg$ is an isomorphism $\lieg\selfmap$, and we define
\[
\begin{align*}
\Aut(\lieg) = \theset{\phi:\lieg\selfmap \suchthat \phi \text{ is an isomorphism }}
.\end{align*}
\]

Proposition:
If $\delta \in \mathrm{Der}(\lieg)$ is nilpotent, then
$$
\exp(\delta)\coloneqq\sum \frac{\delta^{n}} {n!} \in \Aut(\lieg).
$$

This is well-defined because $\delta$ is nilpotent, and a binomial formula holds:
\[
\begin{align*}
\frac{\delta^{n([x,y])}}{n!} = \sum_{i=0}^{n} [\frac{\delta^{i}(x)}{i!}, \frac{\delta^{n-i}(y)}{(n-i)!}]
.\end{align*}
\]
and for $n=1, \delta([x,y]) = [x, \delta(y)] + [\delta(x), y]$.

Exercise: Show that
\[
\begin{align*}
[(\exp \delta)(x), (\exp\delta)(y)] = \sum_{n=0}^{k-1} \frac {\delta^{n}([x, y])} {n!}
.\end{align*}
\]
Example: Let $\lieg = \liesl(2, \FF)$ and define
\[
\begin{align*}
s = \exp(\ad_{e}) \exp(\ad_{-f}) \exp(\ad_{e}) \in \Aut \lieg
.\end{align*}
\]

where $e,f$ are defined as (todo, see written notes).

Then define the Weyl group $W = \generators{s}$.

Exercise:
Check that $s(e) = -f, s(f) = -e, s(h) = -h$, and so the order of $s$ is 2 and $W = \theset{1, s}$.

# Lecture 4

## Solvability

> Idea: Define a semisimple Lie algebra

Definition:
The derived series for $\lieg$ is given by
\[
\begin{align*}
\lieg^{(0)} = \lieg \\
\lieg^{(1)} = [\lieg^{(0)}, \lieg^{(0)}] \\
\cdots \\
\lieg^{(i+1)} = [\lieg^{(i)}, \lieg^{(i)}]
.\end{align*}
\]

The Lie algebra $\lieg$ is *solvable* if there is some $n$ for which $\lieg^{(n)} = 0$.

Exercise (to turn in):
Check that the Lie algebra of upper triangular matrices in $\liegl(n, \FF)$.

Example: Abelian Lie algebras are solvable

Example: Simple Lie algebras are *not* solvable.

Proposition:
Let $\lieg$ be a Lie algebra, then

1. If $\lieg$ is solvable, then all subalgebras and all homomorphic images of $\lieg$ are also solvable.
2. If $I \normal \lieg$ and both $I$ and $\lieg/I$ are solvable, then so is $\lieg$.
3. If $I, J \normal \lieg$ are solvable, then so is $I+J$.

Corollary (of part 3 above):
Any Lie algebra has a unique maximal solvable ideal, which we denote the *radical* $\mathrm{Rad}(\lieg)$.

Definition:
A Lie algebra is semisimple if $\mathrm{Rad}(\lieg) = 0$.

Example:
Any simple Lie algebra is semisimple.

Example: Using part (2) above, we can deduce that we can construct a semisimple Lie algebra from *any* Lie algebra: for any $\lieg$, the quotient $\lieg/\mathrm{Rad}(\lieg)$ is semisimple.

## Nilpotency
\[
\begin{align*}
\lieg^{0} = \lieg \\
\lieg^{1} = [\lieg^{0}, \lieg^{0}] \\
\cdots \\
\lieg^{i+1} = [\lieg^{i}, \lieg^{i}]
.\end{align*}
\]

Much like the previous case, we have

Example:
Abelian Lie algebras are nilpotent.

Example:
Nilpotent Lie algebras are solvable.

Example:
The *strictly* upper triangular matrices (with zero on the diagonal) are nilpotent.

1. If $\lieg$ is nilpotent, then all subalgebras and all homomorphic images of $\lieg$ are also nilpotent.
2. If $\lieg/Z(\lieg)$ is nilpotent, then so is $\lieg$.
3. If $\lieg \neq 0$ is nilpotent, then $Z(\lieg) \neq 0$.

Claim:
If $\lieg$ is nilpotent, then $\ad_x \in \mathrm{End}(\lieg)$ is nilpotent for all $x\in \lieg$.

Proof:
This is because $\lieg^n = 0 \iff [\lieg, [\lieg, [\lieg, \cdots]]] = 0$, and so for every $x_i, y \in \lieg$ we have $[x_1, [x_2, \cdots [x_n, y]]] = 0$, and so $\ad_{x_1} \circ \ad_{x_2} \circ \cdots \ad_{x_n} = 0$ which implies that $\ad_x^n = 0$ for all $x\in \lieg$.

Theorem [Engel]:
If $\ad_x$ is nilpotent for all $x\in \lieg$, then $\lieg$ is nilpotent.

> Remark: This can be confusing if $\lieg$ is a linear algebra, we can consider elements $x \in\lieg$ and ask if it is the case $x$ being nilpotent (as an endomorphism) iff $\lieg g$ is nilpotent? False, a counterexample is $\lieg = \liegl(2, \CC)$, where there exists an $x$ which is *not* nilpotent while $\ad_x$ *is* nilpotent, which contradicts the above theorem.

Proof:

Lemma:
Let $\lieg \subseteq \liegl(V)$ be a Lie subalgebra for some finite dimensional vector space $V$. If $x$ is nilpotent as an endomorphism on $V$ for all $x\in V$, then there exists a nonzero vector $v\in V$ such that $\lieg v =0$, so $x\in \lieg \implies x(v) = 0$.

Proof of lemma
Use induction on $\dim \lieg$, splitting into two separate base cases:
- Case $\dim \lieg = 0$, then $\lieg = \theset{0}$.
- Case $\dim g = 1$, left as an exercise.

Inductive step: Let $A$ be a maximal proper subalgebra and define $\phi: A \to \liegl(\lieg/A)$ where $a \mapsto (x + A \mapsto [a, x] + A)$. We need to check that $\phi$ is a homomorphism, this just follows from using the Jacobi identity.

We also need to show that $\im \phi \leq \liegl(\lieg/a)$ is a Lie subalgebra, and $\dim \im \phi < \dim \lieg$. The claim is that $\phi(a) \in \mathrm{End}(\lieg/A)$ is nilpotent for all $a\in A$. By the inductive hypothesis, there is a nonzero coset $y + A \in \lieg/A$ such that $(\im \phi) \cdot (y+A) = A$. Since $y\not\in A$, then $\phi(a)(y+A) = A$ for all $a\in A$, and so $[a,y]\in A$.

We want to show that $A$ is a subalgebra of codimension 1, and $A \oplus F_y \leq \lieg$ is a Lie subalgebra.
This is because $[a_1 + c_1y, a_2 + c_2 y] = [a_1, a_2] + c_2[a_1, y] - c_2[a_2, y] + c_1c_2[y, y]$. The last term is zero, the middle two terms are in $A$, and because $A$ is closed under the bracket, the first term is in $A$ as well.

But then $A \oplus F_y$ is a larger subalgebra than $A$, which was maximal, so it must be everything. So $A \oplus F_y = \lieg$. So $A \normal \lieg$ because $[a_1, a_2 + cy]$ is in $A, A \oplus F_y = \lieg$ respectively, and this equals $[a_1, a_2] + c[a_1, y]$, where both terms are in $A$.

Proof to be continued on Friday!

# Lecture 5


Last time: we had a theorem that said that if $\lieg \in \liegl(V)$ and every $x\in\lieg$ is nilpotent, then there exists a nonzero $v \in V$ such that $\lieg v = 0$.

We proceeded by induction on the dimension of $V$, constructing $\im \phi \subseteq \liegl(\lieg/A)$, and showed that $\lieg = A \oplus Fy$. Now consider
$$
W = \theset{v\in V \suchthat Av = 0},
$$

which is $\lieg\dash$invariant, so $\lieg(W) \subseteq W$, or for all $a\in A, x\in \lieg, v\in W$, we have $a\actson x(v) = 0$. This is true because $a\actson x = x\circ a + [a, x] \in \liegl(V)$. But $V$ is killed by any element in $A$, and both of these terms are in $A$. In particular, the $y$ appearing in $Fy$ also satisfies $y \in W$. Consider $\restrictionof{y}{W} \in \mathrm{End}(w)$, and we want to apply the inductive hypothesis to $F \restrictionof{y}{W} \subseteq \liegl(V)$.

We need to check that $\restrictionof{y}{W} \in \mathrm{End}(V)$, which is true exactly because $y$ is nilpotent.
So we can construct a nonzero $v\in W \subset V$ such that $y(v) = 0$, and so $\lieg v = 0$.

Claim: $\phi(a) \in \mathrm{End}(\lieg/A)$ is nilpotent. Each $a\in A\subset \lieg$ is nilpotent by assumption.
Define the maps for left multiplication by $a$, $m_\ell: x \mapsto ax$, and the right multiplication $m_r: x \mapsto xa$.
These are nilpotent, and since $m_\ell, m_r$ commute, the difference $m_\ell - m_r$ is nilpotent, and this is exactly $\ad_a$. But then $\phi(a)$ is nilpotent.

> Good proof for using all of the definitions!

Now we can see what the consequences of having such a nonzero vector are. This theorem implies Engel's theorem, which says that if $\ad_x \in \mathrm{End}(\lieg)$ is nilpotent for every $x\in \lieg$, then $\lieg$ is nilpotent.

Proof: By induction on dimension. The base case is easy. For the inductive step, the previous theorem applies to $\ad g \subset \liegl(\lieg)$. So we can produce the nonzero $v\in \lieg$ such that $\ad \lieg v = 0$. Then $[x, v] = 0$ for all $x\in\lieg$, so either $v\in Z(\lieg)$ or $Z(\lieg) \neq 0$. In either case, $\lieg / Z(\lieg)$ has smaller dimension. Since $\ad_x$ is nilpotent, so is $\ad_x + Z(\lieg)$, and so $\lieg/Z(\lieg)$ is nilpotent.
By an earlier proposition, since the quotient is nilpotent, so is the total space. $\qed$

Let $\mathfrak{N}(F)$ be the subalgebra of $\liegl(F)$ consisting of strictly upper triangular matrices. We have a corollary: if $\lieg \subset \liegl(n, F)$ is a Lie subalgebra such every $x\in \lieg$ is nilpotent as an endomorphism of $F$, then the matrices of $\lieg$ with respect to some bases of in $\mathfrak{N}(n, F)$.

The proof is by induction on $n$, where the base case is easy. For the inductive step, we use the previous theorem to get a $v_1$ such that $x(v_1) = 0$ for all $x\in \lieg$. Let $\overline V = F^n/Fv_1 \cong F^{n-1}$, and define $\phi: \lieg \to \liegl(\overline V)$ where $x \mapsto (\overline y \mapsto \overline{y(x)})$.

Then $\im \phi \leq \liegl(n-1, F)$ as a subalgebra, and every $\phi(x)\in \mathrm{End}(F^{n-1})$ is nilpotent, since $x$ was nilpotent on the larger space. But (see notes) then $x$ can be written as a strictly upper-triangular matrix.

## Chapter 2: Semisimple Lie Algebras

We now assume $\mathrm{char} ~F = 0$ and $\overline F = F$.

Theorem:
If $\lieg$ is a solvable Lie subalgebra of $\liegl(V)$ for some finite dimensional $V$, then $V$ contains a common eigenvector for a $x\in\lieg$, i.e. a $\lambda: \lieg \to F, x \mapsto \lambda(x)$ such that $x(v) = \lambda(x) v$ for all $x\in\lieg$.

Proof: We will use induction on the dimension of $\lieg$. For the inductive step:

Claim 1: There is an ideal $A\normal \lieg$ such that $\lieg = A \oplus Fy$ for some $y\neq 0$, so $A$ is a subalgebra of a solvable Lie algebra $\lieg$ and thus solvable itself. By hypothesis, we can produce a $w \in V\setminus\theset{0}$, and thus a functional $\lambda: A \to F$ such that $aw = \lambda(a) w$ for all $a\in A$. So we define
$$
V_\lambda = \theset{v\in V \suchthat av = \lambda(a)v \forall a\in A}
$$

where $w\in V_\lambda$.

Claim 2: $y(V_\lambda) \subseteq V_\lambda$, or $\restrictionof{y}{V_\lambda}\in\mathrm{End}(V_\lambda)$.

Thus $F(\restrictionof{y}{V_\lambda}) \leq \liegl(V_\lambda)$ is a Lie algebra of dimension 1, and thus solvable. By the inductive hypothesis, we can find a $v\in V_\lambda$ and some $\mu \in F$ such that $y(v) = \mu v$. An arbitrary element $x\in\lieg$ can be written as $x = a + cy$ for some $a\in A, c\in F$ and it acts by $x(v) = a(v) + cy(v) = \lambda(a) v + c\mu v = (\lambda(a) + c)v \in V_\lambda$.

# Lecture n+1
Todo

# Lecture n+2

Definition (Jordan Decomposition)

Let $X \in \mathrm{End}(V)$ for $V$ finite dimensional. Then,

(a) There exists a unique $X_s, X_n \in \mathrm{End}(V)$ such that $X = X_s + X_n$ where $X_s$ is semisimple, $X_n$ is nilpotent, and $[X_s, X_n] = 0$.

(b) There exists a $p(t), q(t) \in t \FF[t]$ such that $X_s = p(X), X_n = q(X)$.

(Polynomials with no constant term.)

Proof of (a): Assume $X_s = X_s + X_n = X_s' + X_n'$, so both have bracket zero. Assuming that (b) holds, we have $X_s = p(X)$, and so
$$
[X, X_s] = [X_s + X_n', X_s'] = [X_s', X_s'] + [X_s', X_n'] = 0 \implies
[p(X), X_S'] = 0 = [X_s, X_s']
$$

Using fact (c) from last time, then $X_s, X_s'$ can be diagonalized simultaneously, and so $X_s - X_s'$ is semisimple.

On the other hand, if $X_n', X_n$ are nilpotent, and since these commute, $X_n - X_n'$ is nilpotent. But then this is a Jordan decomposition of the zero map, i.e.
$$
0 = X - X = (X_s - X_s') + (X_n + X_n')
$$
where the first term is semisimple and the second is nilpotent. Then each term is both semisimple *and* nilpotent, so they must be zero, which is what we wanted to show.

Proof of part (b): Let $m(t) = \prod_{i=1}^r (t-\lambda_i)^{m_i}$ be the minimal polynomial of $X$, where each $m_i \geq 1$ and the $\lambda_i$ are distinct.
Then the primary composition of $V$ is given by
$$
V = \bigoplus_{i=1}^r V_i,\quad V_i = \ker(X - \lambda_i I_V) \neq 0, \quad X(V_i) \subseteq V_i
$$
Claim:
There exists a polynomail $p\in F[t]$ such that
\[
\begin{align*}
p &= \lambda \mod (t-\lambda_i)^{m_i} \quad \forall i, \\
p &= 0 \mod t
.\end{align*}
\]

The existence follows from the Chinese Remainder Theorem.

What is $p(x) \actson V_i$? This acts by scalar multiplication by $\lambda_i$ for all $i$. (Check).
Because of the restrictive conditions, $p(x)$ has no constant term.

![???](figures/2019-08-28-09:28.png)

So $p(X) = X_s$ is the semisimple part we want. Now just set $q(t) = t - p(t)$, then $X_n \coloneqq q(X) = X - X_s$ is nilpotent.

Example:
The Jordan Decomposition is invariant under taking adjoints.

If we have $X = X_s + X_n$, then $\ad_X \in \mathrm{End}(\mathrm{End}(V))$. It can be shown that $(\ad_X)_s + (\ad_X)_n = \ad(X_s) + \ad(X_n)$.

Let $e_{ii}$ be the elementary matrix with a 1 in the $i, j$ position. You can write $\ad_X$ as a $4\times 4$ matrix (see image).

![Image](figures/2019-08-28-09:39.png)
![Image](figures/2019-08-28-09:40.png)
![Image](figures/2019-08-28-09:40.png)

You can check that $(\ad_X)_S = 0, \ad(X_s) = 0$, and $(\ad X)_n$ is the Jordan form given above.

Lemma:

(a) $x \in \mathrm{End}(V) \implies \ad(x)_s = \ad(x_s)$ and $\ad(x)_n = \ad(x_n)$.

(b) If $A$ is a finite dimensional $\FF\dash$algebra, then $\delta \in \mathrm{Der}(A) \implies \delta_s, \delta_n \in \mathrm{Der}(A)$ as well.


Proof of (a):

Check that $\ad(x) = \ad(x_s) + \ad(x_n)$.
Then for $y\in \mathrm{End}(V)$, we have
\[
\begin{align*}
(\ad(x))(y)
&= [x, y] \\
&= [x_s + x_n, y] \\
&= [x_s, y] + [x_n, y] \\
&- (\ad(x_s))(y) + (\ad(x_n))(y)
.\end{align*}
\]

Using theorem 3.3, $x_n$ nilpotent $\implies$ $\ad(x_n)$ is also nilpotent. So write $x_s = \sum \lambda_i e_{ii}$ with the eigenvalues on the diagonal. Then $\ad x_s (e_{ij}) = (\lambda_i - \lambda_j)e_{ij}$ for all $i, j$. But then $\ad x_s$ is given by a matrix with $\lambda_i - \lambda_j$ in the $i,j$ position and zeros elsewhere. By the uniqueness of the Jordan decomposition, the statement follows.

Proof of (b):

Since $\delta \in \mathrm{Der}(A)$, the primary decomposition with respect to $\delta$ is given by
$$
A = \bigoplus_{\lambda \in F} A_\lambda \quad \text{where} ~A_\lambda = \theset{a\in A \suchthat (\delta - \lambda I)^k a = 0 ~\text{for some}~ k >> 0}.
$$
So $\delta_s \actson A_\lambda$ by scalar multiplication (by $\lambda$). Then for $\lambda, \mu \in \FF$, we have

![Image](figures/2019-08-28-09:54.png)

So $[A_x, A_y]  \subseteq A_{\lambda + \mu}$ for all $x, y \in A$. But then

![Image](figures/2019-08-28-09:56.png)

and so $\delta_s \in \mathrm{Der}(A)$, and $\delta_n = \delta - \delta_s \in \mathrm{Der}(A)$ as well.

# Lecture n+3

Todo

# Lecture n+4

Review of bilinear forms: let $V = \FF^n$.

Definition: A bilinear form $\beta: V^2 \to \FF$ can be represented by a matrix $B$ with respect to a basis $\theset{\vector v_i}$ such that
$$
\beta \beta(\sum a_i \vector v_i, \sum b_i \vector v_i) = (a_1 ~a_2 ~\cdots) B (b_1 ~b_2 ~\cdots)
$$


- $\beta$ is *symmetric* iff $\beta(a,b) = \beta(b,a)$.
- $\beta$ is *symplectic* iff $\beta(a, b) = -\beta(b, a)$.
- $\beta$ is *isotropic*  iff $\beta(a, a) = 0$.

For a subspace $U \leq V$, define
$$
U^\perp \coloneqq \theset{\vector v\in V \suchthat \beta(\vector u, \vector v) = \vector 0 ~\forall u\in U}.
$$

> Note: in general, left/right orthogonality are distinguished, but these will be identical when $\beta$ is symmetric/symplectic.

The form $\beta$ is said to be *non-degenerate* iff $V^\perp = 0$ iff $\det B \neq 0$.

Assume $F$ is an algebraically closed field, so $\overline F = F$, and $\mathrm{char} F \neq 2$, then

- If $\beta$ is non-degenerate and symmetric, then $B \sim I_n$
- If $\beta$ is non-degenerate and symplectic, then $B \sim [0, I_{n/2}; I_{n/2}, 0]$.

Remark:

$\lieso(n, \FF) = \theset{x\in \liegl(n, F) \suchthat \beta(x(u), v) = -\beta(u, x(v))}$, where $B$ has the matrix $[0, I; I, 0]$ if $n$ is odd, or this matrix with a 1 in the top-left corner if $n$ is odd.

Similarly, $\mathfrak{sp}(2m, \FF)$ can be described this way with the matrix $[0, -I_m; -I_m, 0]$.

Overview:
The killing form is defined as $\kappa: \lieg^2 \to \FF$ where $\kappa(x, y) = \mathrm{tr}(\ad_x \circ \ad_y)$.

Then we have **Cartan's Criteria**:

- $\lieg$ solvable $\iff$ $\kappa(x,y) = 0 \forall x\in [\lieg, \lieg], y\in \lieg$.
- $\lieg$ semisimple $\iff \kappa$ is non-degenerate.

Note that if $\lieg$ is semisimple, then $\lieg = \bigoplus_i I_i$ with each $I_i \normal \lieg$ and simple.

## Cartan's Criteria

Some facts:

1. $\kappa$ is symmetric
2. If $\lieg$ is finite dimensional, then $\kappa$ is associative, i.e $\kappa([x,y], z) = \kappa(x, [y, z])$.

Exercise: Show that if $I \normal \lieg$, then $I^\perp \leq \lieg$ is an ideal.

Proof of (2):
In section 4.3, it was shown that $\mathrm{tr}([a,b] \circ c) = \mathrm{tr}(a \circ [b, c])$ for all $a,b,c \in \mathrm{End}(V)$ (provided $V$ is finite dimensional).

So
\[
\begin{align*}
\kappa([x,y], z) &= \mathrm{tr}(\ad_{[x, y]} \circ \ad_z) \\
&= \mathrm{tr}([\ad_x, \ad_y] \circ \ad_z) \\
&= \mathrm{tr}(\ad_x \circ [\ad_y, \ad_z]) \\
&= \mathrm{tr}(\ad_x \circ \ad_{[y, z]} ) \\
&= \mathrm{tr}(x, [y, z]).
.\end{align*}
\]

Theorem:
$\lieg$ is semisimple iff $\kappa$ is nondegenerate.

Proof:
$\implies$: We want to show that $\lieg^\perp = 0$. Note that $[\lieg^\perp, \lieg^\perp] \subseteq \lieg$, and so for all $x\in [\lieg^\perp, \lieg^\perp]$ and for any $y\in \lieg^\perp$, we have
$$
\kappa(x, y) = \mathrm{tr}(\ad_x \circ \ad_y) = 0
$$
by the const(?) of $\lieg^\perp$. This implies $\lieg^\perp$ is solvable.

Using fact (2), we have $\lieg^\perp \normal \lieg$ and thus $\lieg^\perp \subseteq \mathrm{rad}(\lieg)$, which is 0 since because $\lieg$ is semisimple. 
So either $\lieg^\perp = 0$ or $\kappa$ is nondegenerate.

> Used the fact that the radical was a maximal solvable ideal.

$\impliedby$: We want to show that for all $I \normal \lieg$ where $[I, I] = 0$, we have $I^\perp \subseteq \lieg^\perp$.

For $x\in I, y\in \lieg$, we have
$$
(\ad_x \circ \ad_y)^2 = \lieg \mapsvia{\ad_y} \lieg \mapsvia{\ad_x} I \mapsvia{\ad_y} I \mapsvia{\ad_x} 0
$$

And thus $\mathrm{tr}(\ad_x \circ \ad_y) = 0$ and $I \subseteq \lieg^\perp$.


Suppose that $\lieg$ is *not* semisimple. Then there exists a solvable ideal $J \neq 0$ such that the last term $J^i$ in the derived series is an ideal $I \normal \lieg$ such that $[I, I] = 0$, forcing $J^i \subset \lieg^\perp = 0$, which is a contradiction.

## Section 5.2

Theorem:
If $\lieg$ is semisimple, then

a. There exist ideals $I_i \normal \lieg$ which are simple Lie algebras satisfying $\lieg = \bigoplus I_i$. Note that $[I_i, I_j] \subseteq I_i \intersect I_j = 0$, since direct summands intersect only trivially.

b. Every simple $I \normal \lieg$ is one of these $I_i$.

c. $\kappa_{I_i} = \restrictionof{\kappa_{\lieg}}{I_i \cross I_i}$, so

![Image](figures/2019-09-06-09:48.png)

Remark:
$\lieg$ is semisimple $\iff \lieg = \bigoplus_i I_i$ for some simple Lie algebras $I_i$.

$\impliedby$: For all $i, S \coloneqq \mathrm{rad} \lieg, I_i \normal I_i$ is a solvable ideal. This implies that it is 0, since $I_i$ is simple.

By definition, simple Lie algebras are not abelian.

Supposing that $S = I_i$, we would then have $[S. S] \neq 0$ since $[I_i, I_i] \neq 0$ by definition. But $[S, S] \neq S$ because $S$ is solvable, which says that $S$ is not simple (a contradiction).

Note that $[\mathrm{rad}\lieg, \lieg] \subseteq \bigoplus [\mathrm{rad} \lieg, I_i] = 0$, which forces $\mathrm{rad}\lieg \subseteq Z(\lieg)$. Since $I_i$ is simple, $Z(I_i) = 0$ for all $i$. But $Z(\lieg) = \bigoplus Z(I_i) = 0$, and this forces $\mathrm{rad}(\lieg) \subseteq Z(\lieg) \implies \mathrm{rad}\lieg = 0$. So $\lieg$ is semisimple.

Next time -- starting the representation theory with $\liesl(2, \FF)$.

# Lecture 10?

Recall the killing form:
\[
\begin{align*}
\kappa: lieg^2 \to \FF \\
(x,y) \mapsto \mathrm{tr}(\ad_x \circ \ad_y)
.\end{align*}
\]

and Cartan's criteria:

1. $\lieg$ is solvable  $\iff \kappa(x, y) = 0 ~\forall x\in \[\lieg, \lieg],~y\in\lieg$.
2. $\lieg$ is semisimple $\iff$ $\kappa$ is non-degenerate.


Theorem:
If $\lieg$ is semisimple, then

a. $\lieg = \bigoplus_{i=1}^n I_i$ for some $I_i \normal \lieg$ which are all simple.
b. Every simple ideal $I \normal \lieg$ is one of the $I_i$.
c. $\kappa_{I_i} = {\kappa_\lieg}\mid_{I_i \cross I_i}$.

![Image](figures/2019-09-09-09:40.png)

Proof of (a):
Use induction on $\dim \lieg$. If $\lieg$ has no nonzero proper ideals, then $\lieg$ is simple and we're done.

Otherwise, let $I_1$ be a minimal nonzero ideal of $\lieg$.
Then $I_1^\perp \normal \lieg$ is also an ideal, and thus $I \coloneqq I_1 \intersect I_1^\perp \normal \lieg$ is as well.
Then for all $x\in [I, I]$, we must have $\kappa(x, y)= 0$ for any $y\in I \subseteq I_1^\perp$.
So $I$ is solvable, and thus $I= 0$. So $\lieg = I_1 \oplus I_1^\perp$.

Note that any ideal of $I_1^\perp$ is also an ideal of $\lieg$, which implies that $\mathrm{rad}(I_1^\perp) \subseteq \mathrm{rad}(\lieg)$, which is zero since $\lieg$ is semisimple, and thus $I_1^\perp$ is semisimple as well.

By the inductive hypothesis, $I_1^\perp = I_2 \oplus \cdots \oplus I_n$ where each $I_j \normal I_i^\perp$ is simple.
Then $I_j \normal \lieg \implies [I_1, I_j] \subset I_1 \intersect I_j$, since $I_1$ has no contribution.
But this is a subset of $I_1 \intersect I_1^\perp = 0$. $\qed$

Proof of (b):
If $I \normal \lieg$, then $[I, \lieg] \normal I$ because $[[I, \lieg], I] \subseteq [I, I] \subseteq [I, \lieg]$.

Since $\lieg$ is semisimple, $0 = \mathrm{rad}(\lieg) \supseteq Z(\lieg)$. So $[I, \lieg] \neq 0$, and thus $[I, \lieg] = I$ since $I$ is simple.
But then $[I, \lieg] = \bigoplus [I, I_i]$ is simple as well.
So only one direct summand can survive, since otherwise this would produce at least 2 nontrivial ideals, and $[I, \lieg] = [I, I_i]$ for some $i$.

So for all $j\neq i$, we must have $I_j \intersect I = I_j \intersect [I, I_i] = 0$, and so $I \subseteq I_i$.
But then $I = I_i$ since $I_i$ itself is simple, and we're done.

Proof of (c):

(Without using the simplicity of $I_i$)

For $x,y\in I_i$, we have

![Image](figures/2019-09-09-09:45.png)

## Inner Derivations

Recall that $\ad \lieg \subseteq \mathrm{Der} \lieg$, and in fact (lemma) this is an ideal.

Theorem:
If $\lieg$ is semisimple, then $\ad \lieg = \mathrm{Der} \lieg$.

Proof of lemma:

For all $\delta \in \mathrm{Der} \lieg$ and all $x,y \in \lieg$, we have

\[
\begin{align*}
[\delta, \ad_x](y) &= \delta([x, y]) - [x, \delta(y)] \\
&= [\delta(x), y] \\
&= [\ad_{\delta(x)}](y)
,\end{align*}
\]

and so $[\delta, \ad x] \subseteq \ad \lieg$. $\qed$

Proof of theorem:

If $\lieg$ is semisimple, then $0 = \mathrm{rad} \lieg \supseteq Z(\lieg) = \ker \ad$. Thus $\ad \lieg \cong g/\ker \ad \cong \lieg$ is also semisimple.

This means that $\kappa_{\ad \lieg}$ is non-degenerate, and thus $\ad \lieg \intersect (\ad \lieg)^\perp = 0$, where $(\ad \lieg)^\perp \normal \mathrm{Der}(\lieg)$.

(Note that the non-degeneracy of $\kappa$ already forces $(\ad \lieg)^\perp = 0$.)

Then $[(\ad \lieg)^\perp, \ad \lieg] = 0$, and so for all $\delta \in (\ad \lieg)^\perp$, we have $\delta(x) = [\delta, \ad x]$ by the lemma, but we've shown that this is zero.

But then $\delta$ must be zero because $\ad$ is an isomorphism, and in particular it is injective.
This means that $(\ad \lieg)^\perp = 0$, and thus $\ad \lieg = \lieg$. $\qed$

We can use this to define an abstract Jordan decomposition by pulling back decompositions on adjoints:

![Image](figures/2019-09-09-10:01.png)

# Friday Lecture 

Todo

# Monday September 16th

Let $S = \exp(\ad e) \circ \exp(\ad -f) \circ \exp(\ad ei)$, which has the following matrix:

![Image](figures/2019-09-16-09:13.png)

Where $\exp(\ad e) = 1 + \ad e + \frac 1 2 (\ad e)^2$, which would have the form

![Image](figures/2019-09-16-09:15.png)

Theorem: 
If $\lieg$ is semisimple, then any finite dimensional $\lieg\dash$module $V$ is completely reducible, i.e. it splits into a direct sum of simple modules.

## Proof of Weyl's(?) Theorem

If $V$ itself is simple, then we're done, so suppose it is not.

Assume there exists a nonzero submodule $U \subsetneq V$.
It suffices to show that $V = U \oplus U'$ for some $U'$.

### Step 1: 

If $\dim V = 2$ and $\dim U = 1$.


Then $U, V / U$ are both trivial modules.
So $g\actson u = 0$ for all $u\in U$.
But then $g \actson (v + U) = U$ for all $v\in V$, since $g\actson v \in U$.

So for all $x,y \in lieg$ and all $v\in V$, we have $[x,y]\actson v = x\actson(y\actson v) - y\actson(x\actson v)$. But both of the terms in parenthesis are in $U$, and all elements in $\lieg$ kill elements in $U$, so this is zero. So $[\lieg, \lieg] \actson V$ trivially.

> Exercise: If $\lieg$ is semisimple, then $[\lieg, \lieg] = \lieg$.

So $\lieg \actson V$ trivially. Thus any $U'$ that is a complementary subspace of $U$ will be a submodule of $V$.

### Step 2:

Suppose $U$ is simple and $\dim U > 1$, so $\dim V / U = 1$.

Let $\Omega$ be the Casimir element on $U$ (faithful representation?).
Then $\Omega u = c \u$ for some $c \in \FF$, and so $\Omega(U) \subseteq U$.

Since $\Omega: V \selfmap$ is a homomorphism, $\ker \Omega \subseteq V$ is a $\lieg\dash$submodule.
Then $\dim V / U = 1 \implies V/U$ is a trivial module.
So $\lieg \actson V/U = 0$, i.e. $\lieg \actson V \subseteq U$.

Then $\Omega(v) = \sum_i x_i \actson (y_i \actson v) \in U$ for all $v\in V$. 
What is the matrix of $\Omega$?

![Image](figures/2019-09-16-09:31.png)

In particular, $\Tr(\Omega \mid_{V/U}) = 0$. So $\Tr(\Omega) = \Tr(\Omega\mid_U)$.
From 6.2, we know that $\Tr(\Omega) \neq 0 \implies c\neq 0$, where $c$ is the scalar appearing above.
So $\ker \Omega$ is 1-dimensional, and $\ker \Omega \intersect U = \theset 0$.

So take $U' = \ker \Omega$.

### Step 3:

Suppose $U$ is *not* simple, but $\dim V/U = 1$.

We will induct on the dimension of $U$. 
Pick a proper nonzero submodule $\overline U \subsetneq U$, so that $\dim U / \overline U < \dim U$. 
Now $V/U \cong (V / \overline U) / (U / \overline U)$ by an isomorphism theorem.
So $U / \overline U$ is a submodule of $V/\overline U$ of codimension 1.
Applying the inductive hypothesis, we obtain $V / \overline U = U / \overline U \oplus \overline V / \overline U$ for some $\overline V$ such that $U \subseteq \overline V \subseteq V$.

In particular, since $U \subseteq \overline V$ has codimension 1, $\dim \overline U < \dim U$.
So apply the inductive hypothesis again: $\overline V = \overline U \oplus U'$ for some $U'$, and $V = U \oplus U'$.

### Step 4: The general case

Recall that $\hom(V, U)$ is a $\lieg\dash$module where 
\[
\begin{align*}
(g \actson \phi)(v) = g\actson\phi(v) - \phi(g\actson v)
.\end{align*}
\]

Define
$$
S = \theset{\phi \in \hom(V, U) \suchthat \phi\mid_U \in F 1_U}.
$$

Then $S \leq \hom(V, U)$ as a submodule.
Define $T = \theset{\phi \in S \suchthat \phi\mid_U = 0}$.
Then $T \leq S$ as a submodule, and $\lieg(S) \subseteq T$.

Now each $\phi\in S$is determined ($\mod T$) by the scalar $\phi\mid_U$.
Note that $\dim(S/T) = 1$.
By steps 1-3, we know that $S = T \oplus T'$ for some $T' \subseteq S$ of dimension 1.
Then $T' = \mathrm{span}_\FF(f)$ for some nonzero map $f: V \to U$ such that $f(u) = cu$ for some $c \neq 0$.

Then $\lieg(T \oplus T') = \lieg (S) \subseteq T \implies \lieg(T') = 0$.
So for all $g\in \lieg$, we have $0 = (g\actson f)(v) = f \actson f(v) - f(g\actson v)$.
Then $f: V \to U$ is a lie algebra homomorphism, $\ker f = U'$, and thus $V = U \oplus U'$. 
$\qed$


Some consequences of Weyl's theorem:

## Preservation of Jordan Decomposition

Recall that when $\lieg \in \liegl(V)$ is a linear lie algebra, then for $x\in \lieg$ we have:

Jordan Decomposition: $x = x_s + x_n$ where $x_s, x_n \in \mathrm{End}(V)$.

Abstract Jordan Decomposition: 
\[
\begin{align*}
\lieg \mapsvia{\ad} \ad(\lieg) \\
x \mapsto \ad x \\
x_s \leftarrow(\ad x)_s \\
x_n \leftarrow  (\ad x)_n
.\end{align*}
\]
and so $x = x'_s + x'_n$ for some $x'$. The theorem will be that these recover the usual Jordan decomposition.

Theorem:
If $\lieg \in \liegl(V)$ is semisimple and $V$ is finite dimensional, then $x_s, x_n \in \lieg$, and $x_s = x'_s, x'_n$.

Corollary:
If $\lieg$ is semisimple and finite dimensional and $\phi: \lieg \to \liegl(V)$ is a finite dimensional representation, then if $x = x_s + x_n$ is the abstract Jordan decomposition, then $\phi(x) = \phi(x_s) + \phi(x_n)$ is the Jordan decomposition in $\liegl(V)$.


Example:
If $\lieg = \liesl(2, \CC)$ is semisimple and finite dimensional, and $h$ is diagonal, then by JD $h = h + 0, \phi(h) = \phi(h) + 0$.
Then $h \actson V$ semisimply, or $V = \bigoplus_{\lambda \in \CC} V_\lambda$, where $V_\lambda = \theset{v\in V \suchthat h\actson v = \lambda v}$ are the eigenspaces.

![Image](figures/2019-09-16-10:05.png)

# Wednesday Lecture

Last time:
The abstract Jordan Decomposition coincides with the actual Jordan Decomposition.
\[
\begin{align*}
\phi: \lieg \to \liegl(V) \\ 
x \mapsto \phi(x) = \phi(x)_s + \phi(x)_n = \phi(x_n) + \phi(x_s) \\
x_s + x_n \mapsto \phi(x_s) + \phi(x_n)
.\end{align*}
\]

Therefore $x_s \actson V$ semisimply. 
The example we saw last time was $\lieg = \liesl(2, \CC)$, with a matrix $h = [1, 0; 0, -1]$ and $V = \bigoplus_{\lambda \in \CC} V_ \lambda$.

## Finite Dimensional Representations of $\liesl(2, \CC)$
## Weights and Maximal Vectors

Definition:
If $V_\lambda \neq 0$, then $V_\lambda$ is a *weight space* of $V$ and $\lambda \in \CC$ is a *weight* of $h$ in $V$. We then define $W_t(V) = \theset{\text{weights in } V}$.

Lemma:
If $v\in V_\lambda$ then $e\actson v \in V_{\lambda + 2}$ and $f\actson v \in V_{\lambda - 2}$.

Proof:
\[
\begin{align*}
h \actson (e\actson v) = [h, e] \actson v + e\actson (h \actson v) \\
= 2e \actson v + \lambda e \actson v \\
= (\lambda +2 )e \actson v
.\end{align*}
\]

and
\[
\begin{align*}
h \actson (f\actson v) = [h, f] \actson v + f\actson (h \actson v) \\
= -2f \actson v + \lambda f \actson v \\
= (\lambda -2 )f \actson v
.\end{align*}
\]

So if $V$ is a finite-dimensional $\lieg\dash$module, then there exists a $V_\lambda \neq 0$ such that $V_{\lambda + 2} = 0$. Any nonzero $v\in V_{\lambda}$ is called a *maximal vector*.

> Note: in category $\mathcal O$, these always exist?

Some computations:

- $\lieg = \liegl(2, \CC)$
Then $V = \CC$ is the trivial module, and $g\actson V = 0$. 
So $W_t(V) = \theset{0}$, and $V = V_0$.

If $V = \CC^2$, then take the natural representation $\mathrm{span}_\CC\theset{v_1 = [1, 0], v_2 = [0, 1]}$.
Then $g\actson V$ by matrix multiplication, and if $h = [1, 0; 0, -1]$ then $h\actson v_1 = v_1$ and $h\actson v_2 = -v_2$ by just doing the matrix-vector multiplication.
Then $\CC([1, 0]) = V_1, \CC([0, 1]) = V_{-1}$, so $W_t(V) = \theset{\pm 1}$.

Taking $V = \CC^3 = \ad \lieg = \mathrm{span})_\CC \theset{e, f, h}$, then
\[
\begin{align*}
h\actson f = [h, f] = -2f \\
h\actson h = [h, h] = 0h \\
h\actson e = [h, e] = 2e
.\end{align*}
\]

So $W_t(V) = \theset{2, 0, -2}$ and $V_2 = \CC e, V_0 = \CC h, V_{-2} = \CC f$.

> Note the pattern: some largest value, then jumping by 2 to lower values, ending at negative the largest value. In some sense, the rest of the theory will reduce to the case of $\liesl(2, \CC)$.

Lemma:
Let $V$ be a finite dimensional simple $\liesl(2, \CC)\dash$module, and $V_0 \in V_\lambda$ a maximal vector.

Set $V_{-1} = 0, V_i = f^{(i)} \actson v_0$ (where $f^{(i)} = \frac{f^i}{i!}$). Then for all $i \geq 0$, we have

a. $h\actson v_i = (\lambda - 2i) v_i$
b. $f\actson v_i = (i+1)v_{i+1}$
c. $e\actson v_i = (\lambda - i + 1)v_{i-1}$

Proof of (a):
By lemma 7.1, we have $f\actson v_0 \in V_{\lambda - 2}$, and so inductively $f^{(i)}\actson v_0 \in V_{\lambda - 2i}$

Proof of (b):
By definition.

Proof of (c):
\[
\begin{align*}
ie\actson v_i = ie \actson \frac{f^i \actson v_0}{ i! }\\
= e\actson (f\actson v_{i-1}) \\
= [e,f] \actson v_{i-1} + f\actson (e\actson v_{i-1}) \\
= h \actson v_{i-1} + f\actson((\lambda -i + 2)v_{i-2}) \text{ind} \\
= (\lambda - 2i + 2) v_{i-2} + (\lambda + i - 2)(i-1 v_{i-1}) \\
= i (\text{RHS})
.\end{align*}
\]

Theorem:
If $V$ is a finite dimensional and simple, then $V \cong L(m)$ for some $m \in \ZZ_{\geq 0}$ where $L(m) = \mathrm{span}_\CC\theset{v_0, v)1, \cdots v_m}$ where each $v_i$ is of weight $m - 2i$.

Thus $L(m) = L(m)_m \oplus L(m)_{m-2} \oplus \cdots \oplus L(m)_{-m}$ where $\dim L(m)_\mu = 1$ for all $\mu$ and $\dim L(m = m+1$.)


Proof:
Pick a maximal vector $v_0 \in V_\lambda$ for any weight $\lambda$. 
Define $v_i$ as usual.
Let $m = \min \theset{i \suchthat V_i \neq 0,~ V_{i+1} = 0}$
![Image](figures/2019-09-18-09:42.png)

Definition:
A module $V$ is a *highest weight module* of weight $\lambda$ if $V = \lieg\actson v_0$ for some maximal vector $v_0 \in V_\lambda$.

Then $\lambda$ is referred to as the *highest weight*, and $v_0$ is the *highest weight vector*.

Corollary:
If $V$ is finite-dimensional, then 

a. $V = \bigoplus_{\lambda \in \ZZ} V_\lambda$
b. The number of summands = $\dim V_0 + \dim V_1$.

Proof of (a):
By Weyl's theorem, we know $V = \oplus W_i$ for some simple $W_i$. By theorem 7.2, this is equal to $\oplus_{m\in \ZZ_{\geq 0}} L(m)^{\mu_m}$

Proof of (b):
$\dim V_0 = \#\theset{\text{summands where $m$ is even}}$
$\dim V_1 = \#\theset{\text{summands where $m$ is odd}}$

Remark:
Let $V_d = \theset{f\in\CC[x,y] \suchthat \text{$f$ is homogeneous of total degree $d$}} = \mathrm{span}_\CC\theset{x^d, x^{d-1}y, \cdots, y^d}$.

Then $\liesl(2, \CC)\actson V_d$ by
\[
\begin{align*}
e \mapsto x \dd{}{y} \\
f \mapsto y \dd{}{x} \\
h \mapsto x\dd{}{x} - y\dd{}{y}
.\end{align*}
\]
Fact:
For $L(m), \phi: \liesl(2, \CC) \to \liegl(L(m))$, define
$$
s = (\exp \phi(e)) \circ (\exp \phi(-f)) \circ (\exp \phi(e))
$$

Then $s(v_i) = -v_{m-i}$.

# Friday Lecture

Last time: 
Construction of simple finite-dimensional $\liesl(2, \CC)$ module.

Today:
Root space decomposition for semisimple finite-dimensional $\lieg$.

## Root Space Decomposition

Let $\lieg$ be semisimple and finite dimensional, and let $\FF = \CC$.

### Maximal Toral subalgebra and roots

Definition:
A subalgebra $\mathfrak{h} \leq \lieg$ is *toral* if $\mathfrak{h} \neq 0$ and it consists of only semisimple elements (i.e. $x_n = 0 \forall x\in \mathfrak{h}$)

Lemma:

a. There exists a toral subalgebra of $\lieg$, which is a nontrivial maximal toral subalgebra.
b. Any toral subalgebra is abelian.

Proof of (a):
Want to show that there exists an $x\in \lieg$ such that $x_s \neq 0$, which will imply that $\mathfrak{h} = \CC x_s$ is toral.

Suppose $x_s = 0$ for all $x\in \lieg$, then $\ad x = \ad x_n$ is nilpotent.
By Engel's theorem, this means $\lieg$ must be nilpotent.
But this contradicts $[\lieg, \lieg] = \lieg$ (since $\lieg$ is semisimple) so the derived series can never reach zero.

Proof of (b):
Fix $x\in \mathfrak{h}$, want to show that $[x, h] = 0 \forall h\in \mathfrak{h}$.
Then $x = x_s$, and so $\ad x: \lieg \to \lieg$ is diagonalizable.
It suffices to show that $\restrictionof{\ad x}{\mathfrak h} = 0$ for all $\mathfrak h$.

Suppose that $[x, h] = ah$ for some vector $h$ where $a\neq 0$.
Decompose $\mathfrak h$ into eigenspaces, so $\mathfrak h = \bigoplus_\lambda \mathfrak{h}_\lambda$ where $\mathfrak h_\lambda = \theset{y \in \mathfrak h \suchthat [h, y] = \lambda y}$.
But then $[h, x] \in \mathfrak h_0$, since $[h, [h, x]] = [h, -ah] = 0$.

So write $x = \sum_\lambda c_\lambda x_\lambda$, where $c_\lambda \in \CC$ and $x_\lambda \in \mathfrak h_\lambda$.
Then
\[
\begin{align*}
[h, x] &= \sum_\lambda c_\lambda [h, x_\lambda] \\ 
&= \sum_\lambda c_\lambda \lambda x_\lambda \in \mathfrak h_0,
\end{align*}
\]
so $\lambda c\lambda = 0 \forall \lambda \neq 0$, which means $c_\lambda = 0 \forall \lambda \neq 0$, and thus $x\in \mathfrak h_0$ and $[h,x] = 0$.
But this contradicts $[x, h] = ah$.

Now $\forall x, h\in\mathfrak h, g \in \lieg$, we have $[h, [x, y]] = [x, [h, y]] + [y, [x, h]] = [x, [h, y]]$.
Thus $\ad h \circ \ad x = \ad x \circ \ad h$ as elements of $\mathrm{End}(\lieg)$.

So $\lieg = \bigoplus_{\alpha \in \mathfrak h^*} \lieg_\alpha$, where $\lieg_\alpha = \theset{x\in \lieg \suchthat [h, x] = \alpha(h)x \forall h\in\mathfrak h}$.

Note that $\lieg_0 = \theset{x \in \lieg \suchthat [h, x] = 0 \forall h\in\mathfrak h} = C_\lieg(\mathfrak h) \supseteq \mathfrak h$, i.e. the centralizer of $\mathfrak h$ in $\lieg$.

Definition:
Fix a toral subalgebra $\mathfrak h \subseteq \lieg$, then a *root* is a nonzero $\alpha \in \mathfrak h^*$ such that $\lieg_\alpha \neq 0$. 
$\lieg_\alpha$ is referred to as the *root space*.

We write $\Phi = \theset{\text{roots}}$ and $\lieg = C_\lieg(\mathfrak h) \oplus \left( \bigoplus_{\alpha \in \Phi}\lieg_\alpha \right)$.

Example: $\liesl(3, \CC)$.

TODO: Insert image from phone.

Then $\Phi = \theset{\alpha: \mathfrak h \to \CC, h_1 \mapsto \alpha(h_1) \in \theset{\pm 1, \pm 2}}$.
So 

- $\lieg_0 = \CC h_1 \oplus \CC h_2$
- $\lieg_1 = \CC f_2 \oplus \CC e_3$
- $\lieg_2 = \CC e_1$
- $\lieg_{-1} \CC f_3 \oplus \CC e_2$
- $\lieg_{-2} = \CC f_1$.

TODO: Insert second and third image from phone

From these computations, we collect the eigenvalues as ordered pairs.
If we choose a larger toral subalgebra, we get a finer decomposition.
And if we take a maximal toral subalgebra, then $\mathfrak h = \lieg_0$ and all $\dim \lieg_\alpha = 1$.

Proposition (a): 
$[\lieg_\alpha, \lieg_\beta] \subseteq \lieg_{\alpha + \beta}$ for all $\alpha, \beta \in \mathfrak h^*$.

Proposition (b):
If $x\in \lieg_\alpha$ and $\alpha \neq 0$ then $\ad x$ is nilpotent.

Proposition (c):
If $\alpha, \beta \in \mathfrak h^*$ and $\alpha + \beta = 0$, then $\kappa(x, y) = 0 \forall x\in \lieg_\alpha, y= \lieg_\beta$.

Proof of (a): 
Easy exercise: 

Proof of (b):
For all $y \in \lieg$, $y \in \lieg_\mu$ for some $\mu \in \mathfrak h^*$.
We have $\lieg_u \mapsvia{\ad x} \lieg_{\mu + \alpha} \mapsvia{\ad x} \lieg_{\mu + 2\alpha} \to \cdots$ by $y \mapsto [x,y] \mapsto \cdots$.
Since $\lieg$ is finite dimensional, this must terminate, so $(\ad x)^n(y) = 0$ for some $n$.

Proof of (c):
If $\alpha + \beta  = 0$, then there exists an $h\in \mathfrak h$ such that $\alpha(h) + \beta(h) \neq 0$.
Since the killing form is associative, we have

![Image](figures/2019-09-20-09:54.png)

Corollary: 
$\kappa\mid_{\lieg_0}$ is nondegenerate.

Proof:
We want to show $\kappa(h, y) = 0 \forall y\in\lieg_0 \implies h = 0$ holds for any choice of $y\in \lieg_\alpha$ with $\alpha \neq 0$.

By proposition (c), we have $\kappa(h, y) = 0$.
Note that we have $\lieg = \lieg_0 \oplus (\bigoplus_{\alpha\neq 0} \lieg_\alpha)$.
This implies that $\kappa(h, y) = 0 \forall y\in \lieg$.
But then $h = 0$ because $\kappa$ is nondegenerate and $\lieg$ is semisimple.

# Monday Lecture

Last time:
$\mathfrak{h}$ is a *toral* subalgebra if it contains only semisimple elements, and implies that there is a *root space decomposition*
$$
\lieg = \mathfrak g_0 \oplus \bigoplus_{\alpha \in \Phi} \lieg_\alpha
$$

where $\lieg_\alpha = \theset{x\in\lieg \suchthat [h, x] = \alpha(h)x ~\forall h\in\mathfrak h}$ and $\Phi = \theset{\alpha: \mathfrak h \to \CC \suchthat \lieg_\alpha\neq 0, \alpha \neq 0}$ and $\lieg_0 = C_\lieg(\mathfrak h)$.

Take larger $\mathfrak h$ yields finer decompositions, and a maximal $\mathfrak h$ gives $\dim \lieg_\alpha  = 1 \forall \alpha \in Phi$. 

Corollary: $\restrictionof{\kappa}{\lieg_0}$ is nondegenerate.

## The Centralizer of $\mathfrak h$

If $x,y \in \mathrm{End}(V)$ where $V$ is finite dimensional, $xy=yx$, and $y$ is nilpotent, then $xy$ is nilpotent and $\Tr(xy)=0$.
