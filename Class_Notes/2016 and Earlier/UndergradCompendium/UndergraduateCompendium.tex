\input{"/home/zack/Notes/Latex/preamble.tex"}

\addbibresource{UndergraduateCompendium.bib}

\let\Begin\begin
\let\End\end
\newcommand\wrapenv[1]{#1}

\makeatletter
\def\ScaleWidthIfNeeded{%
 \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\def\ScaleHeightIfNeeded{%
  \ifdim\Gin@nat@height>0.9\textheight
    0.9\textheight
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\setkeys{Gin}{width=\ScaleWidthIfNeeded,height=\ScaleHeightIfNeeded,keepaspectratio}%

\title{
\rule{\linewidth}{1pt} \\
\textbf{
    Undergraduate Compendium
  }
  \rule{\linewidth}{2pt}
}
\titlehead{
    \begin{center}
  \includegraphics[width=\linewidth,height=0.5\textheight,keepaspectratio]{figures/cover.png}
  \end{center}
       \begin{minipage}{.35\linewidth}
    \begin{flushleft}
      \vspace{2em}
      {\fontsize{6pt}{2pt} \textit{Notes: These are notes I took while
studying for the Mathematics Subject GRE. There are likely a lot of
errors and mistakes, please let me know if you find any! } } \\
    \end{flushleft}
    \end{minipage}
    \hfill
    \begin{minipage}{.65\linewidth}
    \end{minipage}
  }







\begin{document}

\date{}
\author{D. Zack Garza}
\maketitle
\begin{flushleft}
\textbf{D. Zack Garza} \\
\textit{University of Georgia} \\
\textit{dzackgarza@gmail.com} \\
{\tiny \textit{Last updated:} 2020-11-10 }
\end{flushleft}


\newpage
\tableofcontents

\hypertarget{differential-calculus}{%
\section{Differential Calculus}\label{differential-calculus}}

\hypertarget{big-theorems-tools}{%
\subsection{Big Theorems / Tools:}\label{big-theorems-tools}}

\begin{proposition}[Fundamental Theorem of Calculus I]

\begin{align*}
\frac{\partial}{\partial x} \int_a^x f(t) dt = f(x) \\ \\
\end{align*}

\end{proposition}

\begin{proposition}[Generalized Fundamental Theorem of Calculus]

\begin{align*}
\frac{\partial}{\partial x} \int_{a(x)}^{b(x)} f(x, t) dt - \int_{a(x)}^{b(x)} \frac{\partial}{\partial x} f(x, t) dt &=
f(x, t) \cdot \frac{\partial}{\partial x}(t) \bigg\rvert_{t=a(x)}^{t=b(x)} \\ \\
&= f(x, b(x))\cdot b'(x) - f(x, a(x))\cdot a'(x) \\ \\
\end{align*}

If \(f(x,t) = f(t)\) doesn't depend on \(x\), then \(\dd{f}{x} = 0\) and
the second integral vanishes:

\begin{align*}
\frac{\partial}{\partial x} \int_{a(x)}^{b(x)} f(t) dt 
&= f(b(x))\cdot b'(x) - f(a(x))\cdot a'(x)
\end{align*}

\end{proposition}

\todo[inline]{Find examples.}

\begin{remark}

Note that you can recover the original FTC by taking
\begin{align*}  
a(x) &= c \\
b(x) &= x \\
f(x,t) &= f(t)
.\end{align*}

\end{remark}

\begin{corollary}[?]

\begin{align*}
    \frac{\partial}{\partial x} \int_{1}^{x} f(x, t) dt = \int_{1}^{x} \frac{\partial}{\partial x} f(x, t) dt + f(x, x)
\end{align*}

\end{corollary}

\begin{proposition}[Extreme Value Theorem]

Todo

\end{proposition}

\todo[inline]{Todo}

\begin{proposition}[Mean Value Theorem]

\begin{align*}  
f \in C^0(I) &\implies \exists p\in I: f(b) - f(a) = f'(p)(b-a) \\
          &\implies \exists p\in I: \int_a^b f(x)~dx = f(p)(b-a)
.\end{align*}

\end{proposition}

\begin{proposition}[Rolle's Theorem]

\end{proposition}

\todo[inline]{todo}

\begin{proposition}[L'Hopital's Rule]

If

\begin{itemize}
\tightlist
\item
  \(f(x)\) and \(g(x)\) are differentiable on \(I - \pt\), and
\end{itemize}

\begin{align*}
  \lim_{x\to \pt} f(x) = \lim_{x\to \pt} g(x) \in \theset{0, \pm \infty}, &&
  \forall x \in I, g'(x) \neq 0, && 
  \lim_{x\to\pt} \frac{ f'(x)}{\ g'(x)} \text{ exists}, \\
\end{align*}

Then it is necessarily the case that
\begin{align*}
\lim _ { x \rightarrow \pt } \frac { f ( x ) } { g ( x ) } 
= \lim _ { x \rightarrow \pt } \frac { f ^ { \prime } ( x ) } { g ^ { \prime } ( x ) }.
\end{align*}

\end{proposition}

\begin{remark}

Note that this includes the following indeterminate forms:
\begin{align*}  
\frac{0}{0}, \quad \frac{\infty}{\infty}, \quad 0 \cdot \infty, \quad 0^{0}, \quad \infty^{0}, \quad 1^{\infty}, \quad \infty-\infty
.\end{align*}

For \(0\cdot \infty\), can rewrite as
\({0 \over {1\over \infty}} = {0\over 0},\) or alternatively
\({\infty \over {1\over 0}} = {\infty \over \infty}.\)

For \(1^\infty, \infty^0,\) and \(0^0\), set
\begin{align*}
L \da \lim f^g \implies \ln L = \lim g \ln(f)
\end{align*} to recover \(\infty\cdot 0, 0 \cdot \infty,\) or
\(0\cdot 0\).

\end{remark}

\begin{proposition}[Taylor Expansion]

\begin{align*}
T(a, x) &= \sum _ { n = 0 } ^ { \infty } \frac { f ^ { ( n ) } ( a ) } { n ! } ( x - a ) ^ { n }  \\
&= f ( a ) + f'(a)( x - a ) + \frac { 1 } { 2 }f ^ { \prime \prime } ( a ) ( x - a ) ^ { 2 } \\ 
& \quad \quad + \frac { 1} { 6 } f ^ { \prime \prime \prime } ( a ) ( x - a ) ^ { 3 } + \frac{1}{24}f^{(4)}(a)(x-a)^4 + ~\cdots
\end{align*} There is a bound on the error:
\begin{align*}
\abs{f(x) - T_k(a,x)} \leq \abs{\frac{f^{(k+1)}(a)}{(k+1)!}}
\end{align*} where
\(T_k(a, x) = \sum _ { n = 0 } ^ { k } \frac { f ^ { ( n ) } ( a ) } { n ! } ( x - a ) ^ { n }\)
is the \(k\)th truncation.

\end{proposition}

\begin{remark}

Approximating change: \(\Delta y \approx f'(x) \Delta x\)

\end{remark}

\hypertarget{limits}{%
\subsection{Limits}\label{limits}}

\hypertarget{tools-for-finding-limits}{%
\subsection{Tools for finding limits}\label{tools-for-finding-limits}}

\todo[inline]{Examples}

How to find \(\lim_{x\to a} f(x)\)in order of difficulty:

\begin{itemize}
\item
  Plug in: if \(f\) is continuous, \(\lim_{x\to a} f(x) = f(a)\).
\item
  Check for indeterminate forms and apply L'Hopital's Rule.
\item
  Algebraic rules
\item
  Squeeze theorem
\item
  Expand in Taylor series at \(a\)
\item
  Monotonic + bounded
\item
  One-sided limits:
  \(\lim_{x\to a^-} f(x) = \lim_{\varepsilon \to 0} f(a-\varepsilon)\)
\item
  Limits at zero or infinity:
  \begin{align*}
  \lim_{x\to\infty} f(x) = \lim_{\frac{1}{x}\to 0} f\qty{\frac{1}{x}} \text{ and } 
  \lim_{x\to 0} f(x) = \lim_{x\to\infty} f\qty{1 \over x}
  \end{align*}

  \begin{itemize}
  \tightlist
  \item
    Also useful: if \(p(x) = p_nx^n + \cdots\) and
    \(q(x) = q_nx^m + \cdots\),
    \begin{align*}
    \lim_{x\to\infty} \frac{p(x)}{q(x)} = 
    \begin{cases}
    0 & \deg p < \deg q \\
    \infty & \deg p > \deg q \\
    \frac{p_n}{q_n} & \deg p = \deg q
    \end{cases}
    \end{align*}
  \end{itemize}
\end{itemize}

\begin{warning}

Be careful: limits may not exist!! Example
\(:\lim_{x\to 0} \frac{1}{x} \neq 0\).

\end{warning}

\hypertarget{asymptotes}{%
\subsection{Asymptotes}\label{asymptotes}}

\begin{itemize}
\tightlist
\item
  Vertical asymptotes: at values \(x=p\) where
  \(\lim_{x\to p} = \pm\infty\)
\item
  Horizontal asymptotes: given by points \(y=L\) where
  \(L \lim_{x\to\pm\infty} f(x) < \infty\)
\item
  Oblique asymptotes: for rational functions, divide - terms without
  denominators yield equation of asymptote (i.e.~look at the asymptotic
  order or ``limiting behavior'').

  \begin{itemize}
  \tightlist
  \item
    Concretely:
  \end{itemize}
\end{itemize}

\begin{align*}
f(x) = \frac{p(x)}{q(x)} = r(x) + \frac{s(x)}{t(x)} \sim r(x)
\end{align*}

\hypertarget{recurrences}{%
\subsection{Recurrences}\label{recurrences}}

\begin{itemize}
\tightlist
\item
  Limit of a recurrence: \(x_n = f(x_{n-1}, x_{n-2}, \cdots)\)

  \begin{itemize}
  \tightlist
  \item
    If the limit exists, it is a solution to \(x = f(x)\)
  \end{itemize}
\end{itemize}

\hypertarget{derivatives}{%
\subsection{Derivatives}\label{derivatives}}

\begin{proposition}[Chain Rule]

\begin{align*}
\dd{}{x}(f\circ g) = (f' \circ g) \cdot g'
\end{align*}

\end{proposition}

\begin{proposition}[Product Rule]

\begin{align*}
\dd{}{x} f\cdot g =f'\cdot g + g' \cdot f
\end{align*}

\end{proposition}

\begin{proposition}[Quotient Rule]

\begin{align*}
\dd{}{x} \frac{f(x)}{g(x)} = \frac{f'g - g'f}{g^2}
\end{align*}

\begin{quote}
Mnemonic: Low d-high minus high d-low
\end{quote}

\end{proposition}

\begin{proposition}[Inverse Rule]

\begin{align*}
\dd{f^{-1}}{x}(f(x_0)) = \left( \dd{f}{x} \right)^{-1}(x_0) = 1/f'(x_0)
\end{align*}

\end{proposition}

\hypertarget{implicit-differentiation}{%
\subsection{Implicit Differentiation}\label{implicit-differentiation}}

\begin{align*}
(f(x))' = f'(x)~dx, (f(y))' = f'(y)~dy
\end{align*} - Often able to solve for \(\dd{y}{x}\) this way.

\begin{itemize}
\tightlist
\item
  Obtaining derivatives of inverse functions: if \(y = f^{-1}(x)\) then
  write \(f(y) = x\) and implicitly differentiate.
\end{itemize}

\hypertarget{related-rates}{%
\subsection{Related Rates}\label{related-rates}}

General series of steps: want to know some unknown rate \(y_t\)

\begin{itemize}
\tightlist
\item
  Lay out known relation that involves \(y\)
\item
  Take derivative implicitly (say w.r.t \(t\)) to obtain a relation
  between \(y_t\) and other stuff.
\item
  Isolate \(y_t = \text{ known stuff }\)
\end{itemize}

\begin{example}[?]

Example: ladder sliding down wall

\begin{itemize}
\tightlist
\item
  Setup: \(l, x_t\) and \(x(t)\) are known for a given \(t\), want
  \(y_t\).
  \begin{align*}
  x(t)^2 + y(t)^2 = l^2 \implies 2xx_t +2yy_t = 2ll_t = 0
  \end{align*} (noting that \(l\) is constant)

  \begin{itemize}
  \tightlist
  \item
    So \(y_t = -\frac{x(t)}{y(t)}x_t\)
  \item
    \(x(t)\) is known, so obtain \(y(t) = \sqrt{l^2 - x(t)^2}\) and
    solve.
  \end{itemize}
\end{itemize}

\end{example}

\hypertarget{integral-calculus}{%
\section{Integral Calculus}\label{integral-calculus}}

\hypertarget{average-values}{%
\subsection{Average Values}\label{average-values}}

\begin{proposition}[Integral formula for average value]

\begin{align*}
\mu_f = \frac{1}{b-a}\int_a^b f(t) dt
\end{align*}

\end{proposition}

\begin{proof}[?]

Apply MVT to \(F(x)\).

\end{proof}

\hypertarget{area-between-curves}{%
\subsection{Area Between Curves}\label{area-between-curves}}

Area in polar coordinates:
\begin{align*}
A = \int_{r_1}^{r_2} \frac{1}{2}r^2(\theta) ~d\theta
\end{align*}

\hypertarget{solids-of-revolution}{%
\subsection{Solids of Revolution}\label{solids-of-revolution}}

\begin{align*}  
\text{Disks} && A = \int \pi r(t)^2 ~dt \\
\text{Cylinders} && A = \int 2\pi r(t)h(t) ~dt
.\end{align*}

\hypertarget{arc-lengths}{%
\subsection{Arc Lengths}\label{arc-lengths}}

\begin{align*}
L 
&= \int ~ds  && ds = \sqrt{dx^2 + dy^2} \\
&= \int_{x_0}^{x_1}\sqrt{1 + \dd{y}{x}}~dx \\
&= \int_{y_0}^{y_1}\sqrt{\dd{x}{y} + 1}~dy
\end{align*}

\begin{align*}
SA = \int 2 \pi r(x) ~ds
\end{align*}

\hypertarget{center-of-mass}{%
\subsection{Center of Mass}\label{center-of-mass}}

Given a density \(\rho(\mathbf x)\) of an object \(R\), the \(x_i\)
coordinate is given by
\begin{align*}
x_i = 
\frac
{\displaystyle\int_R x_i\rho(x) ~dx}
{\displaystyle\int_R \rho(x)~dx}
\end{align*}

\hypertarget{big-list-of-integration-techniques}{%
\subsection{Big List of Integration
Techniques}\label{big-list-of-integration-techniques}}

Given \(f(x)\), we want to find an antiderivative \(F(x) = \int f\)
satisfying \(\frac{\partial}{\partial x}F(x) = f(x)\)

\begin{itemize}
\tightlist
\item
  Guess and check: look for a function that differentiates to \(f\).
\item
  \(u\dash\) substitution

  \begin{itemize}
  \tightlist
  \item
    More generally, any change of variables
    \begin{align*}
    x = g(u) \implies \int_a^b f(x)~dx = \int_{g^{-1}(a)}^{g^{-1}(b)} (f\circ g)(x) ~g'(x)~dx
    \end{align*}
  \end{itemize}
\end{itemize}

\hypertarget{integration-by-parts}{%
\subsubsection{Integration by Parts:}\label{integration-by-parts}}

The standard form:
\begin{align*}
\int u dv = uv - \int v du
\end{align*}

\begin{itemize}
\tightlist
\item
  A more general form for repeated applications: let
  \(v^{-1} = \int v\), \(v^{-2} = \int\int v\), etc.
  \begin{align*}
  \int_a^b uv &= uv^{-1}\bigg\rvert_a^b  - \int_a^b u^{1} v^{-1}\\
  &= uv^{-1} - u^1v^{-2}\bigg\rvert_a^b + \int_a^b u^2v^{-2} \\
  &= uv^{-1} - u^1v^{-2} + u^2v^{-3}\bigg\rvert_a^b - \int_a^b u^3v^{-3} \\
  &\quad\vdots \\
  \implies \int_a^b uv &= \sum_{k=1}^n (-1)^k u^{k-1}v^{-k} \bigg\rvert_a^b + (-1)^n\int_a^b u^nv^{-n} 
  \end{align*}
\item
  Generally useful when one term's \(n\)th derivative is a constant.
\end{itemize}

\hypertarget{shoelace-method}{%
\subsubsection{Shoelace Method}\label{shoelace-method}}

\begin{itemize}
\tightlist
\item
  Note: you can choose \(u\) or \(v\) equal to 1! Useful if you know the
  derivative of the integrand.
\end{itemize}

\begin{longtable}[]{@{}lllll@{}}
\toprule
Derivatives & Integrals & Signs & Result &\tabularnewline
\midrule
\endhead
\(u\) & \(v\) & NA & NA &\tabularnewline
\(u'\) & \(\int v\) & \(+\) & \(u\int v\) &\tabularnewline
\(u''\) & \(\int\int v\) & \(-\) & \(-u'\int\int v\) &\tabularnewline
\(\vdots\) & \(\vdots\) & \(\vdots\) & \(\vdots\) &\tabularnewline
\bottomrule
\end{longtable}

Fill out until one column is zero (alternate signs). Get the result
column by multiplying diagonally, then sum down the column.

\hypertarget{differentiating-under-the-integral}{%
\subsubsection{Differentiating under the
integral}\label{differentiating-under-the-integral}}

\begin{align*}
\frac{\partial}{\partial x} \int_{a(x)}^{b(x)} f(x, t) dt - \int_{a(x)}^{b(x)} \frac{\partial}{\partial x} f(x, t) dt 
&= f(x, \cdot)\frac{\partial}{\partial x}(\cdot) \bigg\rvert_{a(x)}^{b(x)} \\
&= f(x, b(x))~b'(x) - f(x, a(x))~a'(x)
\end{align*}

\begin{proof}[?]

Let \(F(x)\) be an antiderivative and compute \(F'(x)\) using the chain
rule.

\todo[inline]{
For constants, this should allow differentiating under the integral when \(f, f_x\) are "jointly continuous"
}

\begin{itemize}
\item
  LIPET: Log, Inverse trig, Polynomial, Exponential, Trig: generally let
  \(u\) be whichever one comes first.
\item
  The ridiculous trig sub: for any integrand containing only trig terms

  \begin{itemize}
  \item
    Transforms \emph{any} such integrand into a rational function of
    \(x\)
  \item
    Let \(u = 2\tan^{-1}x, ~du = \frac{2}{x^2+1}\), then
    \begin{align*}
    \int_a^b f(x)~dx = \int_{\tan\frac{a}{2}}^{\tan\frac{b}{2}} f(u)~du
    \end{align*}
  \end{itemize}
\end{itemize}

\end{proof}

\begin{example}[?]

\begin{align*}
\int_0^{\pi/2} \frac{1}{\sin \theta}~d\theta = 1/2
\end{align*}

\end{example}

\begin{itemize}
\tightlist
\item
  Trigonometric Substitution
  \begin{align*}
  \sqrt{a^2-x^2} && \Rightarrow && x = a\sin(\theta) &&dx = a\cos(\theta)~d\theta \\
  \sqrt{a^2+x^2} && \Rightarrow && x = a\tan(\theta) &&dx = a\sec^2(\theta)~d\theta \\
  \sqrt{x^2 - a^2} && \Rightarrow && x = a \sec(\theta) &&dx = a\sec(\theta)\tan(\theta)~d\theta
  \end{align*}
\end{itemize}

\hypertarget{partial-fractions}{%
\subsubsection{Partial Fractions}\label{partial-fractions}}

\hypertarget{trigonometric-substitution}{%
\subsubsection{Trigonometric
Substitution}\label{trigonometric-substitution}}

\todo[inline]{Completing the square}

\begin{itemize}
\item
  Trig Formulas
  \begin{align*}
  \sin^2(x) && = && \frac{1}{2}(1-2\cos x) \\
  && = && \\
  && = && \\
  && = && \\
  && = && \\
  \end{align*} \todo[inline]{
  Trig functions, double angle formulas.
  }
\item
  Products of trig functions

  \begin{itemize}
  \tightlist
  \item
    Setup: \(\int \sin^a(x) \cos^b(x) ~dx\)

    \begin{itemize}
    \tightlist
    \item
      Both \(a,b\) even: \(\sin(x)\cos(x) = \frac{1}{2} \sin(x)\)
    \item
      \(a\) odd: \(\sin^2 = 1-\cos^2,~u=\cos(x)\)
    \item
      \(b\) odd: \(\cos^2 = 1-\sin^2,~u=\sin(x)\)
    \end{itemize}
  \item
    Setup: \(\int \tan^a(x) \sec^b(x) ~dx\)

    \begin{itemize}
    \tightlist
    \item
      \(a\) odd: \(\tan^2 = \sec^2 - 1,~ u = \sec(x)\)
    \item
      \(b\) even: \(\sec^2 = \tan^2 - 1, u = \tan(x)\)
    \end{itemize}
  \end{itemize}
\end{itemize}

Other small but useful facts:
\begin{align*}  
\int_0^{2\pi} \sin \theta~d\theta = \int_0^{2\pi} \cos \theta~d\theta = 0
.\end{align*}

\hypertarget{optimization}{%
\subsection{Optimization}\label{optimization}}

\begin{itemize}
\item
  Critical points: boundary points and wherever \(f'(x) = 0\)
\item
  Second derivative test:

  \begin{itemize}
  \tightlist
  \item
    \(f''(p) > 0 \implies p\) is a min
  \item
    \(f''(p) < 0 \implies p\) is a max
  \end{itemize}
\item
  Inflection points of \(h\) occur where the \emph{tangent} of \(h'\)
  changes sign. (Note that this is where \(h'\) itself changes sign.)
\item
  Inverse function theorem: The slope of the inverse is reciprocal of
  the original slope
\item
  If two equations are equal at exactly one real point, they are tangent
  to each other there - therefore their derivatives are equal. Find the
  \(x\) that satisfies this; it can be used in the original equation.
\item
  Fundamental theorem of Calculus: If
  \begin{align*}  
  \int f(x) dx = F(b) - F(a) \implies F'(x) = f(x)
  .\end{align*}
\item
  Min/maxing - either derivatives of Lagranage multipliers!
\item
  Distance from origin to plane: equation of a plane
  \begin{align*}  
  P: ax+by+cz=d
  .\end{align*}

  \begin{itemize}
  \item
    You can always just read off the normal vector
    \(\vector{n} = (a,b,c)\). So we have \(\mathbf{n}\mathbf{x} = d\).
  \item
    Since \(\lambda \mathbf{n}\) is normal to \(P\) for all \(\lambda\),
    solve \(\mathbf{n}\lambda \mathbf{n} = d\), which is
    \(\lambda = \frac{d}{ \norm{\vector n}^2}\)
  \end{itemize}
\item
  A plane can be constructed from a point \(p\) and a normal \(n\) by
  the equation \(np = 0\).
\item
  In a sine wave \(f(x) = \sin(\omega x)\), the period is given by
  \(2\pi/\omega\). If \(\omega > 1\), then the wave makes exactly
  \(\omega\) full oscillations in the interval \([0, 2\pi]\).
\item
  The directional derivative is the gradient dotted against a \emph{unit
  vector} in the direction of interest
\item
  Related rates problems can often be solved via implicit
  differentiation of some constraint function
\item
  The second derivative of a parametric equation is not exactly what
  you'd intuitively think!
\item
  For the love of god, remember the FTC!
  \begin{align*}
  \frac{\partial}{\partial x} \int_0^x f(y) dy = f(x)
  \end{align*}
\item
  Technique for asymptotic inequalities: WTS \(f < g\), so show
  \(f(x_0) < g(x_0)\) at a point and then show
  \(\forall x > x_0, f'(x) < g'(x)\). Good for big-O style problems too.
\item
  Inflection points of \(h\) occur where the \emph{tangent} of \(h'\)
  changes sign. (Note that this is where \(h'\) itself changes sign.)
\item
  Inverse function theorem: The slope of the inverse is reciprocal of
  the original slope
\item
  If two equations are equal at exactly one real point, they are tangent
  to each other there - therefore their derivatives are equal. Find the
  \(x\) that satisfies this; it can be used in the original equation.
\item
  Fundamental theorem of Calculus: If
  \begin{align*}  
  \int f(x) dx = F(b) - F(a) \implies F'(x) = f(x)
  .\end{align*}
\item
  Min/maxing - either derivatives of Lagranage multipliers!
\item
  Distance from origin to plane: equation of a plane
  \begin{align*}  
  P: ax+by+cz=d
  .\end{align*}
\end{itemize}

\hypertarget{vector-calculus}{%
\section{Vector Calculus}\label{vector-calculus}}

\todo[inline]{Need lots of pictures}

Notation:
\begin{align*}  
\vector v, \vector a, \cdots 
&& \text{vectors in }\RR^n \\
\vector{R}, \vector{A}, \cdots 
&& \text{matrices} \\
\vector{r}(t) 
&& \text{A parameterized curve }\vector r: \RR\to \RR^n \\ \\
\hat{\vector v} 
&& {\vector v \over \norm{\vector v}}
.\end{align*}

\hypertarget{plane-geometry}{%
\subsection{Plane Geometry}\label{plane-geometry}}

\begin{proposition}[Slope of a vector in $\RR^2$]

\begin{align*}  
\vector{v} = [x, y] \in \RR^2 \implies  m = \frac{y}{x}
.\end{align*}

\end{proposition}

\begin{proposition}[Rotation matrices in $\RR^2$]

\begin{align*}  
\vector{R}_\theta = 
\left[ \begin{array} { l l } 
{ \cos \theta } & { - \sin \theta } \\ 
{ \sin \theta } & { \cos \theta } 
\end{array} \right] \implies \vector{R}_{\frac{\pi}{2}}
= \left[ \begin{array} { l l } 
{ 0 } & { - 1 } \\ 
{ 1 } & { 0 } 
\end{array}\right] 
.\end{align*}

\end{proposition}

\begin{corollary}[?]

\begin{align*}  
\vector{R}_{\frac{\pi}{2}} \vector x \da 
\vector{R}_{\frac{\pi}{2}} 
\begin{bmatrix} 
x \\ y 
\end{bmatrix} 
= 
\begin{bmatrix} 
-y \\ x 
\end{bmatrix} \in \vector \RR \vector x\perp
.\end{align*} Thus if a planar line is defined by the span of
\(\tv{x, y}\) and a slope of \(m = y/x\), a normal vector is given by
the span of \(\tv{-y, x}\) of slope \(-{1 \over m} = -x/y\).

\end{corollary}

\begin{example}[?]

Given \(\vector{v}\), the rotated vector
\(\vector{R}_{\frac{\pi}{2}} \vector v\) is orthogonal to \(\vector v\),
so this can be used to obtain normals and other orthogonal vectors in
the plane.

\end{example}

\begin{proposition}

There is a direct way to come up with one orthogonal vector to any given
vector:
\begin{align*}  
\vector v = [a,b,c] \implies \vector y \da 
\begin{cases}
\vector [-(b+c), a, a] 
& \vector v = [-1,-1,0], \\
[c,c, -(a+b)] 
& \text{else}
\end{cases}
\in \RR \vector v\perp
.\end{align*}

\end{proposition}

\hypertarget{projections}{%
\subsection{Projections}\label{projections}}

For a subspace given by a single vector \(\vector a\):
\begin{align*}
\mathrm{proj}_\vector{a}( \vecc{Aquamarine} x ) = \inner{\vecc{Aquamarine} x }{\vector a}\vector{\hat a}
\hspace{8em}
\mathrm{proj}_{\vector a}\perp(\vecc{Aquamarine} x) 
= \vecc{Aquamarine} x - \mathrm{proj}_\vector{a}(\vecc{Aquamarine} x) 
= \vecc{Aquamarine} x - \inner{\vecc{Aquamarine} x}{\vector a}\hat{\vector a}
\end{align*}

In general, for a subspace
\(\colspace(A) = \theset{\vector a_1, \cdots \vector a_n}\),

\begin{align*}
\mathrm{proj}_A(\vecc{Aquamarine} x) 
= \sum_{i=1}^n \inner{\vecc{Aquamarine}x }{\vector a_i}\vector{\hat a_i} = A(A^T A)^{-1}A^T\vecc{Aquamarine} x
\end{align*}

\hypertarget{lines}{%
\subsection{Lines}\label{lines}}

\begin{align*}  
\text{General Equation}
&& Ax + By + C = 0 \\ \\
\text{Parametric Equation}
&& \vector r(t) = t\vector x + \vector b 
.\end{align*}

Characterized by an equation in inner products:
\begin{align*}
\vector y \in L \iff \inner{\vector y}{\vector n} = 0
\end{align*}

\begin{proposition}[Equation for a line between two points]

Given \(\vector p_0, \vector p_1\), take
\(\vector x = \vector p_1 - \vector p_0\) and
\(\vector b = \vector p_i\) for either \(i\):
\begin{align*}  
\vector r(t) = t(\vector p_1 - \vector p_0) + \vector p_0 
&& = t\vector p_1 + (1-t) \vector p_0
.\end{align*}

\end{proposition}

\begin{proposition}[Symmetric equation of a line]

If a line \(L\) is given by
\begin{align*}
\vector r(t) = t \tv{x_1, x_2, x_3} + \tv{p_1, p_2, p_3}
,\end{align*} then
\begin{align*}
(x, y, z) \in L \iff \frac{x-p_1}{x_1} = \frac{y-p_{2}}{x_2} = \frac{z-p_{3}}{x_3}
.\end{align*}

\end{proposition}

\begin{example}[?]

The symmetric equation of the line through \(\tv{2,1,-3}\) and
\(\tv{1,4,-3}\) is given by
\begin{align*}  
\frac{x-2}{1}=\frac{y+1}{-5}=\frac{z-3}{6}
.\end{align*}

\end{example}

\hypertarget{tangent-lines-planes}{%
\subsubsection{Tangent Lines / Planes}\label{tangent-lines-planes}}

Key idea: just need a point and a normal vector, and the gradient is
normal to level sets.

\begin{theorem}[The Tangent Plane Equation]

For any locus \(f(\vector x) = 0\), we have
\begin{align*}  
\vector{x} \in T_f(\vector p) \implies \inner{\nabla f(\vector p)}{\vector x-\vector p} = 0 
.\end{align*}

\end{theorem}

\hypertarget{normal-lines}{%
\subsubsection{Normal Lines}\label{normal-lines}}

Key idea: the gradient is normal.

To find a normal line, you just need a single point \(\vector{p}\) and a
normal vector \(\vector n\); then
\begin{align*}  
L = \theset{\vector x \mid \vector x = \vector p + t\vector v}
.\end{align*}

\hypertarget{planes}{%
\subsection{Planes}\label{planes}}

\begin{align*}  
\text{General Equation}
&& A x + B y + C z + D = 0 \\ \\ 
\text{Parametric Equation}
&&\vector y(t,s) = t\vector x_1 + s\vector x_2  + \vector b \\ \\
.\end{align*}

Characterized by an equation in inner products:
\begin{align*}
\vector y \in P \iff \inner{\vector y - \vector p_0}{\vector n} = 0
\end{align*}

\begin{proposition}[Writing equation from a point and a normal]

Determined by a point \(\vector p_0\) and a normal vector \(\vector n\)

\end{proposition}

\begin{proposition}[Writing equation from two vectors]

Given \(\vector v_0, \vector v_1\), set
\(\vector n = \vector v_0 \cross \vector v_1\).

\end{proposition}

\hypertarget{finding-a-normal-vector}{%
\subsubsection{Finding a Normal Vector}\label{finding-a-normal-vector}}

\begin{itemize}
\tightlist
\item
  \textbf{Normal vector to a plane}

  \begin{itemize}
  \tightlist
  \item
    Can read normal off of equation: \(\vector n = [a,b,c]\)
  \end{itemize}
\item
  Computing \(D\):

  \begin{itemize}
  \tightlist
  \item
    \(D = \inner{\vector p_0}{\vector n} = p_1n_1 + p_2n_2 + p_3n_3\)
  \item
    Useful trick: once you have \(\vector n\), you can let
    \(\vector p_0\) be \emph{any} point in the plane (don't necessarily
    need to use the one you started with, so pick any point that's
    convenient to calculate)
  \end{itemize}
\end{itemize}

\hypertarget{distance-from-origin-to-plane}{%
\subsubsection{Distance from origin to
plane}\label{distance-from-origin-to-plane}}

\begin{itemize}
\tightlist
\item
  Given by
  \(D/ \norm{\vector n} = \inner{\vector p_0}{\vector{\hat n}}\). Gives
  a signed distance.
\end{itemize}

\todo[inline]{Distance from origin to plane.}

\hypertarget{distance-from-point-to-plane}{%
\subsubsection{Distance from point to
plane}\label{distance-from-point-to-plane}}

\begin{itemize}
\tightlist
\item
  Given by \(\inner{\wait}{\vector{\hat n}}\)
\item
  Finding vectors in the plane
\item
  Given \(P = [A, B, C] \cdot [x, y, z] = 0\), can take
  \(\tv{-\frac{B}{A},1,0}, \tv{-\frac{C}{A},0,1}\)
\end{itemize}

\todo[inline]{Distance from point to plane.}

\hypertarget{curves}{%
\subsection{Curves}\label{curves}}

\begin{align*}  
\vector r(t) = [x(t), y(t), z(t)]
.\end{align*}

\hypertarget{tangent-line-to-a-curve}{%
\subsubsection{Tangent line to a curve}\label{tangent-line-to-a-curve}}

We have an equation for the tangent vector at each point:
\begin{align*}  
\hat{ \vector{T} }(t) = \hat{\vector r'}(t)
,\end{align*} so we can write
\begin{align*}  
\vector L_{T}(t) 
= \vector r(t_0) + t \hat{ \vector T}(t_0)
\da \vector r(t_0) + t \hat{\vector{r}'}(t_0)
.\end{align*}

\hypertarget{normal-line-to-a-curve}{%
\subsubsection{Normal line to a curve}\label{normal-line-to-a-curve}}

\begin{itemize}
\tightlist
\item
  Use the fact that \(\vector{r}''(t) \in \RR \vector{r}'(t)\perp\), so
  we have an equation for a normal vector at each point:
  \begin{align*}  
  \hat{\vector N}(t) = \hat{\vector r''}(t)
  .\end{align*} Thus we can write
  \begin{align*}  
  \vector L_N(t) 
  = \vector r(t_0) + t \hat{ \vector{N} }(t_0)  
  = \vector r(t_0) + t \hat{ \vector r''} (t_0)
  .\end{align*}
\end{itemize}

\hypertarget{special-case-planar-graphs-of-functions}{%
\paragraph{Special case: planar graphs of
functions}\label{special-case-planar-graphs-of-functions}}

Suppose \(y = f(x)\). Set \(g(x, y) = f(x) - y\), then
\begin{align*}
\nabla g = [f_x(x), -1]\implies m = -\frac{1}{f_x(x)}
\end{align*}

\hypertarget{minimal-distances}{%
\subsection{Minimal Distances}\label{minimal-distances}}

Fix a point \(\vector p\). Key idea: find a subspace and project onto
it.

Key equations: projection and orthogonal projection of \(\vector b\)
onto \(\vector a\):
\begin{align*}
\mathrm{proj}_\vector{a}(\vector b) = \inner{\vector b}{\vector a}\vector{\hat a}
\hspace{8em}
\mathrm{proj}_{\vector a}\perp(\vector b) = \vector b - \mathrm{proj}_\vector{a}(\vector{b}) = \vector b - \inner{\vector b}{\vector a}\hat{\vector a}
\end{align*}

\hypertarget{point-to-plane}{%
\subsubsection{Point to plane}\label{point-to-plane}}

\begin{itemize}
\tightlist
\item
  Given a point \(\vector p\) and a plane
  \(S = \theset{\vector{x} \in \RR^3 \mid n_0x + n_1y + n_2z = d}\), let
  \(\vector n = [n_1, n_2, n_3]\), find any point \(\vector q \in S\),
  and project \(\vector q -\vector p\) onto
  \(S\perp = \mathrm{Span}(\vector n)\) using
\end{itemize}

\begin{align*}  
d = \norm{\mathrm{proj}_{\vector n}(\vector q - \vector p)} = \norm{\inner{\vector q - \vector p}{\vector n} \hat{\vector n}} = \inner{\vector q - \vector p}{\vector n}
.\end{align*}

\begin{itemize}
\tightlist
\item
  Given just two vectors \(\vector u, \vector v\): manufacture a normal
  vector \(\vector n = \vector u \times \vector v\) and continue as
  above.
\end{itemize}

\hypertarget{origin-to-plane}{%
\paragraph{Origin to plane}\label{origin-to-plane}}

Special case: if \(\vector p = \vector 0\),
\begin{align*}  
d = \norm{\mathrm{proj}_{\vector n}(\vector q)} = \norm{\inner{\vector p}{\vector n} \hat{\vector n}} = \inner{\vector p}{\vector n}.
.\end{align*}

\hypertarget{point-to-line}{%
\subsubsection{Point to line}\label{point-to-line}}

\begin{itemize}
\item
  Given a line \(L: \vector x(t) = t\vector v\) for some fixed
  \(\vector v\), use
  \begin{align*}  
  d = \norm{\mathrm{proj}_\vector{v}\perp(\vector{p})} = \norm{\vector p - \inner{\vector p}{\vector v}\hat{\vector v }}
  .\end{align*}
\item
  Given a line \(L: \vector x(t) = \vector w_0 + t\vector w\), let
  \(\vector v = \vector x(1) - \vector x(0)\) and proceed as above.
\end{itemize}

\hypertarget{point-to-curve}{%
\subsubsection{Point to curve}\label{point-to-curve}}

\todo[inline]{todo}

\hypertarget{line-to-line}{%
\subsubsection{Line to line}\label{line-to-line}}

Given \(\vector{r}_1(t) = \vector p_1 + t\vector v_2\) and
\(\vector{r}_2(t) = \vector p_2 + t\vector v_2\), let \(d\) be the
desired distance.

\begin{itemize}
\item
  Let \(\hat{ \vector n} = \hat{\vector v_1 \times \vector v_2}\), which
  is orthogonal to both lines.
\item
  Then project the vector connecting the two fixed points
  \(\vector p_i\) onto this subspace and take its norm:
  \begin{align*}
  d 
  &= \norm{\mathrm{proj}_{\vector n}(\vector p_2 - \vector p_1)} \\
  &= \norm{\inner{\vector p_2 -\vector p_1}{\vector n}\hat{\vector n}} \\
  &= \inner{\vector p_2 - \vector p_1}{\vector n} \\
  &\da \inner{\vector p_2 - \vector p_1}{\vector v_1 \cross \vector v_2}
  .\end{align*}
\end{itemize}

\hypertarget{surfaces}{%
\subsection{Surfaces}\label{surfaces}}

\begin{align*}
S = \theset{(x,y,z) \mid f(x,y, z) = 0} \hspace{10em} z = f(x,y)
\end{align*}

\hypertarget{tangent-plane-to-a-surface}{%
\subsubsection{Tangent plane to a
surface}\label{tangent-plane-to-a-surface}}

\begin{itemize}
\tightlist
\item
  Need a point \(\vector{p}\) and a normal \(\vector{n}\). By cases:
\item
  \(f(x,y, z) = 0\)

  \begin{itemize}
  \tightlist
  \item
    \(\nabla f\) is a normal vector.
  \item
    Write the tangent plane equation
    \(\inner{\vector n}{\vector x - \vector p_0}\), done.
  \end{itemize}
\item
  \(z = g(x,y)\):

  \begin{itemize}
  \tightlist
  \item
    Let \(f(x, y, z) = g(x,y) - z\), then
    \(\vector p \in S \iff \vector p\) is in a level set of \(f\).
  \item
    \(\nabla f\) is normal to level sets (and thus the surface), so
    compute \(\nabla f = [g_x, g_y, -1]\)
  \item
    Proceed as in previous case
  \end{itemize}
\end{itemize}

\hypertarget{surfaces-of-revolution}{%
\subsubsection{Surfaces of revolution}\label{surfaces-of-revolution}}

\begin{itemize}
\tightlist
\item
  Given \(f(x_1 ,x_2) = 0\), can be revolved around either the \(x_1\)
  or \(x_2\) axis.

  \begin{itemize}
  \tightlist
  \item
    \(f(x,y)\) around the \(x\) axis yields
    \(f(x, \pm \sqrt{y^2 + z^2})=0\)
  \item
    \(f(x,y)\) around the \(y\) axis yields
    \(f(\pm\sqrt{x^2 + z^2}, y)=0\)
  \item
    Remaining cases proceed similarly - leave the axis variable alone,
    replace other variable with square root involving missing axis.
  \end{itemize}
\item
  Equations of lines tangent to an intersection of surfaces
  \(f(x,y,z) = g(x,y,z)\):

  \begin{itemize}
  \tightlist
  \item
    Find two normal vectors and take their cross product,
    e.g.~\(n = \nabla f \times \nabla g\), then
    \begin{align*}
    L = \theset{\vector x\mid \vector x = \vector p + t \vector n}
    \end{align*}
  \end{itemize}
\item
  Level curves:

  \begin{itemize}
  \tightlist
  \item
    Given a surface \(f(x,y,z) = 0\), the level curves are obtained by
    looking at e.g.~\(f(x,y,c) = 0\).
  \end{itemize}
\end{itemize}

\hypertarget{multivariable-calculus}{%
\section{Multivariable Calculus}\label{multivariable-calculus}}

\begin{theorem}[Key Theorem]

Given a function \(f: \RR^n \to \RR\), let
\(S_k \da \ts{\vector p\in \RR^n \st f(\vector p) = k}\) denote the
level set for \(k\in \RR\). Then
\begin{align*}  
\nabla f(\vector p) \in S_k\perp
.\end{align*}

\end{theorem}

\hypertarget{notation}{%
\subsection{Notation}\label{notation}}

\begin{align*}
\vector{v} &= [v_1, v_2, \cdots] 
&& \text{a vector} \\ \\
\vector{e}_i &= [0, 0, \cdots, \overbrace{1}^{i \text{th term}}, \cdots, 0] 
&& \text{the } i \text{th standard basis vector} \\ \\
\phi: \RR^n &\to \RR 
&& \text{a functional on } \RR^n\\
\phi(x_1, x_2, \cdots) &= \cdots  && \\ \\
\mathbf{F}: \RR^n &\to \RR^n 
&& \text{a multivariable function}\\
\mathbf{F}(x_1,x_2,\cdots) &= 
[\mathbf{F}_1(x_1, x_2, \cdots), \mathbf{F}_2(x_1, x_2, \cdots), \cdots, \mathbf{F}_n(x_1, x_2, \cdots)]
\end{align*}

\hypertarget{partial-derivatives}{%
\subsection{Partial Derivatives}\label{partial-derivatives}}

\begin{definition}[Partial Derivative]

For a functional \(f:\RR^n\to \RR\), the \textbf{partial derivative} of
\(f\) with respect to \(x_i\) is
\begin{align*}
\dd{f}{x_i}(\mathbf p) 
\da \lim_{h\to 0}\frac{f(\mathbf p + h\mathbf e_i)  - f(\mathbf p)}{h}
\end{align*}

\end{definition}

\begin{example}[$n= 2$]

\begin{align*}
f: \RR^2 &\to \RR \\
\dd{f}{x}(x_0,y_0) 
&= \lim_{h \to 0} \frac{f(x_0+h, y_0) - f(x_0,y_0)}{h}
\end{align*}

\end{example}

\hypertarget{general-derivatives}{%
\subsection{General Derivatives}\label{general-derivatives}}

\begin{definition}[General definition of differentiability]

A function \(f: \RR^n \to \RR^m\) is \textbf{differentiable} iff there
exists a linear transformation \(D_f: \RR^n \to \RR^m\) such that the
following limit exists
\begin{align*}  
\lim _ { \mathbf x \rightarrow \vector{p} } \frac { \left\| f (\mathbf x ) - f (\vector{p} ) - D_f (\mathbf x - \vector{p} ) \right\| } { \| \mathbf x - \vector{p} \| } = 0
.\end{align*}

\end{definition}

\begin{remark}

\(D_f\) is the ``best linear approximation'' to \(f\).

\end{remark}

\begin{definition}[Jacobian]

When \(f\) is differentiable, \(D_f\) can be given in coordinates by\\
\begin{align*}
(D_f)_{ij} = \dd{f_i}{x_j}
\end{align*}

This yields the \textbf{Jacobian} of \(f\):
\begin{align*}
D_f(\vector \vector p) 
\begin{bmatrix}
\vertbar & \vertbar & & \vertbar \\
\nabla f_1(\vector p) & \nabla f_2(\vector p) & \cdots & \nabla f_m(\vector p) \\
\vertbar & \vertbar & & \vertbar
\end{bmatrix}^T
= \left[ \begin{array} { c c c c } { \frac { \partial f _ { 1 } } { \partial x _ { 1 } } ( \vector{p} ) } & { \frac { \partial f _ { 1 } } { \partial x _ { 2 } } ( \vector{p} ) } & { \ldots } & { \frac { \partial f _ { 1 } } { \partial x _ { n } } ( \vector{p} ) } \\ { \frac { \partial f _ { 2 } } { \partial x _ { 1 } } ( \vector{p} ) } & { \frac { \partial f _ { 2 } } { \partial x _ { 2 } } ( \vector{p} ) } & { \dots } & { \frac { \partial f _ { 2 } } { \partial x _ { n } } ( \vector{p} ) } \\ { \vdots } & { \vdots } & { \ddots } & { \vdots } \\ { \frac { \partial f _ { m } } { \partial x _ { 1 } } ( \vector{p} ) } & { \frac { \partial f _ { m } } { \partial x _ { 2 } } ( \vector{p} ) } & { \cdots } & { \frac { \partial f _ { m } } { \partial x _ { n } } ( \vector{p} ) } \end{array} \right].
\end{align*}

\end{definition}

\begin{remark}

This is equivalent to

\begin{itemize}
\tightlist
\item
  Taking the gradient of each component \(f_i\) of \(f\),
\item
  Evaluating \(\nabla f_i\) at \(\vector p\),
\item
  Forming a matrix using these as the columns, and
\item
  Transposing the resulting matrix.
\end{itemize}

\end{remark}

\begin{definition}[Hessian]

For a function \(f: \RR^n \to \RR\), the \textbf{Hessian} is a
generalization of the second derivative, and is given in coordinates by
\begin{align*}
(H_f)_{ij} = \dd{^2f}{x_i x_j}
\end{align*}

Explicitly, we have
\begin{align*}
H_f(\vector p) 
=
\begin{bmatrix}
\vertbar & \vertbar & & \vertbar \\
D \nabla f_1(\vector p) & D\nabla f_2(\vector p) & \cdots & D\nabla f_m(\vector p) \\
\vertbar & \vertbar & & \vertbar
\end{bmatrix}^T
=
\left[ \begin{array} { c c c } { \frac { \partial ^ { 2 } f } { \partial x _ { 1 } \partial x _ { 1 } } ( \mathbf { a } ) } & { \dots } & { \frac { \partial ^ { 2 } f } { \partial x _ { 1 } \partial x _ { n } } ( \mathbf { a } ) } \\ { \vdots } & { \ddots } & { \vdots } \\ { \frac { \partial ^ { 2 } f } { \partial x _ { n } \partial x _ { 1 } } ( \mathbf { a } ) } & { \cdots } & { \frac { \partial ^ { 2 } f } { \partial x _ { n } \partial x _ { n } } ( \mathbf { a } ) } \end{array} \right].
\end{align*}

\end{definition}

\begin{remark}

Mnemonic: make matrix with \(\nabla f\) as the columns, and then
differentiate variables left to right.

\end{remark}

\hypertarget{the-chain-rule}{%
\subsection{The Chain Rule}\label{the-chain-rule}}

\begin{example}[How to expand a partial derivative]

Write out tree of dependent variables:

\begin{center}\includesvg[width=0.7\linewidth]{e9022004f122ca59f186d1990bb1a4cf2c24f643}\end{center}

Then sum each possible path.

Let subscripts denote which variables are held constant, then
\begin{align*}
\left(\dd{z}{x}\right)_y &= \left(\dd{z}{x}\right)_{u,y,v} \\
& + \left(\dd{z}{v}\right)_{x,y,u} \left(\dd{v}{x}\right)_y \\
& + \left(\dd{z}{u}\right)_{x,y,v} \left(\dd{u}{x}\right)_{v,y}  \\
& + \left(\dd{z}{u}\right)_{x,y,v} \left(\dd{u}{v}\right)_{x,y} \left(\dd{v}{x}\right)_y
\end{align*}

\end{example}

\hypertarget{approximation}{%
\subsection{Approximation}\label{approximation}}

Let \(z = f(x,y)\), then to approximate near
\(\vector p_0 = \tv{x_0, y_0}\),
\begin{align*}  
f(\vecc{Aquamarine} x) &\approx f(\vector p) + \nabla f (\vecc{Aquamarine} x - \vector p_0) \\
\implies f(x,y) &\approx f(\vector p) + f_x(\vector p)(x-x_0) + f_y(\vector p)(y-y_0) \\
.\end{align*}

\hypertarget{optimization-1}{%
\subsection{Optimization}\label{optimization-1}}

\hypertarget{classifying-critical-points}{%
\subsubsection{Classifying Critical
Points}\label{classifying-critical-points}}

\begin{definition}[Critical Points]

Critical points of \(f\) given by points \(\vector p\) such that the
derivative vanishes:
\begin{align*}
\crit(f) = \ts{\vector p\in \RR^n \st D_f({\mathbf p}) = 0}
\end{align*}

\end{definition}

\begin{proposition}[Second Derivative Test]

\envlist
1. Compute
\begin{align*}
  \abs{H_f(\mathbf p)} \definedas  \left| \begin{array} { l l } { f _ { x x } } & { f _ { x y } } \\ { f _ { y x } } & { f _ { y y } } \end{array} \right| ({ \mathbf p  })
  \end{align*} 2. Check by cases:

\begin{itemize}
\tightlist
\item
  \(\abs{H(\mathbf p)} = 0\): No conclusion
\item
  \(\abs{H(\mathbf p)} < 0\): Saddle point
\item
  \(\abs{H(\mathbf p)} > 0\):

  \begin{itemize}
  \tightlist
  \item
    \(f_{xx}(\mathbf p) > 0 \implies\) local min
  \item
    \(f_{xx}(\mathbf p) < 0 \implies\) local max
  \end{itemize}
\end{itemize}

\end{proposition}

\begin{remark}

What's really going on?

\begin{itemize}
\item
  Eigenvalues have same sign \(\iff\) positive definite or negative
  definite

  \begin{itemize}
  \item
    Positive definite \(\implies\) convex \(\implies\) local min
  \item
    Negative definite \(\implies\) concave \(\implies\) local max
  \end{itemize}
\end{itemize}

\end{remark}

\begin{itemize}
\tightlist
\item
  Extrema occur on boundaries, so parameterize each boundary to obtain a
  function in one less variable and apply standard optimization
  techniques to yield critical points. Test all critical points to find
  extrema.
\item
  If possible, use constraint to just reduce equation to one dimension
  and optimze like single-variable case.
\end{itemize}

\todo[inline]{Add examples}

\hypertarget{lagrange-multipliers}{%
\subsubsection{Lagrange Multipliers}\label{lagrange-multipliers}}

The setup:
\begin{align*} 
\text{Optimize } f(\mathbf x) &\quad \text{subject to } g(\mathbf x) = c \\ \implies \nabla f &= \lambda \nabla g
\end{align*} 1. Use this formula to obtain a system of equations in the
components of \(x\) and the parameter \(\lambda\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Use \(\lambda\) to obtain a relation involving only components of
  \(\mathbf{x}\).
\item
  Substitute relations \textbf{back into constraint} to obtain a
  collection of critical points.
\item
  Evaluate \(f\) at critical points to find max/min.
\end{enumerate}

\todo[inline]{Add examples}

\hypertarget{change-of-variables}{%
\subsection{Change of Variables}\label{change-of-variables}}

For any \(f: \RR^n \to \RR^n\) and region \(R\),
\begin{align*}
\int _ { g ( R ) } f ( \mathbf { x } ) ~d V = \int _ { R } (f  \circ g) ( \mathbf { x } )  \cdot \abs{D_g  ( \mathbf { x })} ~d V
\end{align*}

\hypertarget{vector-calculus-1}{%
\section{Vector Calculus}\label{vector-calculus-1}}

\hypertarget{notation-1}{%
\subsection{Notation}\label{notation-1}}

\(R\) is a region, \(S\) is a surface, \(V\) is a solid.

\begin{align*}  
\oint _ { \partial S } \mathbf { F } \cdot d \mathbf { r } =    
\oint _ { \partial S } [\mathbf{F}_1, \mathbf{F}_2, \mathbf{F}_3] \cdot [dx, dy, dz] 
= \oint_{\del S} \mathbf{F}_1dx + \mathbf{F}_2dy + \mathbf{F}_3dz
\end{align*}

The main vector operators
\begin{align*}
\nabla: (\RR^n \to \RR) &\to (\RR^n \to \RR^n) \\
\phi &\mapsto \nabla \phi \definedas \sum_{i=1}^n \frac{\partial \phi}{\partial x_i} ~\mathbf{e}_i \\ \\
\text{}
\mathrm{div}(\mathbf{F}): (\RR^n \to \RR^n) &\to (\RR^n \to \RR) \\
\mathbf{F} &\mapsto \nabla \cdot \mathbf{F}
\definedas \sum_{i=1}^n \frac{\partial \mathbf{F}_i}{\partial x_i} \\ \\
\text{}
\mathrm{curl}(\mathbf{F}): (\RR^3 \to \RR^3) &\to (\RR^3 \to \RR^3) \\
\mathbf{F} &\mapsto \nabla \cross \mathbf{F} \\ \\
\text{}
\end{align*} Some terminology:
\begin{align*}
\text{Scalar Field} && \phi:&~ X \to \RR \\
\text{Vector Field} && \mathbf{F}:&~ X\to \RR^n\\
\text{Gradient Field} && \mathbf{F}:&~ X \to \RR^n \mid \exists \phi: X\to \RR \mid \nabla \phi = F
\end{align*}

\begin{itemize}
\tightlist
\item
  The Gradient: lifts scalar fields on \(\RR^n\) to vector fields on
  \(\RR^n\)
\item
  Divergence: drops vector fields on \(\RR^n\) to scalar fields on
  \(\RR^n\)
\item
  Curl: takes vector fields on \(\RR^3\) to vector fields on \(\RR^3\)
\end{itemize}

\begin{align*}
\mathbf x \cdot \mathbf y
= \inner{\mathbf x}{\mathbf y}
= \sum_{i=1}^n {x_i y_i} = x_1y_1 + x_2y_2 + \cdots
&& \text{inner/dot product}
\\
\norm{\mathbf x} = \sqrt{\inner{\mathbf x}{\mathbf x}}
= \sqrt{\sum_{i=1}^n x_i^2}
= \sqrt{x_1^2 + x_2^2 + \cdots}
&& \text{norm}
\\
\mathbf a \cross \mathbf b
= \mathbf{\hat n} \norm{\mathbf a}\norm{\mathbf b}\sin\theta_{\mathbf a,\mathbf b}
= \left| \begin{array}{ccc}
\mathbf{\hat x} & \mathbf{\hat y} & \mathbf{\hat z} \\
a_1 & a_2 & a_3 \\
b_1 & b_2 & b_3
\end{array}\right|
&& \text{cross product} \\ \\
D_\mathbf{u}(\phi) = \nabla \phi \cdot \mathbf{\hat u}
&& \text{directional derivative} \\ \\
\nabla
\definedas \sum_{i=1}^n \frac{\partial}{\partial x_i} \mathbf{e}_i
= \left[\frac{\partial}{\partial x_1}, \frac{\partial}{\partial x_2}, \cdots, \frac{\partial}{\partial x_n}\right]
&& \text{del operator}
\\ \\
\nabla \phi
\definedas \sum_{i=1}^n \frac{\partial \phi}{\partial x_i} ~\mathbf{e}_i
= \tv{ \frac{\partial \phi}{\partial x_1}, \frac{\partial \phi}{\partial x_2}, \cdots, \frac{\partial \phi}{\partial x_n}}
&& \text{gradient} \\ \\
\Delta \phi \definedas \nabla\cdot\nabla \phi \definedas \sum_{i=1}^n \frac{\partial^2 \phi}{\partial x_i^2}
= \frac{\partial^2 \phi}{\partial x_1^2} + \frac{\partial^2 \phi}{\partial x_2} + \cdots + \frac{\partial^2 \phi}{\partial x_n^2}
&& \text{Laplacian}
\\ \\
\nabla \cdot \mathbf{F}
\definedas \sum_{i=1}^n \frac{\partial \mathbf{F}_i}{\partial x_i} = \frac{\partial \mathbf{F}_1}{\partial x_1} + \frac{\partial \mathbf{F}_2}{\partial x_2} + \cdots + \frac{\partial \mathbf{F}_n}{\partial x_n}
&& \text{divergence} \\ \\
\nabla \times \mathbf { F }
= \left| \begin{array} { c c c } { \mathbf { e }_1 } & { \mathbf { e }_2 } & { \mathbf { e }_3 } \\ { \frac { \partial } { \partial x } } & { \frac { \partial } { \partial y } } & { \frac { \partial } { \partial z } } \\ { \mathbf{F} _ { 1 } } & { \mathbf{F} _ { 2 } } & { \mathbf{F} _ { 3 } } \end{array} \right|  = [\mathbf{F}_{3y} - \mathbf{F}_{2z}, \mathbf{F}_{1z}- \mathbf{F}_{3x}, \mathbf{F}_{2x} -\mathbf{F}_{1y}]
&& \text{curl}
\\
\iint _ { S } ( \nabla \times \mathbf { F } ) \cdot d \mathbf { S } = \iint _ { S } ( \nabla \times \mathbf { F } ) \cdot \mathbf { n } ~dS
&& \text{surface integral}
\end{align*}

\hypertarget{big-theorems}{%
\subsection{Big Theorems}\label{big-theorems}}

\hypertarget{stokes-and-consequences}{%
\subsubsection{Stokes' and Consequences}\label{stokes-and-consequences}}

\begin{theorem}[Stokes' Theorem]

\begin{align*}  
\oint _ { \partial S } \mathbf { F } \cdot d \mathbf { r } = \iint _ { S } ( \nabla \times \mathbf { F } ) \cdot d \mathbf { S }
.\end{align*}

\end{theorem}

\begin{remark}

Note that if \(S\) is a closed surface, so \(\del S = \emptyset\), this
integral vanishes.

\end{remark}

\begin{corollary}[Green's Theorem]

\begin{align*}  
\oint _ { \del R } ( L ~d x + M ~d y ) = \iint _ { R } \left( \frac { \partial M } { \partial x } - \frac { \partial L } { \partial y } \right) d x d y
.\end{align*}

\end{corollary}

\begin{proof}[?]

Recovering Green's Theorem from Stokes' Theorem:

Let \(\mathbf{F} = [L, M, 0]\), then
\(\nabla\cross \mathbf{F} = [0, 0, \frac{\partial M}{\partial x} - \frac{\partial L}{\partial y}]\)

\end{proof}

\begin{corollary}[Divergence Theorem]

\begin{align*}  
\iint_ { \partial V } \mathbf { F } \cdot d \mathbf { S } = \iiint _ { V } ( \nabla \cdot \mathbf { F } ) ~d V
.\end{align*}

\end{corollary}

\begin{remark}

\envlist

\begin{itemize}
\tightlist
\item
  \(\nabla\cross(\nabla\phi) = 0\)
\item
  \(\nabla\cdot(\nabla\cross \mathbf{F}) = 0\)
\end{itemize}

\end{remark}

\hypertarget{directional-derivatives}{%
\subsubsection{Directional Derivatives}\label{directional-derivatives}}

\begin{definition}[Directional Derivative]

\begin{align*}  
D_{\vector v} f(\vector p) \da \dd{f}{t}(\vector p + t\vector v) \evalfrom_{t=0}
.\end{align*}

\end{definition}

\begin{remark}

Note that the directional derivative uses a normalized direction vector!

\end{remark}

\begin{theorem}[Dot product expression of directional derivative]

Suppose \(f:\RR^n\to \RR\) and \(\vector v\in \RR^n\). Then
\begin{align*}  
D_{\vector v}f(\vector p) = \inner{\nabla f(\vector p)}{\vector v}
.\end{align*}

\end{theorem}

\begin{proof}[?]

We first use the fact that we can find \(L\), the best linear
approximation to \(f\):
\begin{align*}  
L(\vector x) &\da f(\vector p) + D_f(\vector p)(\vector x - \vector p) \\ \\ 
D_{\vector v}f(\vector p)
&= D_{\vector v} L(\vector p) \\
&= \lim_{t\to 0} {L(\vector p + t\vector v) - L(\vector p) \over t}\\
&= \lim_{t\to 0} {
f(\vector p) + D_f(\vector p)(\vector p + t\vector v - \vector p)
-\qty{f(\vector p) + D_f(\vector p)(\vector p - \vector p)}
\over t
}\\
&= \lim_{t\to 0} { D_f(\vector p)(t\vector v) \over t} \\
&= D_f(\vector p)\vector v \\
&\da \nabla f(\vector p) \cdot \vector v
.\end{align*}

\todo[inline]{Need a better proof, not clear that this works.}

\end{proof}

\hypertarget{computing-integrals}{%
\subsection{Computing Integrals}\label{computing-integrals}}

\hypertarget{changing-coordinates}{%
\subsubsection{Changing Coordinates}\label{changing-coordinates}}

\hypertarget{multivariable-chain-rule}{%
\paragraph{Multivariable Chain Rule}\label{multivariable-chain-rule}}

\todo[inline]{todo}

\hypertarget{polar-and-cylindrical-coordinates}{%
\paragraph{Polar and Cylindrical
Coordinates}\label{polar-and-cylindrical-coordinates}}

\begin{align*}
x = r\cos\theta \\
y = r\sin\theta \\
dV \mapsto r \quad dr~d\theta
\end{align*}

\hypertarget{spherical-coordinates}{%
\paragraph{Spherical Coordinates}\label{spherical-coordinates}}

\begin{align*}
x = r\cos\theta = \rho\sin\phi\cos\theta \\
y = r\sin\theta = \rho\sin\phi\sin\theta \\
dV \mapsto r^2 \sin\phi \quad dr~d\phi~d\theta
\end{align*}

\hypertarget{line-integrals}{%
\subsubsection{Line Integrals}\label{line-integrals}}

\hypertarget{curves-1}{%
\paragraph{Curves}\label{curves-1}}

\begin{itemize}
\tightlist
\item
  Parametrize the path \(C\) as \(\theset{\mathbf{r}(t): t\in[a,b]}\),
  then
\end{itemize}

\begin{align*}
\int_C f ~ds
&\definedas \int_a^b (f\circ \mathbf{r})(t) ~\norm{\mathbf{r}'(t)}~dt \\
&= \int_a^b f(x(t), y(t), z(t)) \sqrt{x_t^2 + y_t^2 + z_t^2} ~dt
\end{align*}

\hypertarget{vector-fields}{%
\paragraph{Vector Fields}\label{vector-fields}}

\begin{itemize}
\tightlist
\item
  If exact:
  \begin{align*}
  \dd{}{y} \mathbf{F_1} = \dd{}{x} \mathbf{F_2} \implies
  \int \mathbf{F_1} ~dx + \mathbf{F_2} ~dy = \phi(\mathbf{p_1}) - \phi(\mathbf{p_0})
  \end{align*}
\end{itemize}

The function \(\phi\) can be found using the same method from ODEs.

\begin{itemize}
\item
  Parametrize the path \(C\) as \(\theset{\mathbf{r}(t): t\in[a,b]}\),
  then
  \begin{align*}
  \int_C \mathbf F \cdot d\mathbf r
  & \definedas \int_a^b (\mathbf F \circ \mathbf r)(t) \cdot \mathbf r'(t) ~dt \\
  &= \int_a^b [\mathbf F_1(x(t), y(t), \cdots), \mathbf F_2(x(t), y(t), \cdots)]\cdot[x_t, y_t, \cdots] ~dt \\
  &= \int_a^b \mathbf F_1(x(t), y(t) \cdots)x_t + \mathbf F_2(x(t), y(t), \cdots)y_t + \cdots ~dt
  \end{align*}
\item
  Equivalently written:
\end{itemize}

\begin{align*}
\int_a^b \mathbf F_1 ~dx + \mathbf F_2 ~dy + \cdots
\definedas \int_C \mathbf F \cdot d\mathbf r
\end{align*} in which case
\([dx, dy, \cdots] \definedas [x_t, y_t, \cdots] = \mathbf r'(t)\).

\begin{itemize}
\tightlist
\item
  \emph{Remember to substitute dx back into the integrand!!}
\end{itemize}

\hypertarget{flux}{%
\subsubsection{Flux}\label{flux}}

\begin{align*}  
\iint_S \mathbf{F}\cdot d\mathbf{S} = \iint_S \mathbf{F}\cdot \mathbf{\hat n} ~dS
.\end{align*}

\hypertarget{area}{%
\subsubsection{Area}\label{area}}

\begin{proposition}[Areas can be computed with Green's Theorem]

Given \(R\) and \(f(x,y) = 0\),
\begin{align*}  
A(R) = \oint _ { \del R } x ~d y = - \oint _ { \del R } y ~d x = \frac { 1 } { 2 } \oint _ { \del R } - y ~d x + x ~d y
.
\end{align*}

\end{proposition}

\begin{proof}[?]

Compute
\begin{align*}
\oint_{\del R} x ~dy = - \oint_{\del R} y ~dx \\
= \frac{1}{2} \oint_{\del R} -y~dx + x~dy
= \frac{1}{2} \iint_R 1 - (-1) ~dA   
=\iint_R 1 ~dA 
\end{align*}

\end{proof}

\hypertarget{surface-integrals}{%
\subsubsection{Surface Integrals}\label{surface-integrals}}

\begin{itemize}
\tightlist
\item
  For a paramterization \(\mathbf r(s,t): U \to S\) of a surface \(S\)
  and any function \(f: \RR^n \to \RR\),
  \begin{align*}
  \iint _ { S } f ~dA = \iint _ { U } ( f \circ \mathbf r) ( s , t )~\norm{\mathbf n}  ~dA
  \end{align*}
\item
  Can obtain a normal vector \(\mathbf n = T _ { u } \times T _ { v }\)
\end{itemize}

\hypertarget{other-results}{%
\subsection{Other Results}\label{other-results}}

\begin{example}[?]

\(\nabla \cdot \mathbf{F} = 0 \not \implies \exists G:~ \mathbf{F} = \nabla\cross G\).
A counterexample

\begin{align*}
\mathbf{F}(x,y,z) =\frac{1}{\sqrt{x^2+y^2+z^2}}[x, y, z]~,\quad S = S^2 \subset \RR^3 \\ 
\implies \nabla \mathbf{F} = 0 \text{ but } \iint_{S^2}\mathbf{F}\cdot d\mathbf{S} = 4\pi \neq 0
\end{align*} Where by Stokes' theorem,
\begin{align*}
\mathbf{F} 
= \nabla\cross\mathbf{G}\implies
\iint_{S^2} \mathbf{F} 
&= \iint_{S^2} \nabla\cross\mathbf{G}  \\ \\
&= \oint_{\del S^2}\mathbf{G}~d\mathbf{r}  && \text{by Stokes}\\
&= 0
\end{align*} since \(\del S^2 = \emptyset\).

\end{example}

\begin{proposition}[Sufficient Conditions]

Sufficient condition: if \(\mathbf{F}\) is everywhere \(C^1\),
\begin{align*}
\exists \mathbf{G}:~ \mathbf{F} 
= \nabla \cross \mathbf{G} 
\iff \iint_S \mathbf{F}\cdot d\mathbf{S} = 0 \text{ for all closed surfaces }S
.\end{align*}

\end{proposition}

\hypertarget{linear-algebra}{%
\section{Linear Algebra}\label{linear-algebra}}

\begin{remark}

The underlying field will be assumed to be \(\RR\) for this section.

\end{remark}

\hypertarget{notation-2}{%
\subsection{Notation}\label{notation-2}}

\begin{align*}  
\mat(m, n) && \text{the space of all } m\times n\text{ matrices} \\
T && \text{a linear map } \RR^n \to \RR^m \\
A\in \mat(m, n)&& \text{an } m\times n \text{ matrix representing }T \\
A^t\in \mat(n, m) && \text{an } n\times m \text{ transposed matrix} \\
\vector{a} && \text{a } 1\times n \text{ column vector} \\
\vector{a}^t && \text{an } n\times 1 \text{ row vector} \\
A = \tv{\vector a_1, \cdots, \vector a_n} && \text{a matrix formed with } \vector a_i \text{ as the columns} \\
V, W && \text{vector spaces} \\
|V|, \dim(W) && \text{dimensions of vector spaces} \\
\det(A) && \text{the determinant of }A \\
\begin{bmatrix}
A &\aug& \vector b
\end{bmatrix}
\da \thevector{\vector a_1, \vector a_2, \cdots \vector a_n, \vector b} && \text{augmented matrices} \\
\begin{bmatrix}
A &\aug& B
\end{bmatrix}
\da \thevector{\vector a_1, \cdots \vector a_n, \vector b_1, \cdots, \vector b_m} && \text{block matrices}\\
\spec(A) && \text{the multiset of eigenvalues of } A \\
A\vector x = \vector b && \text{a system of linear equations} \\
r\da \rank(A) && \text{the rank of }A\\
r_b = \rank\qty{ 
\begin{bmatrix}
A &\aug& \vector b \\
\end{bmatrix}
} && \text{the rank of }A\text{ augmented by }\vector b
.\end{align*}

\hypertarget{big-theorems-1}{%
\subsection{Big Theorems}\label{big-theorems-1}}

\begin{theorem}[Rank-Nullity]

\begin{align*}  
\abs{\ker(A)} + \abs{\im(A)} = \abs{\dom(A)}
,\end{align*} where
\(\nullity(A) = \abs{\im{A}}, \rank(A) = \abs{\im(A)},\) and \(n\) is
the number of columns in the corresponding matrix.

Generalization: the following sequence is always exact:
\begin{align*}
0 \to \ker(A) \injectsvia{\id} \dom(A) \surjectsvia{A} \im(A) \to 0
.\end{align*} Moreover, it always splits, so
\(\dom A = \ker A \oplus \im A\) and thus
\(\abs{\dom(A)} = \abs{\ker(A)} + \abs{\im(A)}\).

\end{theorem}

\begin{remark}

We also have
\begin{align*}  
\dim(\rowspace(A)) = \dim(\colspace(A)) = \rank(A)
.\end{align*}

\end{remark}

\hypertarget{big-list-of-equivalent-properties}{%
\subsection{Big List of Equivalent
Properties}\label{big-list-of-equivalent-properties}}

Let \(A\) be an \(m\times n\) matrix. TFAE: - \(A\) is invertible and
has a unique inverse \(A^{-1}\) - \(A^T\) is invertible -
\(\det(A) \neq 0\) - The linear system \(A\vector{x} = \vector{b}\) has
a unique solution for every \(b\ \in \RR^m\) - The homogeneous system
\(A\vector{x} = 0\) has only the trivial solution \(\vector{x} = 0\) -
\(\rank(A) = n\) - i.e.~\(A\) is full rank -
\(\mathrm{nullity}(A) \definedas \dim\mathrm{nullspace}(A) = 0\) -
\(A = \prod_{i=1}^k E_i\) for some finite \(k\), where each \(E_i\) is
an elementary matrix. - \(A\) is row-equivalent to the identity matrix
\(I_n\) - \(A\) has exactly \(n\) pivots - The columns of \(A\) are a
basis for \(\RR^n\) - i.e.~\(\colspace(A) = \RR^n\) - The rows of \(A\)
are a basis for \(\RR^m\) - i.e.~\(\mathrm{rowspace}(A) = \RR^m\) -
\(\left(\colspace(A)\right)\perp = \left(\mathrm{rowspace}A\right)\perp = \theset{\vector 0}\)
- Zero is not an eigenvalue of \(A\). - \(A\) has \(n\) linearly
independent eigenvectors - The rows of \(A\) are coplanar.

Similarly, by taking negations, TFAE:

\begin{itemize}
\tightlist
\item
  \(A\) is not invertible
\item
  \(A\) is singular
\item
  \(A^T\) is not invertible
\item
  \(\det A = 0\)
\item
  The linear system \(A \vector x = \vector b\) has either no solution
  or infinitely many solutions.
\item
  The homogeneous system \(A \vector x = \vector 0\) has nontrivial
  solutions
\item
  \(\rank A < n\)
\item
  \(\dim \mathrm{nullspace}~ A > 0\)
\item
  At least one row of \(A\) is a linear combination of the others
\item
  The \(RREF\) of \(A\) has a row of all zeros.
\end{itemize}

Reformulated in terms of linear maps \(T\), TFAE: -
\(T^{-1}: \RR^m \to \RR^n\) exists - \(\im(T) = \RR^n\) -
\(\ker(T) = 0\) - \(T\) is injective - \(T\) is surjective - \(T\) is an
isomorphism - The system \(A\vector{x} = 0\) has infinitely many
solutions

\hypertarget{vector-spaces}{%
\subsection{Vector Spaces}\label{vector-spaces}}

\hypertarget{linear-transformations}{%
\subsubsection{Linear Transformations}\label{linear-transformations}}

\begin{definition}[Linear Transformation]

\todo[inline]{todo}

\end{definition}

\begin{remark}

It is common to want to know the range and kernel of a specific linear
transformation \(T\). \(T\) can be given in many ways, but a general
strategy for deducing these properties involves:

\begin{itemize}
\item
  Express an arbitrary vector in \(V\) as a linear combination of its
  basis vectors, and set it equal to an arbitrary vector in \(W\).
\item
  Use the linear properties of \(T\) to make a substitution from known
  transformations
\item
  Find a restriction or relation given by the constants of the initial
  linear combination.
\end{itemize}

\end{remark}

\begin{remark}

Useful fact: if \(V\leq W\) is a subspace and \(\dim(V) \geq \dim(W)\),
then \(V=W\).

\end{remark}

\begin{definition}[Kernel]

\todo[inline]{todo}

\end{definition}

\begin{proposition}[Two-step vector subspace test]

If \(V\subseteq W\), then \(V\) is a subspace of \(W\) if the following
hold:

\begin{align*}  
(1) && \vector 0\in V \\
(2) && \vector a, \vector b\in V\implies t\vector a + \vector b\in V
.\end{align*}

\end{proposition}

\hypertarget{linear-independence}{%
\subsubsection{Linear Independence}\label{linear-independence}}

\begin{proposition}[?]

Any set of two vectors \(\theset{\vector v, \vector w}\) is linearly
\textbf{dependent}
\(\iff \exists \lambda :~\vector v = \lambda \vector w\), i.e.~one is
not a scalar multiple of the other.

\end{proposition}

\hypertarget{bases}{%
\subsubsection{Bases}\label{bases}}

\begin{definition}[Basis and dimension]

A set \(S\) forms a \textbf{basis} for a vector space \(V\) iff

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(S\) is a set of linearly independent vectors, so
  \(\sum \alpha_i \vec{s_i} = 0 \implies \alpha_i = 0\) for all \(i\).
\item
  \(S\) spans \(V\), so \(\vec{v} \in V\) implies there exist
  \(\alpha_i\) such that \(\sum \alpha_i \vec{s_i} = \vec{v}\)
\end{enumerate}

In this case, we define the \textbf{dimension} of \(V\) to be
\(\abs{S}\).

\end{definition}

\todo[inline]{Show how to compute basis of kernel.}

\todo[inline]{Show how to compute basis of row space (nonzero rows in $\ref(A)$?)}

\todo[inline]{Show how to compute basis of column space: leading ones.}

\hypertarget{the-inner-product}{%
\subsubsection{The Inner Product}\label{the-inner-product}}

The point of this section is to show how an inner product can induce a
notion of ``angle'', which agrees with our intuition in Euclidean spaces
such as \(\RR^n\), but can be extended to much less intuitive things,
like spaces of functions.

\begin{definition}[The standard inner product]

The \textbf{Euclidean inner product} is defined as
\begin{align*}
\inner{\vector a}{\vector b} = \sum_{i=1}^n a_i b_i = a_1b_1 + a_2b_2 + \cdots + a_nb_n
.\end{align*}

Also sometimes written as \(\vector a^T\vector b\) or
\(\vector a \cdot \vector b\).

\end{definition}

\begin{proposition}[Inner products induce norms and angles]

Yields a norm
\begin{align*}
\norm{\vector x} \da \sqrt{\inner{\vector x}{\vector x}}
\end{align*}

which has a useful alternative formulation
\begin{align*}
\inner{\vector x}{\vector x} = \norm{\vector x}^2
.\end{align*}

This leads to a notion of angle:
\begin{align*}
\inner{\vector x}{\vector y} = \norm{\vector x} \norm{\vector y} \cos\theta_{x,y}
\implies \cos \theta_{x,y} \definedas \frac{\inner{\vector x}{\vector y}}{\norm{\vector x} \norm{\vector y}} = \inner{\hat{\vector x}}{\hat{\vector y}}
\end{align*} where \(\theta_{x,y}\) denotes the angle between the
vectors \(\vector x\) and \(\vector y\).

\end{proposition}

\begin{remark}

Since \(\cos \theta=0\) exactly when \(\theta = \pm \frac \pi 2\), we
can can declare two vectors to be \textbf{orthogonal} exactly in this
case:
\begin{align*}  
\vector x \in \vector y\perp \iff \inner{\vector x}{\vector y} = 0
.\end{align*}

Note that this makes the zero vector orthogonal to everything.

\end{remark}

\begin{definition}[Orthogonal Complement/Perp]

Given a subspace \(S \subseteq V\), we define its \textbf{orthogonal
complement}
\begin{align*}
S\perp = \theset{\vector v\in V \suchthat \forall \vector s\in S,~ \inner{\vector v}{\vector s} = 0}.
\end{align*}

\end{definition}

\begin{remark}

Any choice of subspace \(S\subseteq V\) yields a decomposition
\(V = S \oplus S\perp\).

\end{remark}

\begin{proposition}[Formula expanding a norm and 'Pythagorean theorem']

A useful formula is
\begin{align*}  
\norm{\vector x + \vector y}^2 = \norm{\vector x}^2 + 2\inner{\vector x}{\vector y} + \norm{\vector y}^2,
.\end{align*}

When \(\vector x\in \vector y\perp\), this reduces to
\begin{align*}  
\norm{\vector x + \vector y}^2 = \norm{\vector x}^2 + \norm{\vector y}^2
.\end{align*}

\end{proposition}

\begin{proposition}[Properties of the inner product]

\envlist
1. \textbf{Bilinearity:}
\begin{align*}
\inner{\sum_j \alpha_j \vector a_j}{\sum_k \beta_k \vector b_k} = \sum_j \sum_i \alpha_j \beta_i \inner{\vector a_j}{\vector b_i}.
\end{align*}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \textbf{Symmetry}:
  \begin{align*}
  \inner{\vector a}{\vector b} = \inner{\vector b}{\vector a}
  \end{align*}
\item
  \textbf{Positivity}:
  \begin{align*}
  \vector a \neq \vector 0 \implies \inner{\vector a}{\vector a} > 0
  \end{align*}
\item
  \textbf{Nondegeneracy}:
  \begin{align*}
  \vector a = \vector 0 \iff \inner{\vector a}{\vector a} = 0
  \end{align*}
\end{enumerate}

\end{proposition}

\todo[inline]{Proof of Cauchy-Schwarz: See Goode page 346.}

\hypertarget{gram-schmidt-process}{%
\subsubsection{Gram-Schmidt Process}\label{gram-schmidt-process}}

Extending a basis \(\theset{\mathbf{x}_i}\) to an orthonormal basis
\(\theset{\vector u_i}\)

\begin{align*}
\vector u_1 &= N(\vector{x_1}) \\
\vector u_2 &= N(\vector x_2 - \inner{\vector x_2}{\vector u_1}\vector u_1)\\
\vector u_3 &= N(\vector x_3 - \inner{\vector x_3}{\vector u_1}\vector u_1 - \inner{\vector x_3}{\vector u_2}\vector u_2 ) \\
\vdots & \qquad \vdots \\
\vector u_k &= N(\vector x_k - \sum_{i=1}^{k-1} \inner{\vector x_k}{\vector u_i}\vector u_i)
\end{align*}

where \(N\) denotes normalizing the result.

\hypertarget{in-more-detail}{%
\paragraph{In more detail}\label{in-more-detail}}

The general setup here is that we are given an orthogonal basis
\(\theset{\vector x_i}_{i=1}^n\) and we want to produce an
\textbf{orthonormal} basis from them.

Why would we want such a thing? Recall that we often wanted to change
from the standard basis \(\mathcal{E}\) to some different basis
\(\mathcal{B} = \theset{\vector b_1, \vector b_2, \cdots}\). We could
form the change of basis matrix
\(B = [\vector b_1, \vector b_2, \cdots]\) acts on vectors in the
\(\mathcal{B}\) basis according to
\begin{align*}  
B[\vector x]_\mathcal{B} = [\vector x]_{\mathcal{E}}
.\end{align*}

But to change from \(\mathcal{E}\) to \(\mathcal{B}\) requires computing
\(B^{-1}\), which acts on vectors in the standard basis according to
\begin{align*}  
B^{-1}[\vector x]_\mathcal{E} = [\vector x]_{\mathcal{B}}
.\end{align*}

If, on the other hand, the \(\vector b_i\) are orthonormal, then
\(B^{-1} = B^T\), which is much easier to compute. We also obtain a
rather simple formula for the coordinates of \(\vector x\) with respect
to \(\mathcal B\). This follows because we can write
\begin{align*}  
\vector x = \sum_{i=1}^n \inner{\vector x}{\vector b_i} \vector b_i \definedas \sum_{i=1}^n c_i \vector b_i,
.\end{align*}

and we find that
\begin{align*}  
[\vector x]_\mathcal{B} = \vector c \definedas [c_1, c_2, \cdots, c_n]^T.
.\end{align*}

This also allows us to simplify projection matrices. Supposing that
\(A\) has orthonormal columns and letting \(S\) be the column space of
\(A\), recall that the projection onto \(S\) is defined by
\begin{align*}  
P_S = Q(Q^TQ)^{-1}Q^T
.\end{align*}

Since \(Q\) has orthogonal columns and satisfies \(Q^TQ = I\), this
simplifies to
\begin{align*}  
P_S = QQ^T.
.\end{align*}

\hypertarget{the-algorithm}{%
\paragraph{The Algorithm}\label{the-algorithm}}

Given the orthogonal basis \(\theset{\vector x_i}\), we form an
orthonormal basis \(\theset{\vector u_i}\) iteratively as follows.

First define
\begin{align*}
N: \RR^n &\to S^{n-1} \\
\vector x &\mapsto \hat{\vector x} \definedas \frac {\vector x} {\norm{\vector  x}}
\end{align*}

which projects a vector onto the unit sphere in \(\RR^n\) by
normalizing. Then,

\begin{align*}
\vector u_1 &= N(\vector{x_1}) \\
\vector u_2 &= N(\vector x_2 - \inner{\vector x_2}{\vector u_1}\vector u_1)\\
\vector u_3 &= N(\vector x_3 - \inner{\vector x_3}{\vector u_1}\vector u_1 - \inner{\vector x_3}{\vector u_2}\vector u_2 ) \\
\vdots & \qquad \vdots \\
\vector u_k &= N(\vector x_k - \sum_{i=1}^{k-1} \inner{\vector x_k}{\vector u_i}\vector u_i)
\end{align*}

In words, at each stage, we take one of the original vectors
\(\vector x_i\), then subtract off its projections onto all of the
\(\vector u_i\) we've created up until that point. This leaves us with
only the component of \(\vector x_i\) that is orthogonal to the span of
the previous \(\vector u_i\) we already have, and we then normalize each
\(\vector u_i\) we obtain this way.

\textbf{Alternative Explanation}:

Given a basis
\begin{align*}
  S = \left\{\mathbf{v_1, v_2, \cdots v_n}\right\},
\end{align*}

the Gram-Schmidt process produces a corresponding orthogonal basis
\begin{align*}
  S' = \left\{\mathbf{u_1, u_2, \cdots u_n}\right\}
\end{align*} that spans the same vector space as \(S\).

\(S'\) is found using the following pattern:
\begin{align*}
\mathbf{u_1} &= \mathbf{v_1} \\
\mathbf{u_2} &= \mathbf{v_2} - \text{proj}_{\mathbf{u_1}} \mathbf{v_2}\\
\mathbf{u_3} &= \mathbf{v_3} - \text{proj}_{\mathbf{u_1}} \mathbf{v_3} - \text{proj}_{\mathbf{u_2}} \mathbf{v_3}\\
\end{align*}

where
\begin{align*}
\text{proj}_{\mathbf{u}} \mathbf{v} = (\text{scal}_{\mathbf{u}} \mathbf{v})\frac{\mathbf{u}}{\mathbf{\norm{u}}}
= \frac{\langle \mathbf{v,u} \rangle}{\norm{\mathbf{u}}}\frac{\mathbf{u}}{\mathbf{\norm{u}}}
= \frac{\ip{\vector{v}}{\vector{u}}}{\norm{\vector{u}}^2}\vector{u}
\end{align*} is a vector defined as the
\textit{orthogonal projection of $\vector{v}$ onto $\vector{u}$.}

\begin{figure}
\centering
\includegraphics{figures/image_2020-11-10-01-22-34.png}
\caption{Image}
\end{figure}

The orthogonal set \(S'\) can then be transformed into an orthonormal
set \(S''\) by simply dividing the vectors \(s\in S'\) by their
magnitudes. The usual definition of a vector's magnitude is
\begin{align*}
\norm{\vector{a}} = \sqrt{\ip{\vector{a}}{\vector{a}}} \text{ and } \norm{\vector{a}}^2 = \ip{\vector{a}}{\vector{a}}
\end{align*}

As a final check, all vectors in \(S'\) should be orthogonal to each
other, such that
\begin{align*}
\ip{\vector{v_i}}{\vector{v_j}} = 0 \text{ when } i \neq j
\end{align*}

and all vectors in \(S''\) should be orthonormal, such that
\begin{align*}
\ip{\vector{v_i}}{\vector{v_j}} = \delta_{ij}
\end{align*}

\hypertarget{the-fundamental-subspaces-theorem}{%
\subsubsection{The Fundamental Subspaces
Theorem}\label{the-fundamental-subspaces-theorem}}

Given a matrix \(A \in \mathrm{Mat}(m, n)\), and noting that
\begin{align*}
A &: \RR^n \to \RR^m,\\
A^T &:  \RR^m \to \RR^n
\end{align*}

We have the following decompositions:
\begin{align*}
&\RR^n &\cong  \ker A &\oplus \im A^T &\cong \mathrm{nullspace}(A) &\oplus~ \mathrm{colspace}(A^T) \\
&\RR^m &\cong  \im A &\oplus \ker A^T &\cong \mathrm{colspace}(A) &\oplus~ \mathrm{nullspace}(A^T)
\end{align*}

\hypertarget{computing-change-of-basis-matrices}{%
\subsubsection{Computing change of basis
matrices}\label{computing-change-of-basis-matrices}}

\todo[inline]{todo}

\hypertarget{matrices}{%
\subsection{Matrices}\label{matrices}}

\begin{remark}

An \(m\times n\) matrix is a map from \(n\)-dimensional space to
\(m\)-dimensional space. The number of \emph{rows} tells you the
dimension of the codomain, the number of \emph{columns} tells you the
dimension of the \emph{domain}.

\end{remark}

\begin{warning}

The space of matrices is not an integral domain! Counterexample: if
\(A\) is singular and nonzero, there is some nonzero \(\vector v\) such
that \(A \vector v = \vector 0\). Then setting
\(B = \tv{\vector v, \vector v, \cdots}\) yields \(AB = 0\) with
\(A\neq 0, B\neq 0\).

\end{warning}

\begin{definition}[Rank of a matrix]

The \textbf{rank} of a matrix \(A\) representing a linear transformation
\(T\) is \(\dim \colspace(A)\), or equivalently \(\dim \im T\).

\end{definition}

\begin{proposition}[?]

\(\rank(A)\) is equal to the number of nonzero rows in \(\rref(A)\).

\end{proposition}

\begin{definition}[Trace of a Matrix]

\begin{align*}
\mathrm{Trace}(A) = \sum_{i=1}^m A_{ii}
\end{align*}

\end{definition}

\begin{definition}[Elementary Row Operations]

The following are \textbf{elementary row operations} on a matrix:

\begin{itemize}
\tightlist
\item
  Permute rows
\item
  Multiple a row by a scalar
\item
  Add any row to another
\end{itemize}

\end{definition}

\begin{proposition}[Formula for matrix multiplication]

If
\(A = \thevector{\vector a_1, \vector a_2, \cdots} \in \mathrm{Mat}(m, n)\)
and
\(B = \thevector{\vector b_1, \vector b_2, \cdots} \in\mathrm{Mat}(n, p)\),
then
\begin{align*}
C \definedas AB \implies c_{ij} = \sum_{k=1}^n a_{ik}b_{kj} = \inner{\mathbf{a_i}}{\mathbf{b_j}}
\end{align*} where \(1\leq i \leq m\) and \(1\leq j \leq p\). In words,
each entry \(c_{ij}\) is obtained by dotting \emph{row} \(i\) of \(A\)
against \emph{column} \(j\) of \(B\).

\end{proposition}

\hypertarget{systems-of-linear-equations}{%
\subsubsection{Systems of Linear
Equations}\label{systems-of-linear-equations}}

\begin{definition}[Consistent and inconsistent]

A system of linear equations is \textbf{consistent} when it has at least
one solution. The system is \textbf{inconsistent} when it has no
solutions.

\end{definition}

\begin{definition}[Homogeneous Systems]

?

\end{definition}

\begin{remark}

Homogeneous systems are always consistent, i.e.~there is always at least
one solution.

\end{remark}

\begin{remark}

\envlist

\begin{itemize}
\tightlist
\item
  Tall matrices: more equations than unknowns, \emph{overdetermined}
\item
  Wide matrices: more unknowns than equations, \emph{underdetermined}
\end{itemize}

\end{remark}

\begin{proposition}[Characterizing solutions to a system of linear equations]

There are three possibilities for a system of linear equations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  No solutions (inconsistent)
\item
  One unique solution (consistent, square or tall matrices)
\item
  Infinitely many solutions (consistent, underdetermined, square or wide
  matrices)
\end{enumerate}

\includegraphics{figures/solution_sets.png}

These possibilities can be check by considering \(r\da \rank(A)\):

\begin{itemize}
\tightlist
\item
  \(r < r_b\): case 1, no solutions.
\item
  \(r = r_b\): case 1 or 2, at least one solution.

  \begin{itemize}
  \tightlist
  \item
    \(r_b = n\): case 2, a unique solution.
  \item
    \(r_b < n:\) case 3, infinitely many solutions.
  \end{itemize}
\end{itemize}

\end{proposition}

\hypertarget{determinants}{%
\subsubsection{Determinants}\label{determinants}}

\begin{proposition}[?]

\begin{align*}
\det{(A \mod p}) \mod p \equiv (\det{A}) \mod p
\end{align*}

\end{proposition}

\begin{proposition}[Inverse of a $2\times 2$ matrix]

For \(2\times 2\) matrices,
\begin{align*}
A^{-1} = \left( \begin{array}{cc} a & b \\ c & d \end{array}\right)^{-1} = \frac{1}{\det{A}}\left( \begin{array}{cc} d & -b \\ -c & a \end{array}\right)
\end{align*} In words, swap the main diagonal entries, and flip the
signs on the off-diagonal.

\end{proposition}

\begin{proposition}[Properties of the determinant]

Let \(A \in \mathrm{Mat}(m, n)\), then there is a function
\begin{align*}
\det: \mat(m, m) &\to \RR \\
A &\mapsto \det(A)
\end{align*} satisfying the following properties:

\begin{itemize}
\item
  \(\det\) is a group homomorphism onto \((\RR, \cdot)\):
  \begin{align*}
  \det(AB) = \det(A) \det(B)
  \end{align*}

  \begin{itemize}
  \tightlist
  \item
    Some corollaries:
    \begin{align*}  
    \det{A^k} &= k\det{A} \\
    \det(A^{-1}) &= (\det A)^{-1}
    \det(A^t) &= \det(A)
    .\end{align*}
  \end{itemize}
\item
  Invariance under adding scalar multiples of any row to another:
  \begin{align*}
  \det 
  \begin{bmatrix}
  & \vdots & \\
  \horzbar & \vecc{Aquamarine}a_i & \horzbar \\
  & \vdots & \\
  \end{bmatrix}
  =
  \det
  \begin{bmatrix}
  & \vdots & \\
  \horzbar & \vecc{Aquamarine}a_i + t\vector{a_j} & \horzbar \\
  & \vdots & \\
  \end{bmatrix}
  \end{align*}
\item
  Sign change under row permutation:
  \begin{align*}
  \det 
  \begin{bmatrix}
  & \vdots & \\
  \horzbar & \vecc{Aquamarine}a_i & \horzbar \\
  & \vdots & \\
  \horzbar & \vector a_j & \horzbar \\
  & \vdots & \\
  \end{bmatrix}
  =
  (-1) \det
  \begin{bmatrix}
  & \vdots & \\
  \horzbar & \vector a_j & \horzbar \\
  & \vdots & \\
  \horzbar & \vecc{Aquamarine}a_i & \horzbar \\
  & \vdots & 
  \end{bmatrix}
  \end{align*}

  \begin{itemize}
  \tightlist
  \item
    More generally, for a permutation \(\sigma\in S_n\),
    \begin{align*}
    \det 
    \begin{bmatrix}
    & \vdots & \\
    \horzbar & \vector a_i & \horzbar \\
    & \vdots & \\
    \horzbar & \vector a_j & \horzbar \\
    & \vdots & \\
    \end{bmatrix}
    =
    (-1)^{\sgn(\sigma)} \det
    \begin{bmatrix}
    & \vdots & \\
    \horzbar & \vector a_{\sigma(j)} & \horzbar \\
    & \vdots & \\
    \horzbar & \vector a_{\sigma(i)} & \horzbar \\
    & \vdots & 
    \end{bmatrix}
    \end{align*}
  \end{itemize}
\item
  Multilinearity in rows:
  \begin{align*}  
  \det 
  \begin{bmatrix}
  & \vdots & \\
  \horzbar & t \vecc{Aquamarine}a_i & \horzbar \\
  & \vdots & \\
  \end{bmatrix}
  &= t \det
  \begin{bmatrix}
  & \vdots & \\
  \horzbar & \vecc{Aquamarine}a_i & \horzbar \\
  & \vdots & \\
  \end{bmatrix} \\
  \det 
  \begin{bmatrix}
  \horzbar & t \vector a_1 & \horzbar \\
  \horzbar & t \vector a_2 & \horzbar \\
  & \vdots & \\
  \horzbar & t \vector a_m & \horzbar \\
  \end{bmatrix}
  &= t^m \det
  \begin{bmatrix}
  \horzbar & \vector a_1 & \horzbar \\
  \horzbar & \vector a_2 & \horzbar \\
  & \vdots & \\
  \horzbar & \vector a_m & \horzbar \\
  \end{bmatrix} \\
  \det 
  \begin{bmatrix}
  \horzbar & t_1 \vector a_1 & \horzbar \\
  \horzbar & t_2 \vector a_2 & \horzbar \\
  & \vdots & \\
  \horzbar & t_m \vector a_m & \horzbar \\
  \end{bmatrix}
  &= \prod_{i=1}^m t_i \det
  \begin{bmatrix}
  \horzbar & \vector a_1 & \horzbar \\
  \horzbar & \vector a_2 & \horzbar \\
  & \vdots & \\
  \horzbar & \vector a_m & \horzbar \\
  \end{bmatrix}
  .\end{align*}
\item
  Linearity in each row:
  \begin{align*}  
  \det
  \begin{bmatrix}
  & \vdots & \\
  \horzbar & \vecc{Aquamarine}a_i + \vecc{red} a_j & \horzbar \\
  & \vdots & \\
  \end{bmatrix}
  =\det
  \begin{bmatrix}
  & \vdots & \\
  \horzbar & \vecc{Aquamarine}a_i & \horzbar \\
  & \vdots & \\
  \end{bmatrix} +
  \det
  \begin{bmatrix}
  & \vdots & \\
  \horzbar & \vecc{red}a_j & \horzbar \\
  & \vdots & \\
  \end{bmatrix} 
  .\end{align*}
\item
  \(\det(A)\) is the volume of the parallelepiped spanned by the columns
  of \(A\).
\item
  If any row of \(A\) is all zeros, \(\det(A) = 0\).
\end{itemize}

\end{proposition}

\begin{proposition}[Characterizing singular matrices]

TFAE:

\begin{itemize}
\tightlist
\item
  \(\det(A) = 0\)
\item
  \(A\) is singular.
\end{itemize}

\end{proposition}

\hypertarget{computing-determinants}{%
\subsubsection{Computing Determinants}\label{computing-determinants}}

Useful shortcuts:

\begin{itemize}
\tightlist
\item
  If \(A\) is upper or lower triangular, \(\det(A) = \prod_i a_{ii}\).
\end{itemize}

\begin{definition}[Minors]

The \textbf{minor} \(M_{ij}\) of \(A\in \mat(n, n)\) is the
\emph{determinant} of the \((n-1) \times (n-1)\) matrix obtained by
deleting the \(i\)th row and \(j\)th column from \(A\).

\end{definition}

\begin{definition}[Cofactors]

\label{def:cofactor} The \textbf{cofactor} \(C_{ij}\) is the scalar
defined by
\begin{align*}  
C_{ij} \da (-1)^{i+j} M_{ij}
.\end{align*}

\end{definition}

\begin{proposition}[Laplace/Cofactor Expansion]

For any fixed \(i\), there is a formula
\begin{align*}  
\det(A) = \sum_{j=1}^n a_{ij} C_{ij}
.\end{align*}

\end{proposition}

\begin{example}[?]

Let
\begin{align*}  
A = 
\left[\begin{array}{lll}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{array}\right]
.\end{align*}

Then
\begin{align*}  
\det A =
1 \cdot\left|\begin{array}{ll}
5 & 6 \\
8 & 9
\end{array}\right|-2 \cdot\left|\begin{array}{ll}
4 & 6 \\
7 & 9
\end{array}\right|+3 \cdot\left|\begin{array}{ll}
4 & 5 \\
7 & 8
\end{array}\right|
= 1 \cdot(-3)-2 \cdot(-6)+3 \cdot(-3)
= 0
.\end{align*}

\end{example}

\begin{proposition}[Computing determinant from RREF]

\(\det(A)\) can be computed by reducing \(A\) to \(\rref(A)\) (which is
upper triangular) and keeping track of the following effects:

\begin{itemize}
\tightlist
\item
  \(R_i \mapsfrom R_i \pm t R_j\): no effect.
\item
  \(R_i \tofrom R_j\): multiply by \((-1)\).
\item
  \(R_i \mapsfrom tR_i\): multiply by \(t\).
\end{itemize}

\end{proposition}

\hypertarget{inverting-a-matrix}{%
\subsubsection{Inverting a Matrix}\label{inverting-a-matrix}}

\begin{proposition}[Cramer's Rule]

Given a linear system \(A\vector x = \vector b\), writing
\(\vector x = \tv{x_1, \cdots, x_n}\), there is a formula
\begin{align*}
x_i = \frac{\det(B_i)}{\det(A)}
\end{align*} where \(B_i\) is \(A\) with the \(i\)th column deleted and
replaced by \(\vector{b}\).

\end{proposition}

\begin{proposition}[Gauss-Jordan Method for inverting a matrix]

Under the equivalence relation of elementary row operations, there is an
equivalence of augmented matrices:
\begin{align*}
\begin{bmatrix}
A &\aug& I
\end{bmatrix}
\sim
\begin{bmatrix}
I &\aug& A^{-1}
\end{bmatrix}
\end{align*} where \(I\) is the \(n\times n\) identity matrix.

\end{proposition}

\begin{proposition}[Cofactor formula for inverse]

\begin{align*}  
A^{-1} = {1\over \det(A)} \tv{C_{ij}}^t
.\end{align*} where \(C_{ij}\) is the
\emph{cofactor}(\cref{def:cofactor}) at position \(i,j\).\footnote{Note
  that the matrix appearing here is sometimes called the
  \emph{adjugate}.}

\end{proposition}

\begin{example}[Inverting a $2\times 2$ matrix]

\begin{align*}
\left(\begin{array}{cc} 
a& b \\ 
c& d 
\end{array}\right)^{-1} 
= 
{1 \over a d - b c} 
\left(\begin{array}{rr} 
d  & -b \\ 
-c & a 
\end{array}\right)
\quad 
\text{ where } ad-bc \ne 0
\end{align*}

What's the pattern?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Always divide by determinant
\item
  Swap the diagonals
\item
  Hadamard product with checkerboard
\end{enumerate}

\end{example}

\begin{example}[Inverting a $3\times 3$ matrix]

\begin{align*}
\begin{bmatrix}
+ & - \\
- & +
\end{bmatrix}
\end{align*}

\begin{align*}
A^{-1} \da
\begin{bmatrix}
a & b & c \\
d & e & f \\
g & h & i
\end{bmatrix}
^{-1} 
= 
{1 \over {\det A}} 
\begin{bmatrix}
e i - f h & -(b i - c h) & b f - c e \\ 
-(d i - f g) &a i - c g &-(a f -c d) \\ 
d h - e g & -(a h - b g)& a e - b d 
\end{bmatrix}
.\end{align*}

The pattern:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Divide by determinant
\item
  Each entry is determinant of submatrix of \(A\) with corresponding
  col/row deleted
\item
  Hadamard product with checkerboard
\end{enumerate}

\begin{align*}
\begin{bmatrix}
+ & - & + \\ 
- & + & - \\  \
+ & - & + 
\end{bmatrix}
\end{align*}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Transpose at the end!!
\end{enumerate}

\end{example}

\hypertarget{bases-for-spaces-of-a-matrix}{%
\subsubsection{Bases for Spaces of a
Matrix}\label{bases-for-spaces-of-a-matrix}}

Let \(A\in \mat(m, n)\) represent a map \(T:\RR^n\to \RR^m\).

\todo[inline]{Add examples.}

\begin{definition}[Pivot]

?

\end{definition}

\todo[inline]{todo}

\begin{proposition}

\begin{align*}  
\dim \rowspace(A) = \dim \colspace(A)
.\end{align*}

\end{proposition}

\hypertarget{the-row-space}{%
\paragraph{The row space}\label{the-row-space}}

\begin{align*}
\im(T)\dual = \rowspace(A) \subset \RR^n
.\end{align*}

Reduce to RREF, and take nonzero rows of \(\mathrm{RREF}(A)\).

\hypertarget{the-column-space}{%
\paragraph{The column space}\label{the-column-space}}

\begin{align*}
\im(T) = \colspace(A) \subseteq \RR^m
\end{align*}

Reduce to RREF, and take columns with pivots from original \(A\).

\begin{remark}

Not enough pivots implies columns don't span the entire target domain

\end{remark}

\hypertarget{the-nullspace}{%
\paragraph{The nullspace}\label{the-nullspace}}

\begin{align*}
\ker(T) = \nullspace(A) \subseteq \RR^n
\end{align*}

Reduce to RREF, zero rows are free variables, convert back to equations
and pull free variables out as scalar multipliers.

\hypertarget{eigenspaces}{%
\paragraph{Eigenspaces}\label{eigenspaces}}

For each \(\lambda \in \spec(A)\), compute a basis for
\(\ker(A - \lambda I)\).

\hypertarget{eigenvalues-and-eigenvectors}{%
\subsubsection{Eigenvalues and
Eigenvectors}\label{eigenvalues-and-eigenvectors}}

\begin{definition}[Eigenvalues, eigenvectors, eigenspaces]

A vector \(\vector v\) is said to be an \textbf{eigenvector} of \(A\)
with \textbf{eigenvalue} \(\lambda\in \spec(A)\) iff
\begin{align*}
A\vector v = \lambda \vector v
\end{align*} For a fixed \(\lambda\), the corresponding
\textbf{eigenspace} \(E_\lambda\) is the span of all such vectors.

\end{definition}

\begin{remark}

\envlist

\begin{itemize}
\item
  Similar matrices have identical eigenvalues and multiplicities.
\item
  Eigenvectors corresponding to distinct eigenvalues are \textbf{always}
  linearly independent
\item
  \(A\) has \(n\) distinct eigenvalues \(\implies A\) has \(n\) linearly
  independent eigenvectors.
\item
  A matrix \(A\) is diagonalizable \(\iff A\) has \(n\) linearly
  independent eigenvectors.
\end{itemize}

\end{remark}

\begin{proposition}[How to find eigenvectors]

For \(\lambda\in \spec(A)\),
\begin{align*}  
\vector v\in E_\lambda \iff \vector v \in \ker(A-I\lambda)
.\end{align*}

\end{proposition}

\begin{remark}

Some miscellaneous useful facts:

\begin{itemize}
\item
  \(\lambda \in \spec(A) \implies \lambda^2 \in \spec(A^2)\) with the
  same eigenvector.
\item
  \(\prod \lambda_i = \det A\)
\item
  \(\sum \lambda_i = \mathrm{Tr}~A\)
\end{itemize}

\end{remark}

\hypertarget{finding-generalized-eigenvectors}{%
\paragraph{Finding generalized
eigenvectors}\label{finding-generalized-eigenvectors}}

\todo[inline]{todo}

\hypertarget{diagonalizability}{%
\paragraph{Diagonalizability}\label{diagonalizability}}

\begin{remark}

An \(n\times n\) matrix \(P\) is diagonalizable iff its eigenspace is
all of \(\mathbb{R}^n\) (i.e.~there are \(n\) linearly independent
eigenvectors, so they span the space.)

\end{remark}

\begin{remark}

\(A\) is diagonalizable if there is a basis of eigenvectors for the
range of \(P\).

\end{remark}

\hypertarget{useful-counterexamples}{%
\subsubsection{Useful Counterexamples}\label{useful-counterexamples}}

\begin{align*}
A \da \left[ \begin{array} { c c } { 1 } & { 1 } \\ { 0 } & { 1 } \end{array} \right] 
&\implies A^n = 
\left[ \begin{array} { c c } { 1 } & { n } \\ { 0 } & { 1 } \end{array} \right], 
&& \spec(A) = [1,1] \\
A \da \left[ \begin{array} { c c } { 1 } & { 1 } \\ { 0 } & { - 1 } \end{array} \right] 
&\implies A^2 = I_2,
&& \spec(A) = [1, -1]
\end{align*}

\hypertarget{example-problems}{%
\subsection{Example Problems}\label{example-problems}}

\begin{problem}[?]

Determine a basis for
\begin{align*}
S = \left\{a_0 + a_1 x + a_2 x^2\mid a_0,a_1,a_2 \in \mathbb{R} \land a_0 - a_1 -2a_2 =0\right\}.
\end{align*}

\end{problem}

\hrulefill

\begin{solution}

Let \(a_2=t, a_1=s, a_0=s+2t\), then
\begin{align*}
S     &= \left\{ (s+2t) + (sx+tx^2)\mid s,t\in\mathbb{R} \right\} \\
&= \left\{ (s+sx) + (2t+tx^2)\mid s,t\in\mathbb{R} \right\} \\
&= \left\{ s(1+x) + t(2+x^2)\mid s,t\in\mathbb{R} \right\} \\
&= \text{span}\left\{(1+x),(2+x^2)\right\}
\end{align*}

and a basis for \(S\) is
\begin{align*}
\left\{(1+x), (2+x^2)\right\}
\end{align*}

\end{solution}

\begin{problem}[?]

\textbf{T/F}: If \(V\) is an \(n\)-dimensional vector space, then every
set \(S\) with fewer than \(n\) vectors can be extended to a basis for
\(V\).

\end{problem}

\begin{solution}

\textbf{False.} Only \textit{linearly independent} sets with fewer than
\(n\) vectors can be extended to form a basis for \(V\).

\end{solution}

\begin{problem}[?]

\textbf{T/F}: The set of all 3 x 3 \textit{upper triangular} matrices
forms a three-dimensional subspace of \(M_{3}(\mathbb{R})\).

\end{problem}

\begin{solution}

\textbf{False.} This set forms a 6-dimensional subspace. A basis for
this space would require six elements.

\end{solution}

\begin{problem}[?]

Given \(A=\)
\begin{align*}\begin{bmatrix}
  1     & 3 `   \\
  -2    & -6
\end{bmatrix}\end{align*} what is the dimension of the null space of
\(A\)?

\end{problem}

\begin{solution}

The augmented matrix for the system \(A\mathbf{x} = \mathbf{0}\) is
\begin{align*}\begin{bmatrix}[cc|c]
1 & 3 & 0   \\
0 & 0 & 0
\end{bmatrix}\end{align*} which has one free variable.

Writing one variable in terms of another results in
\(x_1 + 3x_2 = 0 \Rightarrow x_1 = 3x_2\).

Let \(x_2 = r\) where \(r \in R\), then
\(S = \left\{ x \in \mathbb{R}^2 : \mathbf{x} = r(3,1), r \in \mathbb{R}\right\} = \text{span}\left\{(3,1)\right\}\).

So, the set \(B = \left\{(3,1)\right\}\) is a basis for the null space
of \(A\) and\textbackslash{}
\underline{the dimension of the null space is 1}.

\end{solution}

\begin{problem}[?]

Let \(S\) be the subspace of \(\mathbb{R}^3\) that consists of all
solutions to the equation \(x-3y+z = 0\). Determine a basis for \(S\),
and find dim{[}\(S\){]}.

\end{problem}

\begin{solution}

The first goal is to find a way to express the set of 3-tuples that
satisfy this equation.

Let \(y=r\) and \(z=s\), then \(x=r-s\). Then vectors \(\mathbf{v}\)
that satisfy the equation are all of the form
\begin{align*}
\mathbf{v} = (3r-s, r, s) = (3r,r,0)+(-s,0,s) = r(3,1,0) + s(-1,0,1).
\end{align*} (Note - the goal here is to separate the dependent
variables into different vectors so they can be written as a linear
combination of something.)

The set \(S\) that satisfies this equation is then
\begin{align*}
S &= \left\{ \mathbf{v} \in \mathbb{R}^3 : \mathbf{v} =r(3,1,0) + s(-1,0,1) \land r,s\in\mathbb{R} \right\} \\
&= \text{span}\left\{ (3,1,0), (-1,0,1)\right\}
\end{align*}

All that remains is to check that the vectors in this span are linearly
independent. This can be done by showing that \textbf{if}
\begin{align*}
a(3,1,0) + b(-1,0,1) = (0,0,0)
\end{align*} \textbf{then} \(a=b=0\).

Since the two vectors are linearly independent and span the solution set
\(S\), they form a basis for \(S\) of dimension 2.

\end{solution}

\begin{problem}[?]

Determine a basis for the subspace of \(M_2(\mathbb{R})\) spanned by
\begin{align*}\begin{bmatrix}
-1 & 3 \\
-1 & 2
\end{bmatrix}\end{align*}

\begin{align*}\begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix}\end{align*}

\begin{align*}\begin{bmatrix}
-1 & 4 \\
1 & 1
\end{bmatrix}\end{align*}

\begin{align*}\begin{bmatrix}
5 & 6 \\
-5 & 1
\end{bmatrix}\end{align*}.\\

\end{problem}

\begin{solution}

Note that because the set contains the zero matrix, it is linearly
dependent. So only consider the other three, as they span the same
subspace as the original set.

First, determine if \(\left\{ A_1, A_2, A_3\right\}\) is linearly
independent. Start with the equation
\begin{align*}
c_1A_1 + c_2A_2 + c_3A_3 = 0_2
\end{align*}

which gives
\begin{align*}
c_1 -   c_2 +   5c_3    &= 0 \\
3c_1 +  4c_2 -  6c_3    &= 0 \\
-c_1 +  c_2 -   5c_3    &= 0 \\
2c_1 +  c_2 +   c_3     &= 0
\end{align*}

which has the solution \((-2r,3r,r)\). So the set is linearly dependent
by the relation
\begin{align*}
-2A_1 + 3A_2 + A_3 = 0 \text{ or }\\
A_3 = 2A_1 - 3A_2
\end{align*}

So \(\left\{A_1, A_2\right\}\) spans the same subspace as the original
set. It is also linearly independent, and therefore forms a basis for
the original subspace.

\end{solution}

\begin{problem}[?]

Let \(A, B, C \in M_2 (\mathbb{R})\). Define
\(\langle A,B\rangle = a_{11}b_{11}+2a_{12}b{12}+3a_{21}b_{21}\). Does
this define an inner product on \(M_2 (\mathbb{R})\)?

\end{problem}

\begin{problem}[?]

Instead, let \(\langle A,B\rangle = a_{11} + b_{22}\). Does this define
an inner product on \(M_2(\mathbb{R})\)?

\end{problem}

\begin{problem}[?]

Let \(p=a_0 + a_1 x + a_2 x^2\) and \(q=b_0 + b_1 x + b_2 x^2\). Define
\(\langle p,q\rangle = \sum_{i=0}^{2}(i+1)a_i b_i\). Does this define an
inner product on \(P_2\)?

\end{problem}

\hypertarget{linear-algebra-advanced-topics}{%
\section{Linear Algebra: Advanced
Topics}\label{linear-algebra-advanced-topics}}

\hypertarget{changing-basis}{%
\subsection{Changing Basis}\label{changing-basis}}

\begin{proposition}[Changing to the standard basis]

The transition matrix from a given basis
\(\mathcal{B} = \ts{\vector b_i}_{i=1}^n\) to the standard basis is
given by
\begin{align*}  
A\da
\begin{bmatrix}
\vertbar    & \vertbar    &        & \vertbar \\
\vector b_1 & \vector b_2 & \cdots & \vector b_n \\
\vertbar    & \vertbar    &        & \vertbar \\
\end{bmatrix}
,\end{align*} and the transition matrix from the standard basis to
\(\mathcal{B}\) is \(A\inv\).

\end{proposition}

\hypertarget{orthogonal-matrices}{%
\subsection{Orthogonal Matrices}\label{orthogonal-matrices}}

Given a notion of orthogonality for vectors, we can extend this to
matrices. A square matrix is said to be orthogonal iff
\(QQ^T = Q^TQ = I\). For rectangular matrices, we have the following
characterizations:
\begin{align*}
QQ^T = I \implies &\text{The rows of } Q \text { are orthogonal,} \\
Q^TQ = I \implies &\text{The columns of } Q \text{ are orthogonal.}
\end{align*}

To remember which condition is which, just recall that matrix
multiplication \(AB\) takes the inner product between the \textbf{rows}
of \(A\) and the \textbf{columns} of \(B\). So if, for example, we want
to inspect whether or not the columns of \(Q\) are orthogonal, we should
let \(B=Q\) in the above formulation -- then we just note that the rows
of \(Q^T\) are indeed the columns of \(Q\), so \(Q^TQ\) computes the
inner products between all pairs of the columns of \(Q\) and stores them
in a matrix.

\hypertarget{projections-1}{%
\subsection{Projections}\label{projections-1}}

\begin{remark}

A projection \(P\) induces a decomposition
\begin{align*}  
\dom(P) = \ker(P) \oplus \ker(P)\perp
.\end{align*}

\end{remark}

\todo[inline]{Check! Domain or range..?}

Distance from a point \(\vector p\) to a line
\(\vector a + t\vector b\): let \(\vector w = \vector p - \vector a\),
then: \(\norm{\vector w - P(\vector w, \vector v)}\)

\begin{proposition}[Projection onto range]

\begin{align*}  
\projection_{\range(A)}(\vector x) = A(A^t A)^{-1} A^t \vector x
.\end{align*}

Mnemonic:
\begin{align*}  
P \approx {A^t A \over AA^t}
.\end{align*}

\end{proposition}

With an inner product in hand and a notion of orthogonality, we can
define a notion of \textbf{orthogonal projection} of one vector onto
another, and more generally of a vector onto a subspace spanned by
multiple vectors.

\hypertarget{projection-onto-a-vector}{%
\subsubsection{Projection Onto a
Vector}\label{projection-onto-a-vector}}

Say we have two vectors \(\vector x\) and \(\vector y\), and we want to
define ``the component of \(\vector x\) that lies along \(\vector y\)'',
which we'll call \(\vector p\). We can work out what the formula should
be using a simple model:

We notice that whatever \(p\) is, it will in the direction of
\(\vector y\), and thus \(\vector p = \lambda \hat {\vector y}\) for
some scalar \(\lambda\), where in fact \(\lambda = \norm {\vector p}\)
since \(\norm{\hat{\vector y}} = 1\). We will find that
\(\lambda = \inner{\vector x}{\hat{\vector y}}\), and so
\begin{align*}  
\vector p = \inner{\vector x}{\hat{\vector y}}\hat{\vector y} = \frac{\inner{\vector x}{\vector y}}{\inner{\vector y}{\vector y}} \vector y
.\end{align*}

Notice that we can then form a ``residual'' vector
\(\vector r = \vector x - \vector p\), which should satisfy
\(\vector r \perp \vector p\). If we were to let \(\lambda\) vary as a
function of a parameter \(t\) (making \(\vector r\) a function of \(t\)
as well) we would find that this particular choice minimizes
\(\norm{\vector r (t)}\).

\hypertarget{projection-onto-a-subspace}{%
\subsubsection{Projection Onto a
Subspace}\label{projection-onto-a-subspace}}

In general, supposing one has a subspace
\(S = \mathrm{span}\theset{\vector y_1, \vector y_2, \cdots, \vector y_n}\)
and (importantly!) the \(\vector y_i\) are orthogonal, then the
projection of \(\vector p\) of \(x\) onto \(S\) is given by the sum of
the projections onto each basis vector, yielding

\begin{align*}
\vector p = \sum_{i=1}^n \frac{\inner{\vector x}{\vector y_i}}{\inner{\vector y_i}{\vector y_i}} \vector y_i = \sum_{i=1}^n \inner{\vector x}{\vector y_i} \hat{\vector y_i}
.\end{align*}

\begin{quote}
Note: this is part of why having an orthogonal basis is desirable!
\end{quote}

Letting \(A = [\vector y_1, \vector y_2, \cdots]\), then the following
matrix projects vectors onto \(S\), expressing them in terms of the
basis \(\vector y_i\)\footnote{For a derivation of this formula, see the
  section on least-squares approximations.}:
\begin{align*}
\tilde P_A = (AA^T)^{-1}A^T,
\end{align*}

while this matrix performs the projection and expresses it in terms of
the standard basis:
\begin{align*}
P_A = A(AA^T)^{-1}A^T.
\end{align*}

Equation of a plane: given a point \(\vector p_0\) on a plane and a
normal vector \(\vector n\), any vector \(\vector x\) on the plane
satisfies
\begin{align*}
\inner{\vector x - \vector p_0}{\vector n} = 0
\end{align*}

To find the distance between a point \(\vector a\) and a plane, we need
only project \(\vector a\) onto the subspace spanned by the normal
\(\vector n\):
\begin{align*}  
d = \inner{\vector a}{\vector n}
.\end{align*}

One important property of projections is that for any vector
\(\vector v\) and for any subspace \(S\), we have
\(\vector v - P_S(\vector v) \in S\perp\). Moreover, if
\(\vector v \in S\perp\), then \(P_s(\vector v)\) must be zero. This
follows by noting that in equation \(\ref{projection_equation}\), every
inner product appearing in the sum vanishes, by definition of
\(\vector v \in S\perp\), and so the projection is zero.

\hypertarget{least-squares}{%
\subsubsection{Least Squares}\label{least-squares}}

\begin{proposition}[Normal Equations]

\(\vector x\) is a least squares solution to \(A\vector x = \vector b\)
iff
\begin{align*}
A^t A \vector x = A^t \vector b
\end{align*}

\end{proposition}

\todo[inline]{Derivation of normal equations.}

The general setup here is that we would like to solve
\(A\vector x = \vector b\) for \(\vector x\), where \(\vector b\) is not
in fact in the range of \(A\). We thus settle for a unique ``best''
solution \(\tilde{\vector x}\) such that the error
\(\norm{A\tilde{\vector x} - \vector b}\) is minimized.

Geometrically, the solution is given by projecting \(\vector b\) onto
the column space of \(A\). To see why this is the case, define the
residual vector \(\vector r = A\tilde{\vector x} - \vector b\). We then
seek to minimize \(\norm{\vector r}\), which happens exactly when
\(\vector r \perp \im A\). But this happens exactly when
\(\vector r \in (\im A)\perp\), which by the fundamental subspaces
theorem, is equivalent to \(\vector r \in \ker A^T\).

From this, we get the equation
\begin{align*}
A^T \vector r = \vector 0 \\
\implies A^T(A \tilde{\vector x} - \vector b) = \vector 0\\
\implies A^TA\tilde{\vector x} = A^T \vector b,
\end{align*}

where the last line is described as the \textbf{normal equations}.

If \(A\) is an \(m\times n\) matrix and is of full rank, so it has \(n\)
linearly independent columns, then one can show that \(A^T A\) is
nonsingular, and we thus arrive at the least-squares solution
\begin{align*}
\tilde{\vector x} = (A^TA)^{-1}A^T \vector b \qed
\end{align*}

These equations can also be derived explicitly using Calculus applied to
matrices, vectors, and inner products. This requires the use of the
following formulas:
\begin{align*}
\dd{}{\vector x} \inner{\vector x}{\vector a} &= \vector a \\
\dd{}{\vector x} \inner{\vector x}{\vector A\vector x} &= (A+A^T)\vector x
\end{align*}

as well as the adjoint formula
\begin{align*}  
\inner{A\vector x}{\vector x} = \inner{\vector x}{A^T \vector x}.
.\end{align*}

From these, by letting \(A=I\) we can derive
\begin{align*}  
\dd{}{\vector x} \norm{\vector x}^2 = \dd{}{\vector x} \inner{\vector x}{\vector x} = 2\vector x\\
.\end{align*}

The derivation proceeds by solving the equation
\begin{align*}  
\dd{}{\vector x} \norm{\vector b - A\vector x}^2 = \vector 0.
.\end{align*}

\hypertarget{normal-forms}{%
\subsection{Normal Forms}\label{normal-forms}}

\begin{remark}

Every square matrix is similar to a matrix in Jordan canonical form.

\end{remark}

\hypertarget{decompositions}{%
\subsection{Decompositions}\label{decompositions}}

\hypertarget{the-qr-decomposition}{%
\subsubsection{The QR Decomposition}\label{the-qr-decomposition}}

Gram-Schmidt is often computed to find an orthonormal basis for, say,
the range of some matrix \(A\). With a small modification to this
algorithm, we can write \(A = QR\) where \(R\) is upper triangular and
\(Q\) has orthogonal columns.

Why is this useful? One reason is that this also allows for a
particularly simple expression of least-squares solutions. If \(A=QR\),
then \(R\) will be invertible, and a bit of algebraic manipulation will
show that
\begin{align*}  
\tilde{\vector x} = R^{-1}Q^T\vector b.
.\end{align*}

How does it work? You simply perform Gram-Schmidt to obtain
\(\theset{\vector u_i}\), then
\begin{align*}Q = [\vector u_1, \vector u_2, \cdots ].\end{align*}

The matrix \(R\) can then be written as

\begin{align*}
r_{ij} = \begin{cases}
\inner{\vector u_i}{\vector x_j}, & i\leq j, \\
0, & \text{else}.
\end{cases}
\end{align*}

Explicitly, this yields the matrix
\begin{align*}
R = 
\begin{bmatrix}
\inner{\vector u_1}{\vector x_1} & \inner{\vector u_1}{\vector x_2} & \inner{\vector u_1}{\vector x_3} & \cdots & \\
0 & \inner{\vector u_2}{\vector x_2} & \inner{\vector u_2}{\vector x_3} & \cdots & \\
0 & 0 & \inner{\vector u_3}{\vector x_3} & \cdots & \\
\vdots & \vdots & \vdots & \ddots \\
\end{bmatrix}
\end{align*}

\todo[inline]{Explain shortcut for diagonal.}

\hypertarget{appendix-lists-of-things-to-know}{%
\section{Appendix: Lists of things to
know}\label{appendix-lists-of-things-to-know}}

\begin{quote}
Textbook: Leon, \emph{Linear Algebra with Applications}
\end{quote}

\hypertarget{topics}{%
\subsection{Topics}\label{topics}}

\begin{itemize}
\tightlist
\item
  1.6: Partition Matrices
\item
  3.5: Change of Basis
\item
  4.1: Linear Transformations
\item
  4.2: Matrix Representations
\item
  4.3: Similarity

  \begin{itemize}
  \tightlist
  \item
    \emph{Exam 1}\\
  \end{itemize}
\item
  5.1: Scalar Product in \(\RR^n\)
\item
  5.2: Orthogonal Subspaces
\item
  5.3: Least Squares
\item
  5.4: Inner Product Spaces
\item
  5.5: Orthonormal Sets
\item
  5.6: Gram-Schmidt
\item
  6.1: Eigenvalues and Eigenvectors

  \begin{itemize}
  \tightlist
  \item
    \emph{Exam 2}
  \end{itemize}
\item
  6.2: Systems of Linear Differential Equations
\item
  6.3: Diagonalization
\item
  6.6: Quadratic Forms
\item
  6.7: Positive Definite Matrices
\item
  6.5: Singular Value Decomposition
\item
  7.7: The Moore-Penrose Pseudo-Inverse

  \begin{itemize}
  \tightlist
  \item
    \emph{Final Exam}
  \end{itemize}
\end{itemize}

\hypertarget{definitions}{%
\subsection{Definitions}\label{definitions}}

\begin{itemize}
\tightlist
\item
  System of equations
\item
  Homogeneous system
\item
  Consistent/inconsistent system
\item
  Matrix
\item
  Matrix (i.e.~\(A \vector x = \vector b\))
\item
  Inverse matrix
\item
  Singular matrix
\item
  Determinant
\item
  Trace
\item
  Rank
\item
  Elementary row operation
\item
  Row equivalence
\item
  Pivot
\item
  Row Echelon Form
\item
  Reduced Row Echelon Form
\item
  Gaussian elimination
\item
  Block matrix
\item
  Vector space
\item
  Vector subspace
\item
  Linear transformation
\item
  Span
\item
  Linear independence
\item
  Basis
\item
  Change of basis
\item
  Dimension
\item
  Row space
\item
  Column space
\item
  Image
\item
  Null space
\item
  Kernel
\item
  Direct sum
\item
  Projection
\item
  Orthogonal subspaces
\item
  Orthogonal complement
\item
  Normal equations
\item
  Least-squares solution
\item
  Orthonormal
\item
  Eigenvalue
\item
  Eigenvector
\item
  Characteristic polynomial
\item
  Similarity
\item
  Diagonalizable
\item
  Inner product
\item
  Bilinearity
\item
  Multilinearity
\item
  Defective
\item
  Singular value decomposition
\item
  QR factorization
\item
  Gram-Schmidt process
\item
  Spectral theorem
\item
  Symmetric matrix
\item
  Orthogonal matrix
\item
  Positive-definite
\item
  Quadratic form
\end{itemize}

\hypertarget{lower-division-review}{%
\subsection{Lower-division review}\label{lower-division-review}}

\begin{itemize}
\tightlist
\item
  Systems of linear equations

  \begin{itemize}
  \tightlist
  \item
    Consistent vs.~Inconsistent
  \item
    Possibilities for solutions
  \item
    Geometric interpretation
  \end{itemize}
\item
  Matrix Inverses

  \begin{itemize}
  \tightlist
  \item
    Detecting if a matrix is singular
  \item
    Computing the inverse

    \begin{itemize}
    \tightlist
    \item
      Formula for 2x2 case
    \item
      Augment with the identity
    \item
      Cramer's Rule
    \end{itemize}
  \end{itemize}
\item
  Vector Spaces

  \begin{itemize}
  \tightlist
  \item
    Definition in terms of closures
  \item
    Span
  \item
    Linear Independence
  \item
    Subspace and the subspace test
  \item
    Basis
  \end{itemize}
\item
  Common Computations

  \begin{itemize}
  \tightlist
  \item
    Reduction to RREF
  \item
    Eigenvalues and eigenvectors
  \item
    Basis for the column space
  \item
    Basis for the nullspace
  \item
    Basis for the eigenspace
  \item
    Construct matrix from a given linear map
  \item
    Construct change of basis matrix
  \item
    Construct matrix projection onto subspace
  \item
    Convert a basis to an orthonormal basis
  \end{itemize}
\end{itemize}

\hypertarget{things-to-compute}{%
\subsection{Things to compute}\label{things-to-compute}}

\begin{itemize}
\tightlist
\item
  Construct a matrix representing a linear map

  \begin{itemize}
  \tightlist
  \item
    With respect to the standard basis in both domain and range
  \item
    With respect to a nonstandard basis in the range
  \item
    With respect to a nonstandard basis in the domain
  \item
    With respect to nonstandard bases in both the domain and range
  \end{itemize}
\item
  Construct a change of basis matrix
\item
  Check that a map is a linear transformation
\item
  Compute the following spaces of a matrix and their orthogonal
  complements:

  \begin{itemize}
  \tightlist
  \item
    Row space
  \item
    Column space
  \item
    Null space
  \end{itemize}
\item
  Compute the shortest distance between a point and a plane
\item
  Compute the least squares solution to linear system
\item
  Prove that something is a vector space
\item
  Prove that a map is an inner product
\item
  Compute determinants
\item
  Compute the RREF of a matrix
\item
  Compute characteristic polynomials, eigenvalues, and eigenvectors
\item
  Diagonalize a matrix
\item
  Solve a system of ODEs resulting arising from tank mixing
\item
  Compute the singular value decomposition of a matrix
\item
  Compute the rank and nullity of a matrix
\item
  Convert a set of vectors to a basis
\item
  Convert a basis to an orthonormal basis
\item
  Determine if a matrix is diagonalizable
\item
  Compute the matrix for a projection onto a subspace
\item
  Find the QR factorization of a matrix
\end{itemize}

\hypertarget{things-to-prove}{%
\subsection{Things to prove}\label{things-to-prove}}

\begin{itemize}
\tightlist
\item
  Prove facts about block matrices
\item
  Prove facts about injective linear maps
\item
  Prove facts about similar matrices
\item
  Prove facts about orthogonal spaces and orthogonal complements
\item
  Prove facts about inner products
\item
  Prove facts about orthonormal sets
\item
  Prove facts about eigenvalues/eigenvectors
\item
  Understand when a matrix can be diagonalized
\item
  Prove facts about diagonalizable matrices
\item
  Prove facts about the orthogonal decomposition theorem
\end{itemize}

\hypertarget{ordinary-differential-equations}{%
\section{Ordinary Differential
Equations}\label{ordinary-differential-equations}}

\hypertarget{techniques-overview}{%
\subsection{Techniques Overview}\label{techniques-overview}}

\begin{align*}
p(y)y' = q(x) && \hspace{10em} \text{separable} \\ \\
y'+p(x)y = q(x) && \text{integrating factor} \\ \\
y' = f(x,y), f(tx,ty) = f(x,y) && y = xV(x)\text{ COV reduces to separable} \\ \\
y' +p(x)y = q(x)y^n && \text{Bernoulli, divide by $y^n$ and COV $u = y^{1-n}$} \\ \\
M(x,y)dx + N(x,y)dy = 0 && M_y = N_x: \phi(x,y) = c (\phi_x =M, \phi_y = N) \\ \\
P(D)y = f(x,y) && x^ke^{rx} \text{ for each root }
\end{align*}

\emph{Where \(e^{zx}\) yields \(e^{ax}\cos bx, e^{ax}\sin bx\)}

\hypertarget{types-of-equations}{%
\subsection{Types of Equations}\label{types-of-equations}}

\begin{itemize}
\tightlist
\item
  Separable equations:
  \begin{align*}p(y)\frac{dy}{dx} - q(x) = 0 \implies \int p(y) dy = \int q(x) dx + C\end{align*}
  \begin{align*}
  \frac{dy}{dx} = f(x)g(y) \implies \int \frac{1}{g(y)}dy = \int f(x) dx + C
  \end{align*}

  \begin{itemize}
  \tightlist
  \item
    Population growth:
    \begin{align*}\frac{dP}{dt} = kP \implies \qquad P = P_0 e^{kt}\end{align*}
  \item
    Logistic growth:

    \begin{itemize}
    \tightlist
    \item
      General form: \(\frac{dP}{dt} =(B(t) - D(t))P(t)\)
    \item
      Assume birth rate is constant \(B(t) = B_0\) and death rate is
      proportional to instantaneous population \(D(t) = D_0 P(t)\). Then
      let \(r = B_0, C = B_0/D_0\) be the \emph{carrying capacity}:
      \begin{align*}\frac{dP}{dt} = r\left( 1 - \frac{P}{C} \right)P \implies \qquad P(t) = \frac{P_0}{\frac{P_0}{C} + e^{-rt}(1 - \frac{P_0}{C})}\end{align*}
    \end{itemize}
  \end{itemize}
\item
  First order linear:
  \begin{align*}\frac{dy}{dx} + p(x)y = q(x) \implies I(x) = e^{\int p(x) dx},\qquad y(x) = \frac{1}{I(x)}\left(\int q(x)I(x) dx + C\right)\end{align*}
\item
  Exact:

  \begin{itemize}
  \tightlist
  \item
    \(M(x,y)dx + N(x,y)dy = 0 \text{ is exact } \iff \exists \phi: \frac{\partial\phi}{\partial x} = M(x, y),~\frac{\partial\phi}{\partial y} = N(x, y) \\ \iff \frac{\partial M}{\partial y} = \frac{\partial N}{x}\)
  \item
    General solution:
    \begin{align*}\phi(x, y) = \int^x M(s, y) ds + \int^y N(x, t) dt - \int^y \frac{\partial}{\partial t} \left(\int^x M(s, t) ds\right)dt\end{align*}
    (where \(\int^x f(t) dt\) means take the antiderivative of \(f\) and
    consider it a function of \(x\))
  \end{itemize}
\item
  Cauchy Euler: \#todo
\item
  Bernoulli: todo
\end{itemize}

\hypertarget{linear-homogeneous}{%
\subsection{Linear Homogeneous}\label{linear-homogeneous}}

General form:
\begin{align*}
y^{(n)} + c_{n-1} y^{(n-1)} + \cdots + c_2y'' + cy' + cy = 0 \\
p(D)y = \prod (D-r_i)^{m_i} y= 0
\end{align*} where \(p\) is a polynomial in the differential operator
\(D\) with roots \(r_i\):

\begin{itemize}
\item
  Real roots: contribute \(m_i\) solutions of the form
  \begin{align*}e^{rx}, xe^{rx}, \cdots, x^{m_i-1}e^{rx}\end{align*}
\item
  Complex conjugate roots: for \(r=a+bi\), contribute \(2m_i\) solutions
  of the form
  \begin{align*}e^{(a\pm bi)x}, xe^{(a\pm bi)x}, ~\cdots,~ x^{m_i-1}e^{(a\pm bi)x} \\
    = e^{ax}\cos(bx), e^{ax}\sin(bx),~ xe^{ax}\cos(bx), xe^{ax}\sin(bx),~ \cdots,~
    \end{align*}
\end{itemize}

Example: by cases, second order equation of the form
\begin{align*}ay'' + by' + cy = 0\end{align*} - Two distinct roots:
\(c_1 e^{r_1 x} + c_2 e^{r_2 x}\) - One real root:
\(c_1 e^{rx} + c_2 x e^{rx}\) - Complex conjugates
\(\alpha \pm i \beta\):
\(e^{\alpha x}(c_1 \cos \beta x + c_2 \sin \beta x)\)

\hypertarget{linear-inhomogeneous}{%
\subsection{Linear Inhomogeneous}\label{linear-inhomogeneous}}

General form:
\begin{align*}
y^{(n)} + c_{n-1} y^{(n-1)} + \cdots + c_2y'' + cy' + cy = F(x) \\
p(D)y = \prod (D-r_i)^{m_i} y= 0
\end{align*}

Then solutions are of the form \(y_c + y_p\), where \(y_c\) is the
solution to the associated homogeneous system and \(y_p\) is a
particular solution.

Methods of obtaining particular solutions

\hypertarget{undetermined-coefficients}{%
\subsubsection{Undetermined
Coefficients}\label{undetermined-coefficients}}

\begin{itemize}
\tightlist
\item
  Find an operator \(p(D)\) the annihilates \(F(x)\) (so \(q(D)F = 0\))
\item
  Find solution of \(q(D)p(D) = 0\), subtract of known solutions from
  homogeneous part to obtain the form of the trial solution \(A_0f(x)\),
  where \(A_0\) is the undetermined coefficient
\item
  Substitute trial solution into original equation to determine \(A_0\)
\end{itemize}

Useful Annihilators:
\begin{align*}
&F(x) = p(x): & D^{\deg(p)+1} \\
&F(x) = p(x)e^{ax}: & (D-a)^{\deg(p)+1}\\
&F(x) = \cos(ax) + \sin(ax): & D^2 + a^2\\
&F(x) = e^{ax}(a_0\cos(bx) + b_0\sin(bx)): & (D-z)(D-\conjugate{z}) = D^2 -2aD + a^2 + b^2 \\
&F(x) = p(x)e^{ax}\cos(bx) + p(x)e^{ax}\cos(bx): & \left( (D-z)(D-\conjugate{z})\right)^{\max(\deg(p), \deg(q))+ 1}
\end{align*}

\hypertarget{variation-of-parameters}{%
\subsubsection{Variation of Parameters}\label{variation-of-parameters}}

\todo[inline]{todo}

\hypertarget{reduction-of-order}{%
\subsubsection{Reduction of Order}\label{reduction-of-order}}

\todo[inline]{todo}

\hypertarget{systems-of-differential-equations}{%
\subsection{Systems of Differential
Equations}\label{systems-of-differential-equations}}

General form:
\begin{align*}
\frac{\partial \vector{x}(t) }{\partial t} = A\vector {x}(t) + \vector {b}(t) \iff \vector{x}'(t) = A\vector{x}(t) + \vector{b}(t)
\end{align*}

General solution to homogeneous equation:
\begin{align*}
c_1\vector{x_1}(t) + c_2\vector{x_2}(t)+ \cdots +c_n\vector{x_n}(t) = \vector X(t)\vector{c}
\end{align*}

If \(A\) is a matrix of constants:
\(\vector{x}(t) = e^{\lambda_i t}~\vector{v}_i\) is a solution for each
eigenvalue/eigenvector pair \((\lambda_i, \vector v_i)\) - If \(A\) is
defective, you'll need generalized eigenvectors.

Inhomogeneous Equation: particular solutions given by
\begin{align*}
\vector{x}_p(t) = \vector X(t) \int^t \vector X^{-1}(s)\vector{b}(s) ~ds
\end{align*}

\hypertarget{laplace-transforms}{%
\subsection{Laplace Transforms}\label{laplace-transforms}}

Definitions:
\begin{align*}
H_ { a } ( t ) \da \left\{ \begin{array} { l l } { 0 , } & { 0 \leq t < a } \\ { 1 , } & { t \geq a } \end{array} \right.\\
\delta(t): \int_\RR \delta(t-a)f(t)~dt &= f(a),\quad \int_\RR \delta(t-a)~dt = 1\\
(f \ast g )(t) &= \int_0^t f(t-s)g(s)~ds \\
L[f(t)] &= L[f] =\int_0^\infty e^{-st}f(t)dt = F(s)
.\end{align*} Useful property: for \(a\leq b\),
\(H_a(t) - H_b(t) = \indic{[a,b]}\).
\begin{align*}
t^n, n\in\NN \quad&\iff  &\frac{n!}{s^{n+1}},\quad &s > 0 \\
t^{-\frac{1}{2}} \quad&\iff &\sqrt{\pi} s^{-\frac{1}{2}}\quad & s>0\\
e^{at} \quad&\iff &\frac{1}{s-a},\quad &s > a \\
\cos(bt) \quad&\iff &\frac{s}{s^2+b^2},\quad &s>0 \\
\sin(bt) \quad&\iff &\frac{b}{s^2+b^2},\quad &s>0 \\
\cosh(bt) \quad&\iff &\frac{s}{s^2 - b^2},\quad &? \\
\sinh(bt) \quad&\iff &\frac{b}{s^2-b^2},\quad &? \\
\delta(t-a) \quad&\iff &e^{-as} \quad& \\
H_a(t) \quad&\iff &s^{-1}e^{-as}\quad& \\
e^{at}f(t) \quad&\iff &F(s-a)\quad & \\
H_a(t)f(t-a) \quad&\iff &e^{-as}F(s)& \\
f'(t) \quad&\iff & sL(f) - f(0) & \\
f''(t) \quad&\iff &s^2L(f) -sf(0) - f'(0) &\\
f^{(n)}(t) \quad&\iff & s^nL(f) - \sum_{i=0}^{n-1} s^{n-1-i}f^{(i)}(0) & \\
f(t)g(t) \quad&\iff &F(s) \ast G(s)\quad &
\end{align*}

\begin{itemize}
\tightlist
\item
  For \(f\) periodic with period \(T\),
  \(L(f) = \frac{1}{1+e^{-sT}}\int_0^T e^{-st}f(t)~dt\)
\end{itemize}

\begin{align*}
p(y)y' = q(x)                  &  & \hspace{10em} \text{separable} \\ \\
y'+p(x)y = q(x)                &  & \text{integrating factor} \\ \\
y' = f(x,y), f(tx,ty) = f(x,y) &  & y = xV(x)\text{ COV reduces to separable} \\ \\
y' +p(x)y = q(x)y^n            &  & \text{Bernoulli, divide by $y^n$ and COV $u = y^{1-n}$} \\ \\
M(x,y)dx + N(x,y)dy = 0        &  & M_y = N_x: \phi(x,y) = c (\phi_x =M, \phi_y = N) \\ \\
P(D)y = f(x,y)                 &  & x^ke^{rx} \text{ for each root }
\end{align*}

\begin{theorem}[First Shifting Theorem]

\begin{align*}  
L[e^{at} f(t)] = \int_0^\infty e^{(a-s)}f(t)dt = F(s-a),
.\end{align*}

\end{theorem}

\begin{remark}

The general technique for solving differential equations with Laplace
Transforms: - Take the Laplace Transform of all terms on both sides. -
Solve for \(L[y]\) in terms of \(s\). - Attempt an inverse Laplace
Transformations - This may involve partial fraction decomposition,
completing the square, and splitting numerators to match terms with
known inverse transformations.

\end{remark}

\hypertarget{systems-of-differential-equations-1}{%
\subsection{Systems of Differential
Equations}\label{systems-of-differential-equations-1}}

\begin{definition}[Wronksian]

For a collection of \(n\) functions \(f_i: \RR^n \to \RR\), define the
\(n\times 1\) column vector
\begin{align*}
W(f_i)(\vector p) \da 
\begin{bmatrix}
f_i(\vector p) \\
f_i'(\vector p) \\
f_i''(\vector p) \\
\vdots \\
f^{(n-1)}(\vector p)
\end{bmatrix}
.\end{align*}

The \textbf{Wronskian} of this collection is defined as
\begin{align*}  
W(f_1, \cdots, f_n)(\vector p) \da
\det 
\begin{bmatrix}
\vertbar & \vertbar & & \vertbar \\
W(f_1)(\vector p) & W(f_2)(\vector p) & \cdots & W(f_n)(\vector p)\\
\vertbar & \vertbar & & \vertbar \\
\end{bmatrix}
.\end{align*}

\end{definition}

\begin{proposition}[Wronskian detects linear dependence of functions]

A set of functions \(\theset{f_i}\) is linearly independent on
\(I \iff \exists x_0 \in I: W(x_0) \neq 0\).

\end{proposition}

\begin{warning}

\(W \equiv 0\) on \(I\) does \emph{not} imply that \(\theset{f_i}\) is
linearly dependent! Counterexample: \(\theset{x, x+x^2, 2x-x^2}\) where
\(W \equiv 0\) but \(x+x^2 = 3(x) + (2x-x^2)\) is a linear combination
of the other two functions.

\end{warning}

\todo[inline]{Sufficient condition: each $f_i$ is the solution to a linear homogeneous ODE $L(y) = 0$.}

\hypertarget{linear-equations-of-order-n}{%
\subsubsection{\texorpdfstring{Linear Equations of Order
\(n\)}{Linear Equations of Order n}}\label{linear-equations-of-order-n}}

The standard form of such equations is \begin{align*}
    y^{(n)} + a_1y^{(n-1)} + a_2y^{(n-2)} + \cdots +a_ny'' + a_{n-1}y' + y = F(x).
\end{align*}

All solutions will be the sum of the solution to the associated
homogeneous equation and a single particular solution.

In the homogeneous case, examine the discriminant of the characteristic
polynomial. Three cases arise:

\begin{enumerate}
    \item $D>0 \Rightarrow$ 2 Real solutions, $c_1e^{r_1x} + c_2e^{r_2x}$
    \item $D=0 \Rightarrow$ 1 Real, 1 Complex, $(c_1 +c_2x)e^{r_1x}$
    \item $D<0 \Rightarrow$ 2 Complex, $e^{ax}(c_1\cos bx + c_2\sin bx)$
\end{enumerate}

That is, every real root contributes a term of \(ce^{rx}\), while a
multiplicity of \(m\) multiplies the solution by a polynomial in \(x\)
of degree \(m-1\).

Every pair of complex roots contributes a term
\(ce^r(a\cos \omega x + b\sin \omega x)\), where \(r\) is the real part
of the roots and \(\omega\) is the complex part.

In the nonhomogeneous case, assume a solution in the most general form
of \(F(x)\), and substitute it into the equation to solve for constant
terms. For example,

\begin{enumerate}
    \item $F(x) = P(x^n) \Rightarrow y_p = a+bx+cx^2+\cdots+(n+1)x^n$
    \item $F(x) = e^x \Rightarrow y_p = Ae^x$
    \item $F(x) = A\cos (\omega x) \Rightarrow y_p = a\cos(\omega x) + b\sin(\omega x)$
\end{enumerate}

\subsection{Annihilators}

Use to reduce a nonhomogeneous equation to a homogeneous one as a
polynomial in the operator \(D\).

\begin{enumerate}
    \item $(D-a) \Rightarrow e^{ax}$
    \item $(D-a)^{k+1} \Rightarrow x^k e^{ax}, x^{k-1}e^{ax}, \cdots, e^{ax}$
    \item $D^{k+1} \Rightarrow x^k, x^{k-1}, \cdots,C$
    \item $D^2-2aD+a^2+b^2 \Rightarrow e^{ax}\cos(bx), e^{ax}\sin(bx)$
    \item $(D^2-2aD+a^2+b^2)^{k+1} \Rightarrow x^k e^{ax}\cos(bx), x^{k-1} e^{ax}\cos(bx), x^k e^{ax}\sin(bx), x^{k-1}e^{ax}\sin(bx),\cdots$
\end{enumerate}

\subsection{Complex Solutions}

\(F(x)\) of the form \(e^{ax}sin(kx)\) can be rewritten as
\(e^{(a+ki)x}\)

\hypertarget{algebra}{%
\section{Algebra}\label{algebra}}

\todo[inline]{This section is very sketchy!}

\hypertarget{to-sort}{%
\subsection{To Sort}\label{to-sort}}

\begin{itemize}
\item
  Burnside's Lemma
\item
  Cauchy's Theorem

  \begin{itemize}
  \tightlist
  \item
    If \(\abs{G} = n = \prod p_i^{k_i}\), then for each \(i\) there
    exists a subgroup \(H\) of order \(p_i\).
  \end{itemize}
\item
  The Sylow Theorems

  \begin{itemize}
  \tightlist
  \item
    If \(\abs{G} = n = \prod p_i^{k_i}\), for each \(ii\) and each
    \(1 \leq k_j \leq k_i\) then there exists a subgroup \(H\) of order
    \(p_i^{k_j}\).
  \end{itemize}
\item
  Galois Theory
\item
  More terms:
  \url{http://mathroughguides.wikidot.com/glossary:abstract-algebra}
\item
  Order \(p\): One, \(Z_p\)
\item
  Order \(p^2\): Two abelian groups, \(Z_{p^2}, Z_p^2\)
\item
  Order \(p^3\):

  \begin{itemize}
  \item
    3 abelian \(Z_{p^3}, Z_p \times Z_{p^2}. Z_p^3\),
  \item
    2 others \(Z_p \rtimes Z_{p^2}\).

    \begin{itemize}
    \tightlist
    \item
      The other is the quaternion group for p = 2 and a group of
      exponent p for p \textgreater{} 2.
    \end{itemize}
  \end{itemize}
\item
  Order \(pq\):

  \begin{itemize}
  \tightlist
  \item
    \(p \mid q-1\): Two groups, \(Z_{pq}\) and \(Z_q \rtimes Z_p\)
  \item
    Else cyclic, \(Z_{pq}\)
  \end{itemize}
\item
  Every element in a permutation group is a product of disjoint cycles,
  and the order is the lcm of the order of the cycles.
\item
  The product ideal \(IJ\) is \emph{not} just elements of the form
  \(ij\), it is all sums of elements of this form! The product alone
  isn't enough.
\item
  The intersection of any number of ideals is also an ideal
\end{itemize}

\hypertarget{big-list-of-notation}{%
\subsection{Big List of Notation}\label{big-list-of-notation}}

\begin{align*}
C(x)            = && \theset{g\in G : gxg^{-1} = x}                 && \subseteq G          && \text{Centralizer} \\
C_G(x)          = && \theset{gxg^{-1} : g\in G}                     && \subseteq G          && \text{Conjugacy Class} \\
G_x             = && \theset{g.x : x\in X}                          && \subseteq X          && \text{Orbit} \\
x_0             = && \theset{g\in G : g.x = x}                      && \subseteq G          && \text{Stabilizer} \\
Z(G)            = && \theset{x\in G: \forall g\in G,~ gxg^{-1} = x} && \subseteq G          && \text{Center} \\
\mathrm{Inn}(G) = && \theset{\phi_g(x) = gxg^{-1} }                 && \subseteq \Aut(G)    && \text{Inner Aut.} \\
\mathrm{Out}(G) = && \Aut(G) / \mathrm{Inn}(G)                      && \injects \Aut(G)     && \text{Outer Aut.} \\
N(H)            = && \theset{g\in G: gHg^{-1} = H}                  && \subseteq G          && \text{Normalizer}
\end{align*}

\hypertarget{group-theory}{%
\subsection{Group Theory}\label{group-theory}}

Notation: \(H < G\) a subgroup, \(N < G\) a normal subgroup,
concatenation is a generic group operation.

\begin{itemize}
\item
  \(\ZZ_n\) the unique cyclic group of order \(n\)
\item
  \(\mathbf{Q}\) the quaternion group
\item
  \(G^n = G\times G \times \cdots G\)
\item
  \(Z(G)\) the center of \(G\)
\item
  \(o(G)\) the order of a group
\item
  \(S_n\) the symmetric group
\item
  \(A_n\) the alternating group
\item
  \(D_n\) the dihedral group of order \(2n\)
\item
  Group Axioms

  \begin{itemize}
  \tightlist
  \item
    Closure: \(a,b \in G \implies ab \in G\)
  \item
    Identity: \(\exists e\in G \mid a\in G \implies ae = ea = a\)
  \item
    Associativity: \(a,b,c \in G \implies (ab)c = a(bc)\)
  \item
    Inverses: \(a\in G \implies \exists b \in G \mid ab =ba = e\)
  \end{itemize}
\item
  Definitions:

  \begin{itemize}
  \tightlist
  \item
    Order

    \begin{itemize}
    \tightlist
    \item
      Of a group: \(o(G) = \abs{G}\), the cardinality of \(G\)
    \item
      Of an element: \(o(g) = \min\theset{n\in \NN : g^n = e}\)
    \end{itemize}
  \item
    Index
  \item
    Center: the elements that commute with everything
  \item
    Centralizer: all elements that commute with a given
    element/subgroup.
  \item
    Group Action: a function \(f: X\times G \to G\) satisfying

    \begin{itemize}
    \tightlist
    \item
      \(x\in X, g_1,g_2 \in G \implies g_1.(g_2.x) = (g_1g_2). x\)
    \item
      \(x\in X \implies e.x = x\)
    \end{itemize}
  \item
    Orbits partition any set
  \item
    Transitive Action
  \item
    Conjugacy Class: \(C \subset G\) is a conjugacy class \(\iff\)

    \begin{itemize}
    \tightlist
    \item
      \(x\in C, g\in G \implies gxg^{-1} \in C\)
    \item
      \(x,y \in C \implies \exists g\in G : gxg^{-1} = y\)
    \item
      i.e.~subsets that are closed under \(G\) acting on itself by
      conjugation and on which the action is transitive
    \item
      i.e.~orbits under the conjugation action
    \item
      The order of any conjugacy class divides the order of \(G\)
    \end{itemize}
  \item
    \(p\)-group: Any group of order \(p^n\).
  \item
    Simple Group: no nontrivial normal subgroups
  \item
    Normal Series: \(0 \normal H_0 \normal H_1 \cdots \normal G\)
  \item
    Composition Series: The successive quotients of the normal series
  \item
    Solvable: \(G\) is solvable \(\iff\) \(G\) has an abelian
    composition series.
  \end{itemize}
\item
  One step subgroup test:
  \begin{align*}  
  a,b \in H \implies a b^{-1} \in H \\
  .\end{align*}
\item
  Useful isomorphism invariants:

  \begin{itemize}
  \tightlist
  \item
    Order profile of elements: \(n_1\) elements of order \(p_1\),
    \(n_2\) elements of order \(p_2\), etc

    \begin{itemize}
    \tightlist
    \item
      Useful to look at elements of order \(2\)!
    \end{itemize}
  \item
    Order profile of subgroups
  \item
    \(Z(A) \cong Z(B)\)
  \item
    Number of generators (generators are sent to generators)
  \item
    Number and size of conjugacy classes
  \item
    Number of Sylow\(\dash p\) subgroups.
  \item
    Commutativity
  \item
    ``Being cyclic''
  \item
    Automorphism Groups
  \item
    Solvability
  \item
    Nilpotency
  \end{itemize}
\item
  Useful homomorphism invariants

  \begin{itemize}
  \tightlist
  \item
    \(\phi(e) = e\)
  \item
    \(\abs{g} = m < \infty \implies \abs{\phi(g)} = m\)
  \item
    Inverses, i.e.~\(\phi(a)^{-1} = \phi(a^{-1})\)
  \item
    \(H < G \implies \phi(H) < G'\)

    \begin{itemize}
    \tightlist
    \item
      \(H' < G' \implies \phi^{-1}(H') < G\)
    \end{itemize}
  \item
    \(\abs{G} < \infty \implies \phi(G)\) divides \(\abs{G}, \abs{G'}\)
  \end{itemize}
\end{itemize}

\hypertarget{big-theorems-2}{%
\subsection{Big Theorems}\label{big-theorems-2}}

\begin{itemize}
\item
  Classification of Abelian Groups
  \begin{align*}  
  G \cong \ZZ_{p_1^{k_1}} \oplus \ZZ_{p_2^{k_2}} \oplus \cdots \oplus \ZZ_{p_n^{k_n}}
  ,\end{align*} where \((p_i, k_i)\) are the set of elementary divisors
  of \(G\).
\item
  Isomorphism Theorems
\end{itemize}

\begin{align*}
\phi: G \to G \implies             && \frac{G}{\ker{\phi}} \cong       &~ \phi(G) \\
H \normal G,~ K < G \implies        && \frac{K}{H\intersect K} \cong    &~ \frac{HK}{H} \\
H,K \normal G,~ K < H \implies  && \frac{G/K}{H/K} \cong            &~ \frac{G}{H}
\end{align*}

\begin{itemize}
\item
  Lagrange's Theorem: \(H < G \implies o(H) \mid o(G)\)

  \begin{itemize}
  \tightlist
  \item
    Converse is false: \(o(A_4) = 12\) but has no order 6 subgroup.
  \end{itemize}
\item
  The \(GZ\) Theorem: \(G/Z(G)\) cyclic implies that
  \(G \in \mathbf{Ab}\).
\item
  Orbit Stabilizer Theorem: \(G / x_0 \cong Gx\)
\item
  The Class Equation

  \begin{itemize}
  \tightlist
  \item
    Let \(G\actson X\) and \(\mathcal{O}_i \subseteq X\) be the
    nontrivial orbits, then
    \begin{align*}  
    \abs{X} = \abs{ X_0 } + \sum_{[x_i] \in X/G} \abs{Gx} 
    .\end{align*}
  \item
    The right hand side is the number of fixed points, plus a sum over
    all of the orbits of size greater than 1, where any representative
    within the orbit is chosen and we look at the index of its
    stabilizer in \(G\).
  \item
    Let \(G\actson G\) and for each nontrivial conjugacy class \(C_G\)
    choose a representative \([x_i] = C_G = C_G(x_i)\) to obtain
  \end{itemize}

  \begin{align*}  
  \abs{G} = \abs{Z(G)} + \sum_{[x_i] = C_G(x_i)} \left[ G: [x_i] \right]
  .\end{align*}
\item
  Useful facts:

  \begin{itemize}
  \tightlist
  \item
    \(H < G \in \mathbf{Ab} \implies H \normal G\)

    \begin{itemize}
    \tightlist
    \item
      Converse doesn't hold, even if all subgroups are normal.
      Counterexample: \(\mathbf{Q}\)
    \end{itemize}
  \item
    \(G / Z(G) \cong \mathrm{Inn}(G)\)
  \item
    \(H, K < G\) with \(H \cong K \not\implies G/H \cong G/K\)

    \begin{itemize}
    \tightlist
    \item
      Counterexample:
      \(G = \ZZ_4 \cross \ZZ_2, H = <(0,1)>, K = <(2,0)>\). Then
      \(G/H \cong \ZZ_4 \not\cong \ZZ_2^2 \cong G/K\)
    \end{itemize}
  \item
    \(G\in\mathbf{Ab} \implies\) for each \(p\) dividing \(o(G)\), there
    is an element of order \(p\)
  \item
    Any surjective homomorphism \(\phi: A \surjects B\) where
    \(o(A) = o(B)\) is an isomorphism
  \item
    If \(G\) is abelian, for each \(d\mid \abs{G}\) there is exactly one
    subgroup of order \(d\).
  \end{itemize}
\item
  Sylow Subgroups:

  \begin{itemize}
  \tightlist
  \item
    Todo
  \end{itemize}
\item
  Big List of Interesting Groups

  \begin{itemize}
  \tightlist
  \item
    \(\ZZ_4, \ZZ_2^2\)
  \item
    \(D_4\)
  \item
    \(Q = \langle a , b | a ^ { 4 } = 1 , a ^ { 2 } = b ^ { 2 } , a b = b a ^ { 3 } \rangle\)
    the quaternion group
  \item
    \(S^3\), the smallest nonabelian group
  \end{itemize}
\item
  Chinese Remainder Theorem:
  \begin{align*}
  \ZZ_{pq} \cong \ZZ_p \oplus \ZZ_q \iff (p,q) = 1
  \end{align*}

  \begin{itemize}
  \tightlist
  \item
    Fundamental Theorem of Finitely Generated Abelian Groups:
  \item
    \(G = \ZZ^n \oplus \bigoplus \ZZ_{q_i}\)
  \end{itemize}
\item
  Finding all of the unique groups of a given order: \#todo
\end{itemize}

\hypertarget{cyclic-groups}{%
\subsubsection{Cyclic Groups}\label{cyclic-groups}}

\begin{itemize}
\tightlist
\item
  Generated by ?
\item
  For each \(d\) dividing \(o(G)\), there exists a subgroup \(H\) of
  order \(d\).

  \begin{itemize}
  \tightlist
  \item
    If \(G = <a>\), then take \(H = <a^{\frac{n}{d}}>\)
  \end{itemize}
\end{itemize}

\hypertarget{the-symmetric-group}{%
\subsubsection{The Symmetric Group}\label{the-symmetric-group}}

\begin{itemize}
\tightlist
\item
  Generated by:

  \begin{itemize}
  \tightlist
  \item
    Transpositions
  \item
    \#todo
  \end{itemize}
\item
  Cycle types: characterized by the number of elements in the cycle.

  \begin{itemize}
  \tightlist
  \item
    Two elements are in the same conjugacy class \(\iff\) they have the
    same cycle type.
  \end{itemize}
\item
  Inversions: given \(\tau = (p_1 \cdots p_n)\), a pair \(p_i, p_j\) is
  \emph{inverted} iff \(i < j\) but \(p_j < p_i\)
\item
  Can count inversions \(N(\tau)\)

  \begin{itemize}
  \tightlist
  \item
    Equal to minimum number of transpositions to obtain non-decreasing
    permutation
  \end{itemize}
\item
  Sign of a permutation: \(\sigma(\tau) = (-1)^{N(\tau)}\)
\item
  Parity of permutations \(\cong (\ZZ, +)\)

  \begin{itemize}
  \tightlist
  \item
    even \(\circ\) even = even
  \item
    odd \(\circ\) odd = even
  \item
    even \(\circ\) odd = odd
  \end{itemize}
\end{itemize}

\hypertarget{ring-theory}{%
\subsection{Ring Theory}\label{ring-theory}}

\todo[inline]{Ring Axioms}

\begin{itemize}
\tightlist
\item
  Examples:
\item
  Non-Examples:
\item
  Definition of an Ideal
\item
  Definitions of types of rings:

  \begin{itemize}
  \tightlist
  \item
    Field
  \item
    Unique Factorization Domain (UFD)
  \item
    Principal Ideal Domain (PID)
  \item
    Euclidean Domain:
  \item
    Integral Domain
  \item
    Division Ring
    \begin{align*}  
    \text{field} \implies \text{Euclidean Domain} \implies \text{PID} \implies \text{UFD} \implies \text{integral domain}
    .\end{align*}
  \end{itemize}
\item
  Counterexamples to inclusions are strict:

  \begin{itemize}
  \tightlist
  \item
    An ED that is not a field:
  \item
    A PID that is not an ED: \(\QQ[\sqrt {19}]\)
  \item
    A UFD that is not a PID:
  \item
    An integral domain that is not a UFD:
  \end{itemize}
\item
  Integral Domains
\item
  Unique Factorization Domains
\item
  Prime Elements
\item
  Prime Ideals
\item
  Field Extensions
\item
  The Chinese Remainder Theorem for Rings
\item
  Polynomial Rings

  \begin{itemize}
  \tightlist
  \item
    Irreducible Polynomials

    \begin{itemize}
    \tightlist
    \item
      Over \(\ZZ_2\):~
      \begin{align*}  
      x,~ x+1,~ x^2+x+1,~ x^3+x+1,~ x^3+x^2+1 
      .\end{align*}
    \item
      Eisenstein's Criterion
    \end{itemize}
  \end{itemize}
\item
  Gauss' Lemma
\end{itemize}

\todo[inline]{When is $\QQ(\sqrt d)$ a field?}

\hypertarget{number-theory}{%
\section{Number Theory}\label{number-theory}}

\hypertarget{notation-and-basic-definitions}{%
\subsection{Notation and Basic
Definitions}\label{notation-and-basic-definitions}}

\begin{align*}  
(a, b) \da \gcd(a, b) && \text{the greatest common divisor} \\
\ZZ_n && \text{the ring of integers} \mod n \\
\ZZ_n\units && \text{the group of units}\mod n
.\end{align*}

\begin{definition}[Multiplicative Functions]

A function \(f:\ZZ \to \ZZ\) is said to be \textbf{multiplicative} iff
\begin{align*}  
(a, b) = 1 \implies f(ab) = f(a) f(b)
.\end{align*}

\end{definition}

\hypertarget{big-theorems-3}{%
\subsection{Big Theorems}\label{big-theorems-3}}

\todo[inline]{Link to theorems.}

\hypertarget{primes}{%
\subsection{Primes}\label{primes}}

\begin{theorem}[The fundamental theorem of arithmetic]

Every \(n\in \ZZ\) can be written uniquely as
\begin{align*}
n = \prod_{i=1}^m p_i^{k_i}
\end{align*} where the \(p_i\) are the \(m\) distinct prime divisors of
\(n\).

\end{theorem}

\begin{remark}

Note that the number of distinct prime factors is \(m\), while the total
number of factors is \(\prod_{i=1}^m(k_i + 1)\).

\end{remark}

\hypertarget{divisibility}{%
\subsection{Divisibility}\label{divisibility}}

\begin{definition}[Divisibility]

\begin{align*}
a\divides b \iff b \equiv 0 \mod a \iff \exists k \text{ such that } ak = b
\end{align*}

\end{definition}

\hypertarget{gcd-lcm}{%
\subsubsection{\texorpdfstring{\(\gcd, \lcm\)}{\textbackslash gcd, \textbackslash lcm}}\label{gcd-lcm}}

\begin{remark}

\(\gcd(a, b)\) can be computed by taking prime factorizations of \(a\)
and \(b\), intersecting the primes occurring, and taking the lowest
exponent that appears. Dually, \(\lcm(a, b)\) can be computed by taking
the \emph{union} and the \emph{highest} exponent. \todo[inline]{Check}

\end{remark}

\begin{proposition}[Relationship between $\gcd$ and $\lcm$]

\begin{align*}
xy = \gcd{(x,y)}~\mathrm{lcm}{(x,y)}
\end{align*}

\end{proposition}

\begin{proposition}[?]

If \(d\mid x\) and \(d\mid y\), then
\begin{align*}
\gcd(x,y) &= d\cdot \gcd\qty{ \frac x d, \frac y d} \\
\lcm(x,y) &= d\cdot \lcm \qty{ \frac x d, \frac y d}
\end{align*} \todo[inline]{Check}

\end{proposition}

\begin{proposition}[Useful properties of $\gcd$]

\begin{align*}  
\gcd(x, y, z) &= \gcd(\gcd(x,y), z) \\
\gcd(x, y)    &= \gcd(x\bmod y, y) \\
\gcd(x,y)     &= \gcd(x-y, y)
.\end{align*}

\end{proposition}

\hypertarget{the-euclidean-algorithm}{%
\subsubsection{The Euclidean Algorithm}\label{the-euclidean-algorithm}}

\(\gcd(a, b)\) can be computed via the Euclidean algorithm, taking the
final bottom-right coefficient.

\todo[inline]{Example of Euclidean algorithm,}

\hypertarget{modular-arithmetic}{%
\subsection{Modular Arithmetic}\label{modular-arithmetic}}

Generally concerned with the multiplicative group \((\ZZ_n, \times)\).

\begin{theorem}[The Chinese Remainder Theorem]

The system
\begin{align*}
\begin{array} { c } { x \equiv a _ { 1 }   \mod m _ { 1 }  } \\ { x \equiv a _ { 2 }   \mod m _ { 2 }  } \\ { \vdots } \\ { x \equiv a _ { r }   \mod m _ { r }  } \end{array}
\end{align*}

has a unique solution \(x \mod \prod m_i \iff (m_i, m_j) = 1\) for each
pair \(i,j\), given by
\begin{align*}  
x = \sum_{j=1}^r a_j \frac{\prod_i m_i}{m_j} \left[ \frac{\prod_i m_i}{m_j} \right]^{-1}_{\mod m_j}
.\end{align*}

\end{theorem}

\begin{theorem}[Euler's Theorem]

\begin{align*}  
a^{\phi(p)} \equiv 1 \mod n
.\end{align*}

\end{theorem}

\begin{theorem}[Fermat's Little Theorem]

\begin{align*}
x^{p}  &\equiv x \mod p \\
x^{p-1} &\equiv 1 \mod p \quad \text{ if } p \notdivides a
\end{align*}

\end{theorem}

\hypertarget{diophantine-equations}{%
\subsubsection{Diophantine Equations}\label{diophantine-equations}}

\begin{proposition}[Solutions to linear Diophantine equations]

Consider \(ax + by = c\). This has solutions iff
\(c = 0 \mod (a,b) \iff \gcd(a,b) \text{ divides } c\).

\todo[inline]{How to obtain solutions.}

\end{proposition}

\hypertarget{computations}{%
\subsubsection{Computations}\label{computations}}

\begin{proposition}[?]

If \(x\equiv 0 \mod n\), then \(x\equiv 0 \mod p^k\) for all \(p^k\)
appearing in the prime factorization of \(n\).

\end{proposition}

\begin{remark}

If there are factors of the modulus in the equation, peel them off with
addition, using the fact that \(nk \equiv 0 \mod n\).
\begin{align*}
x 
&\equiv nk + r \mod n  \\
&\equiv r \mod n
\end{align*}

So take \(x=463, n = 4\), then use \(463 = 4\cdot 115 + 4\) to write
\begin{align*}  
463 &\equiv y \mod 4 \\
\implies 4\cdot 115 + 3 &\equiv y \mod 4 \\
\implies  3&\equiv y\mod 4
.\end{align*}

\end{remark}

\begin{proposition}[Repeated square/fast exponentiation]

For any \(n\),
\begin{align*}  
x^k \mod n \equiv (x^{k/d} \bmod n)^d \mod n
.\end{align*}

\end{proposition}

\begin{example}[?]

\begin{align*}
2^{25} 
&\equiv (2^5 \mod 5)^5 \mod 5 \\
&\equiv 2^5  \mod 5 \\
&\equiv 2 \mod 5 
\end{align*}

\end{example}

\begin{remark}

Make things easier with negatives! For example, \(\mod 5\),
\begin{align*}
4^{25} 
&\equiv (-1)^{25} \mod 5\\
&\equiv (-1) \mod 5\\
&\equiv 4 \mod 5
\end{align*}

\end{remark}

\hypertarget{invertibility}{%
\subsubsection{Invertibility}\label{invertibility}}

\begin{proposition}[Reduction of modulus]

\begin{align*}  
xa = xb \mod n \implies a = b \mod \frac{n}{(x,n)}
.\end{align*}

\end{proposition}

\begin{proposition}[Characterization of invertibility]

\begin{align*}  
x\in \ZZ_n\units \iff (x, n) = 1
,\end{align*} and thus
\begin{align*}
\ZZ_n^\times = \theset{1\leq x \leq n : (x,n) = 1}
\end{align*} and \(\abs{\ZZ_n^\cross} = \phi(n)\).

\end{proposition}

\begin{example}[Using invertibility]

One can reduce equations by dividing through by a unit. Pick any \(x\)
such that \(x\divides a\) and \(x\divides b\) with \((x,n) = 1\), then
\begin{align*}  
a =b \mod n \implies \frac a x = \frac b x \mod n
.\end{align*}

\end{example}

\hypertarget{the-totient-function}{%
\subsection{The Totient Function}\label{the-totient-function}}

\begin{definition}[Euler's Totient Function]

\begin{align*}
\phi(n) = \abs{\theset{1\leq x \leq n : (x,n) = 1}}
\end{align*}

\end{definition}

\begin{example}[?]

\begin{align*}
\phi(1) & = \abs{\theset{1}} = 1 \\
\phi(2) & = \abs{\theset{1}} = 1 \\
\phi(3) & = \abs{\theset{1,2}} = 2 \\
\phi(4) & = \abs{\theset{1,3}} = 2 \\
\phi(5) & = \abs{\theset{1,2,3,4}} = 4
\end{align*}

\end{example}

\begin{proposition}[Formulas involving the totient]

\begin{align*}
\phi(p)   & = p-1 \\
\phi(p^k) & = p^{k-1}(p-1) \\
\phi(n) &= n\prod_{i=1}^{?} \qty{1 - {1\over p_i}} \\
n &= \sum_{\tiny d\divides n} \phi(d) 
\end{align*}

\end{proposition}

\begin{proof}[?]

All numbers less than \(p\) are coprime to \(p\); there are \(p^k\)
numbers less than \(p^k\) and the only numbers \emph{not} coprime to
\(p^k\) are multiples of \(p\), i.e.~\(\theset{p, p^2, \cdots p^{k-1}}\)
of which there are \(k-1\), yielding \(p^k - p^{k-1}\)

Along with the fact that \(\phi\) is multiplicative, so
\((p,q) = 1 \implies \phi(pq) = \phi(p)\phi(q)\), compute this for any
\(n\) by taking the prime factorization.

With these properties, one can compute:
\begin{align*}
\phi(n) 
&= \phi\qty{ \prod_i p_i^{k_i}} \\
&= \prod_i p_i^{k_i-1}(p_i-1) \\ 
&= n \left(\frac{\prod_i (p_i-1)}{\prod_i p_i}\right) \\ 
&= n\prod_i \qty{ 1 - \frac{1}{p_i}}
\end{align*}

\textbackslash todo{[}inline{]}\{Check and explain\}

\end{proof}

\hypertarget{quadratic-residues}{%
\subsection{Quadratic Residues}\label{quadratic-residues}}

\begin{definition}[Quadratic Residue]

\(x\) is a \textbf{quadratic residue} \(\bmod n\) iff there exists an
\(a\) such that \(a^2 = x \mod n\).

\end{definition}

\begin{proposition}[?]

In \(\ZZ_p\), exactly half of the elements (even powers of generator)
are quadratic residues.

\end{proposition}

\begin{proposition}[?]

\begin{align*}  
-1\text{ is a quadratic residue in } \ZZ_p \iff p = 1 \mod 4
.\end{align*}

\end{proposition}

\begin{definition}[The Jacobi Symbols]

\todo[inline]{todo}

\end{definition}

\begin{definition}[The Legendre Symbol]

\todo[inline]{todo}

\end{definition}

\hypertarget{primality-tests}{%
\subsection{Primality Tests}\label{primality-tests}}

\begin{proposition}[Fermat Primality Test]

If \(n\) is prime, then
\begin{align*}
a^{n-1} = 1 \mod n
\end{align*}

\end{proposition}

\begin{proposition}[Miller-Rabin Primality Test]

\(n\) is prime iff
\begin{align*}
x^2 = 1 \mod n \implies x = \pm 1
\end{align*}

\end{proposition}

\hypertarget{sequences-in-metric-spaces}{%
\subsection{Sequences in Metric
Spaces}\label{sequences-in-metric-spaces}}

\begin{theorem}[Bolzano-Weierstrass]

Every bounded sequence has a convergent subsequence.

\end{theorem}

\begin{theorem}[Heine-Borel]

In \(\RR^n, X\) is compact \(\iff X\) is closed and bounded.

\end{theorem}

\begin{remark}

Necessity of \(\RR^n\): \(X = (\ZZ, d(x,y) = 1)\) is closed, complete,
bounded, but not compact since \(\theset{1,2,\cdots}\) has no convergent
subsequence

\end{remark}

\begin{proposition}[Converse of Heine-Borel]

Converse holds iff bounded is replaced with totally bounded

\end{proposition}

\hypertarget{sequences}{%
\section{Sequences}\label{sequences}}

Notation: \(\theset{a_n}_{n\in\NN}\) is a \textbf{sequence},
\(\displaystyle\sum_{i\in\NN} a_i\) is a \textbf{series}.

\hypertarget{known-examples}{%
\subsection{Known Examples}\label{known-examples}}

\begin{itemize}
\tightlist
\item
  Known sequences: let \(c\) be a constant.
  \begin{align*}
  c, c^2, c^3, \ldots 
  &= \theset{c^n}_{n=1}^\infty \to 0 
  && \forall \abs{c} < 1 \\ \\ 
  \frac{1}{c},\frac{1}{c^2},\frac{1}{c^3},\ldots 
  &= \theset{\frac{1}{c^n}}_{n=1}^\infty \to 0 
  &&\forall  \abs{c} > 1 \\ \\
  1,\frac{1}{2^c},\frac{1}{3^c},\ldots 
  &= \theset{\frac{1}{n^c}}_{n=1}^\infty \to 0 
  && \forall c > 0
  \end{align*}
\end{itemize}

\hypertarget{convergence}{%
\subsection{Convergence}\label{convergence}}

\begin{definition}[Convergence of a Sequence]

A sequence \(\ts{x_j}\) \textbf{converges} to \(L\) iff
\begin{align*}  
\forall \varepsilon > 0,\, \exists N > 0 \text{ such that } \quad n\geq N \implies \abs{x_n - L} < \eps
.\end{align*}

\end{definition}

\begin{theorem}[Squeeze Theorem]

\begin{align*}
b_n \leq a_n \leq c_n \text{ and } b_n,c_n \to L \implies a_n \to L
\end{align*}

\end{theorem}

\begin{theorem}[Monotone Convergence Theorem for Sequences]

If \(\theset{a_j}\) monotone and bounded, then
\(a_j \to L = \lim\sup a_i < \infty\).

\end{theorem}

\begin{theorem}[Cauchy Criteria]

\(\abs{a_m - a_n} \to 0 \in \RR \implies \theset{a_i}\) converges.

\end{theorem}

\hypertarget{checklist}{%
\subsubsection{Checklist}\label{checklist}}

\begin{itemize}
\item
  Is the sequence bounded?

  \begin{itemize}
  \tightlist
  \item
    \(\theset{a_i}\) not bounded \(\implies\) not convergent
  \item
    If bounded, is it monotone?

    \begin{itemize}
    \tightlist
    \item
      \(\theset{a_i}\) bounded and monotone \(\implies\) convergent
    \end{itemize}
  \end{itemize}
\item
  Use algebraic properties of limits
\item
  Epsilon-delta definition
\item
  Algebraic properties and manipulation:

  \begin{itemize}
  \item
    Limits commute with \(\pm, \times, \div\) and \(\lim C = C\) for
    constants.

    \begin{itemize}
    \item
      E.g. Divide all terms by \(n\) before taking limit
    \item
      Clear denominators
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{sums-series}{%
\section{Sums (``Series'')}\label{sums-series}}

\begin{definition}[Series]

A \textbf{series} is an function of the form
\begin{align*}  
f(x) = \sum_{j=1}^\infty c_j x^j
.\end{align*}

\end{definition}

\hypertarget{known-examples-1}{%
\subsection{Known Examples}\label{known-examples-1}}

\hypertarget{conditionally-convergent}{%
\subsubsection{Conditionally
Convergent}\label{conditionally-convergent}}

\begin{align*}
\sum_{k=1}^\infty k^p &< \infty 
&&\iff p \leq 1 \\
\sum_{k=1}^\infty \frac{1}{k^p} &< \infty 
&&\iff p > 1 \\
\sum_{k=1}^\infty \frac{1}{k} &= \infty 
&&
\end{align*}

\hypertarget{convergent}{%
\subsubsection{Convergent}\label{convergent}}

\begin{align*}
\sum_{n=1}^\infty \frac{1}{n^2}           & < \infty \\
\sum_{n=1}^\infty \frac{1}{n^3}           & < \infty \\
\sum_{n=1}^\infty \frac{1}{n^\frac{3}{2}} & < \infty \\
\sum_{n=1}^\infty \frac{1}{n!}            & = e \\
\sum_{n=1}^\infty \frac{1}{c^n}           & = \frac{c}{c-1} \\
\sum_{n=1}^\infty (-1)^n \frac{1}{c^n}    & = \frac{c}{c+1} \\
\sum_{n=1}^\infty (-1)^n \frac{1}{n}      & = \ln 2
\end{align*}

\hypertarget{divergent}{%
\subsubsection{Divergent}\label{divergent}}

\begin{align*}
\sum_{n=1}^\infty \frac{1}{n} = \infty \\
\sum_{n=1}^\infty \frac{1}{\sqrt n} = \infty 
\end{align*}

\hypertarget{convergence-1}{%
\subsection{Convergence}\label{convergence-1}}

\begin{quote}
Useful reference:
\url{http://math.hawaii.edu/~ralph/Classes/242/SeriesConvTests.pdf}
\end{quote}

\begin{definition}[Absolutely Convergent]

\todo[inline]{todo}

\end{definition}

\begin{remark}

\(a_n\to 0\) does not imply \(\sum a_n < \infty\). Counterexample: the
harmonic series.

\end{remark}

\begin{proposition}[?]

Absolute convergence \(\implies\) convergence

\end{proposition}

\begin{proposition}[The Cauchy Criterion]

\begin{align*}
\limsup a_i \to 0 \implies \sum a_i \text{ converges }
\end{align*}

\end{proposition}

\hypertarget{the-big-tests}{%
\subsubsection{The Big Tests}\label{the-big-tests}}

\begin{theorem}[Comparison Test]

\envlist

\begin{itemize}
\tightlist
\item
  \(a_n < b_n \and \sum b_n < \infty \implies \sum a_n < \infty\)
\item
  \(b_n < a_n \and \sum b_n = \infty \implies \sum a_n = \infty\)
\end{itemize}

\end{theorem}

\begin{theorem}[Ratio Test]

\begin{align*}
R =\lim_{n\to\infty} \abs{\frac{a_{n+1}}{a_n}}
\end{align*}

\begin{itemize}
\tightlist
\item
  \(R < 1\): absolutely convergent
\item
  \(R > 1\): divergent
\item
  \(R = 1\): inconclusive
\end{itemize}

\end{theorem}

\begin{theorem}[Root Test]

\begin{align*}
R = \limsup_{n \to \infty} \sqrt[n]{\abs{a_n}}
\end{align*} - \(R < 1\): convergent - \(R > 1\): divergent - \(R = 1\):
inconclusive

\end{theorem}

\begin{theorem}[Integral Test]

\begin{align*}
f(n) = a_n \implies \sum a_n < \infty \iff \int_1^\infty f(x) dx < \infty
\end{align*}

\end{theorem}

\begin{theorem}[Limit Test]

\begin{align*}
\lim_{n\to\infty}\frac{a_n}{b_n} = L < \infty \implies \sum a_n < \infty \iff \sum b_n < \infty
\end{align*}

\end{theorem}

\begin{theorem}[Alternating Series Test]

\begin{align*}
a_n \downarrow 0 \implies \sum (-1)^n a_n < \infty
\end{align*}

\end{theorem}

\begin{theorem}[Weierstrass $M\dash$Test]

\begin{align*}
\sum_{n=1}^\infty \norm{f_n}_\infty < \infty 
\implies \exists f\text{ such that } \norm{ \sum_{n=1}^\infty f_n - f}_\infty \to 0
\end{align*} In other words, the series converges uniformly.

\begin{quote}
Slogan: Convergence of the sup norms implies uniform convergence"
\end{quote}

\end{theorem}

\begin{remark}

The \(M\) in the name comes from defining
\(\sup\theset{f_k(x)} \da M_n\) and requiring
\(\sum \abs{M_n} < \infty\).

\end{remark}

\hypertarget{checklist-1}{%
\subsubsection{Checklist}\label{checklist-1}}

\begin{itemize}
\tightlist
\item
  Do the terms tend to zero?

  \begin{itemize}
  \tightlist
  \item
    \(a_i \not\to 0 \implies \sum a_i = \infty\).

    \begin{itemize}
    \tightlist
    \item
      Can check with L'Hopital's rule
    \end{itemize}
  \end{itemize}
\item
  There are exactly 6 tests at our disposal:

  \begin{itemize}
  \tightlist
  \item
    Comparison, root, ratio, integral, limit, alternating
  \end{itemize}
\item
  Is the series alternating?

  \begin{itemize}
  \tightlist
  \item
    If so, does \(a_n \downarrow 0\)?

    \begin{itemize}
    \tightlist
    \item
      If so, \textbf{convergent}
    \end{itemize}
  \end{itemize}
\item
  Is this series bounded above by a known convergent series?

  \begin{itemize}
  \tightlist
  \item
    \(p\) series with \(p>1\), i.e.~:
    \(\sum a_n \leq \sum \frac{1}{n^p} < \infty\)
  \item
    Geometric series with \(\abs{x} < 1\),
    i.e.~\(\sum a_n \leq \sum x^n\)
  \end{itemize}
\item
  Is this series bounded below by a known divergent series?

  \begin{itemize}
  \tightlist
  \item
    \(p\) series with \(p\leq 1\),
    i.e.~\(\infty = \sum \frac{1}{n^p} \leq \sum a_i\)
  \end{itemize}
\item
  Are the ratios strictly less than or greater than 1?

  \begin{itemize}
  \tightlist
  \item
    \(<1 \implies\) \textbf{convergent}
  \item
    \(>1 \implies\) \textbf{convergent}
  \end{itemize}
\item
  Does the integral analogue converge?

  \begin{itemize}
  \tightlist
  \item
    Integral converges \(\iff\) sum converges
  \end{itemize}
\item
  Try the root test

  \begin{itemize}
  \tightlist
  \item
    \(<1 \implies\) \textbf{convergent}
  \item
    \(>1 \implies\) \textbf{convergent}
  \end{itemize}
\item
  Try the limit test

  \begin{itemize}
  \tightlist
  \item
    Attempt to divide each term to obtain a known convergent/divergent
    series
  \end{itemize}
\end{itemize}

Some Pattern Recognition:

\begin{itemize}
\tightlist
\item
  \((\text{stuff})!\): Ratio test (only test that will work with
  factorials!!)
\item
  \((\text{stuff})^n\): Root test or ratio test
\item
  Replace \(a_n\) with an \(f(x)\) that's easy to integrate - integral
  test
\item
  \(p(x)\) or \(\sqrt{p(x)}\): comparison or limit test
\end{itemize}

\hypertarget{radius-of-convergence}{%
\subsection{Radius of Convergence}\label{radius-of-convergence}}

\begin{proposition}[Finding the radius of convergence]

Use the fact that
\begin{align*}
\lim_{k\to\infty} \abs{\frac{a_{k+1}x^{k+1}}{a_kx^k}} 
= \abs{x}\lim_{k\to\infty} \abs{\frac{a_{k+1}}{a_k}} < 1 
\implies \sum a_k x^k < \infty
,\end{align*} so take \(L \da \lim_{k\to\infty} \frac{a_{k+1}}{a_k}\)
and then obtain the radius as
\begin{align*}
R = \frac{1}{L} = \lim_{k\to\infty} {a_k \over a_{k+1}}
\end{align*}

\end{proposition}

\begin{remark}

\envlist

\begin{itemize}
\tightlist
\item
  Note \(L=0 \implies\) absolutely convergent everywhere
\item
  \(L = \infty \implies\) convergent only at \(x=0\).
\item
  Also need to check endpoints \(R, -R\) manually.
\end{itemize}

\end{remark}

\hypertarget{real-analysis}{%
\section{Real Analysis}\label{real-analysis}}

\hypertarget{notation-3}{%
\subsection{Notation}\label{notation-3}}

\begin{definition}[Continuously Differentiable]

A function is \textbf{continuously differentiable} iff \(f\) is
differentiable and \(f'\) is continuous.

Conventions:

\begin{itemize}
\tightlist
\item
  \emph{Integrable} means \emph{Riemann integrable}.
\end{itemize}

\end{definition}

\begin{align*}  
f && \text{a functional }\RR^n \to \RR \\
\vector{f} && \text{a function } \RR^n\to \RR^m \\
A, E, U, V && \text{open sets} \\
A' && \text{the limit points of }A \\
\bar{A} && \text{the closure of }A \\
A\interior\da A\sm A' && \text{the interior of }A \\
K && \text{a compact set} \\
\mathcal{R}_A && \text{the space of Riemann integral functions on }A \\
C^j(A) && \text{the space of }j\text{ times continuously differentiable functions }f: \RR^n \to \RR \\
\ts{f_n} && \text{a sequence of functions} \\
\ts{x_n} && \text{a sequence of real numbers}\\
f_n \to f && \text{pointwise convergence} \\
f_n \uniformlyconverges f && \text{uniform convergence} \\
x_n \increasesto x && x_i\leq x_j \text{ and }x_j\text{ converges to }x \\
x_n \decreasesto x && x_i\geq x_j \text{ and }x_j\text{ converges to }x \\
\sum_{k\in \NN} f_k && \text{a series}\\
D(f) && \text{the set of discontinuities of }f
.\end{align*}

\hypertarget{big-ideas}{%
\subsection{Big Ideas}\label{big-ideas}}

Summary for GRE:

\begin{itemize}
\item
  Limits,
\item
  Continuity,
\item
  Boundedness,
\item
  Compactness,
\item
  Definitions of topological spaces,
\item
  Lipschitz continuity
\item
  Sequences and series of functions.
\item
  Know the interactions between the following major operations:

  \begin{itemize}
  \tightlist
  \item
    Continuity (pointwise limits)
  \item
    Differentiability
  \item
    Integrability
  \item
    Limits of sequences
  \item
    Limits of series/sums
  \end{itemize}
\item
  The derivative of a continuous function need not be continuous
\item
  A continuous function need not be differentiable
\item
  A uniform limit of differentiable functions need not be differentiable
\item
  A limit of integrable functions need not be integrable
\item
  An integrable function need not be continuous
\item
  An integrable function need not be differentiable
\end{itemize}

\begin{theorem}[Generalized Mean Value Theore]

\begin{align*}
f,g\text{ differentiable on } [a,b] \implies \exists c\in[a,b] :
\left[f ( b ) - f ( a ) \right] g' ( c ) = \left[g ( b ) - g ( a )\right] f' ( c )
\end{align*}

\end{theorem}

\begin{corollary}[Mean Value Theorem]

? \todo[inline]{todo}

\end{corollary}

\hypertarget{important-examples}{%
\subsection{Important Examples}\label{important-examples}}

\hypertarget{limits-1}{%
\subsection{Limits}\label{limits-1}}

\todo[inline]{todo}

\hypertarget{commuting-limits}{%
\subsection{Commuting Limits}\label{commuting-limits}}

\begin{itemize}
\tightlist
\item
  Suppose \(f_n \to f\) (pointwise, not necessarily uniformly)
\item
  Let \(F(x) = \int f(t)\) be an antiderivative of \(f\)
\item
  Let \(f'(x) = \frac{\partial f}{\partial x}(x)\) be the derivative of
  \(f\).
\end{itemize}

Then consider the following possible ways to commute various limiting
operations:

Does taking the derivative of the integral of a function always return
the original function?
\begin{align*}
[\frac{\partial}{\partial x}, \int dx]:\qquad\qquad \frac{\partial}{\partial x}\int f(x, t)dt =_? \int \frac{\partial}{\partial x} f(x, t)dt\\
\text{}
\end{align*}

\textbf{Answer}: Sort of (but possibly not).

\textbf{Counterexample}:
\begin{align*}
f(x) = 
\begin{cases}
1 & x > 0 \\ 
-1 & x \leq 0
\end{cases}
\implies \int f \approx \abs{x},
\end{align*} which is not differentiable. (This is remedied by the
so-called ``weak derivative'')

\textbf{Sufficient Condition}: If \(f\) is continuous, then both are
always equal to \(f(x)\) by the FTC.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Is the derivative of a continuous function always continuous?
\begin{align*}
[\frac{\partial}{\partial x}, \lim_{x_i\to x}]:\qquad\qquad
\lim_{x_i \to x} f'(x_n) =_? f'(\lim_{x_i\to x} x)
\end{align*} \textbf{Answer}: No.

\textbf{Counterexample}:
\begin{align*}
f ( x ) = \left\{ \begin{array} { l l } { x ^ { 2 } \sin ( 1 / x ) } & { \text { if } x \neq 0 } \\ { 0 } & { \text { if } x = 0 } \end{array} \right.
\implies f ^ { \prime } ( x ) = \left\{ \begin{array} { l l } { 2 x \sin \left( \frac { 1 } { x } \right) - \cos \left( \frac { 1 } { x } \right) } & { \text { if } x \neq 0 } \\ { 0 } & { \text { if } x = 0 } \end{array} \right.
\end{align*} which is discontinuous at zero.

\textbf{Sufficient Condition}: There doesn't seem to be a general one
(which is perhaps why we study \(C^k\) functions).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Is the limit of a sequence of differentiable functions differentiable
\textbf{and} the derivative of the limit?

\begin{align*}
[\frac{\partial}{\partial x}, \lim_{f_n \to f}]:\qquad\qquad \lim_{f_n \to f}\frac{\partial}{\partial x}f_n(x) =_? \frac{\partial }{\partial x}\lim_{f_n \to f} f_n(x)
\end{align*} \textbf{Answer}: \emph{Super} no -- even the uniform limit
of differentiable functions need not be differentiable!

\textbf{Counterexample}:
\(f_n(x) = \frac{\sin(nx)}{\sqrt{n}} \rightrightarrows f = 0\) but
\(f_n' \not\to f' = 0\)

\textbf{Sufficient Condition}: \(f_n \rightrightarrows f\) and
\(f_n \in C^1\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Is the limit of a sequence of integrable functions integrable
\textbf{and} the integral of the limit?

\begin{align*}
[\int dx, \lim_{f_n \to f}](f):\qquad\qquad
\lim_{f_n \to f}\int f_n(x) dx =_? \int \lim_{f_n \to f} f_n(x) dx
\end{align*}

\textbf{Answer}: No.

\textbf{Counterexample}: Order \(\QQ\intersect[0,1]\) as
\(\theset{q_i}_{i\in\NN}\), then take
\begin{align*}
f_n(x) = \sum_{i=1}^n \indic{q_n} \to \indic{{\QQ\intersect[0,1]}}
\end{align*} where each \(f_n\) integrates to zero (only finitely many
discontinuities) but \(f\) is not Riemann-integrable.

\textbf{Sufficient Condition}: - \(f_n \rightrightarrows f\), or - \(f\)
integrable and \(\exists M: \forall n, \abs{f_n} < M\) (\(f_n\)
uniformly bounded)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Is the integral of a continuous function also continuous?

\begin{align*}
[\int dx, \lim_{x_i \to x}]:\qquad\qquad
\lim_{x_i \to x} F(x_i) =_? F(\lim_{x_i \to x} x_i)
\end{align*}

\textbf{Answer}: Yes.

\textbf{Proof}: \(|f(x)| < M\) on \(I\), so given \(c\) pick a sequence
\(x\to c\). Then
\begin{align*}
\abs{f(x)} < M \implies \left\vert \int_c^x f(t)dt \right\vert < \int_c^x M dt \implies \abs{F(x) - F(c)} < M(b-a) \to 0
\end{align*}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Is the limit of a sequence of continuous functions also continuous?

\begin{align*}
[\lim_{x_i \to x}, \lim_{f_n \to f}]: \qquad\qquad
\lim_{f_n \to f}\lim_{x_i \to x} f(x_i) =_? \lim_{x_i \to x}\lim_{f_n \to f} f_n(x_i)\\
\text{}\\
\end{align*}

\textbf{Answer}: No.

\textbf{Counterexample}: \(f_n(x) = x^n \to \delta(1)\)

\textbf{Sufficient Condition}: \(f_n \rightrightarrows f\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Does a sum of differentiable functions necessarily converge to a
differentiable function?

\begin{align*}
\left[\frac{\partial}{\partial x}, \sum_{f_n}\right]: \qquad\qquad
\frac{\partial}{\partial x} \sum_{k=1}^\infty f_k =_? \sum_{k=1}^\infty \frac{\partial}{\partial x} f_k \\
\text{} \\
\text{}\\
\end{align*}

\textbf{Answer}: No.

\textbf{Counterexample}:
\(f_n(x) = \frac{\sin(nx)}{\sqrt{n}} \rightrightarrows 0 \definedas f\),
but \(f_n' = \sqrt{n}\cos(nx) \not\to 0 = f'\) (at, say, \(x=0\))

\textbf{Sufficient Condition}: When
\(f_n \in C^1, \exists x_0: f_n(x_0) \to f(x_0)\) and
\(\sum \norm{f_n'}_\infty < \infty\) (continuously differentiable,
converges at a point, and the derivatives absolutely converge)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuity}{%
\subsection{Continuity}\label{continuity}}

\begin{definition}[Limit definition of continuity]

\begin{align*}
f\text{ continuous } \iff \lim_{x \to p} f(x) = f(p)
\end{align*}

\end{definition}

\begin{definition}[$\eps\dash\delta$ definition of continuity]

\begin{align*}
f:(X, d_X) \to (Y, d_Y) \text{ continuous } \iff \forall \varepsilon,~ \exists \delta \mid ~
d_X(x,y) < \delta \implies d_Y(f(x), f(y)) < \varepsilon
\end{align*}

\end{definition}

\begin{example}[A nonobviously discontinuous function]

\begin{align*}
f(x) = \sin\qty{ \frac{1}{x} } \implies 0\in D(f)
\end{align*}

\end{example}

\begin{proof}[?]

\todo[inline]{todo}

\end{proof}

\begin{example}[The Dirichlet function]

The Dirichlet function is nowhere continuous:
\begin{align*}
f(x) = \indic{\QQ}
\end{align*}

\end{example}

\begin{proposition}[Thomae's function: the set of points of continuity and of discontinuity can both be infinite]

The following function continuous at infinitely many points and
discontinuous at infinitely many points:
\begin{align*}
f(x) = 
\begin{cases}
0 & x\in\RR\sm\QQ \\
\frac{1}{q} & x = \frac{p}{q} \in \QQ
\end{cases}
\end{align*} Then \(f\) is discontinuous on \(\QQ\) and continuous on
\(\RR\sm\QQ\).

\end{proposition}

\begin{proof}[?]

\envlist

\textbf{\(f\) is continuous on \(\QQ\)}:

\begin{itemize}
\tightlist
\item
  Fix \(\varepsilon\), let \(x_0 \in \RR-\QQ\), choose
  \(n: \frac{1}{n} < \varepsilon\) using Archimedean property.

  \begin{itemize}
  \tightlist
  \item
    Define \(S = \theset{x\in\QQ: x\in (0,1), x=\frac{m}{n'}, n' < n}\)
  \item
    Then \(\abs{S} \leq 1+2+\cdots (n-1)\), so choose
    \(\delta = \min_{s\in S}\abs{s-x_0}\)
  \item
    Then
    \begin{align*}
    x \in N_\delta(x_0) \implies f(x) < \frac{1}{n} < \varepsilon
    .\end{align*}
  \end{itemize}
\end{itemize}

\textbf{\(f\) is discontinuous on \(\RR\sm\QQ\)}:

\begin{itemize}
\tightlist
\item
  Let \(x_0 = \frac{p}{q} \in \QQ\) and
  \(\theset{x_n} = \theset{x-\frac{1}{n\sqrt 2}}\). Then
  \begin{align*}
  x_n \uparrow x_0\text{ but } f(x_n) = 0 \to 0 \neq \frac{1}{q} = f(x_0)
  \end{align*}
\end{itemize}

\end{proof}

\begin{remark}

There are no functions that are continuous on \(\QQ\) but discontinuous
on \(\RR-\QQ\)

\end{remark}

\begin{definition}[Uniform Continuity]

\todo[inline]{todo}

\end{definition}

\begin{definition}[Absolute Continuity]

\end{definition}

\begin{theorem}[Extreme Value Theorem]

A continuous function on a compact space attains its extrema.

\end{theorem}

\hypertarget{lipschitz-continuity}{%
\subsubsection{Lipschitz Continuity}\label{lipschitz-continuity}}

\hypertarget{differentiability}{%
\subsection{Differentiability}\label{differentiability}}

\begin{align*}
f'(p) \definedas \frac{\partial f}{\partial x}(p) = \lim_{x\to p} \frac{f(x) - f(p)}{x-p}
\end{align*}

\begin{itemize}
\tightlist
\item
  For multivariable functions: existence \textbf{and continuity} of
  \(\frac{\partial \mathbf{f}}{\partial x_i} \forall i \implies \mathbf{f}\)
  differentiable

  \begin{itemize}
  \tightlist
  \item
    Necessity of continuity: example of a continuous functions with all
    partial and directional derivatives that is not differentiable:
    \begin{align*}  
    f(x, y) = 
    \begin{cases}
    \frac{y^3}{x^2+y^2} & (x,y) \neq (0,0) \\ 
    0 & \text{else}
    \end{cases}
    .\end{align*}
  \end{itemize}
\end{itemize}

\hypertarget{properties-strongest-to-weakest}{%
\subsubsection{Properties, strongest to
weakest}\label{properties-strongest-to-weakest}}

\begin{align*}  
C^\infty \subsetneq C^k \subsetneq \text{ differentiable } \subsetneq C^0 \subset \mathcal{R}_K
.\end{align*}

\begin{itemize}
\tightlist
\item
  Example showing \(f\in C^0 \notimplies f\) is differentiable
  \textbf{and} \(f\) not differentiable \(\notimplies f \not\in C^0\).

  \begin{itemize}
  \tightlist
  \item
    Take \(f(x) = \abs{x}\) at \(x=0\).
  \end{itemize}
\item
  Example showing that \(f\) differentiable \(\notimplies f \in C^1\):

  \begin{itemize}
  \tightlist
  \item
    Take
    \begin{align*}
    f(x) = 
    \begin{cases}
    x^2\sin\qty{ \frac{1}{x} } & x \neq 0 \\ 
    0 & x =0
    \end{cases}
    \implies f'(x) = 
    \begin{cases}
    -\cos\qty{\frac{1}{x}} + 2x\sin\qty{ \frac{1}{x} } & x \neq 0 \\ 
    0 & x=0
    \end{cases}
    \end{align*} but \(\lim_{x\to 0}f'(x)\) does not exist and thus
    \(f'\) is not continuous at zero.
  \end{itemize}
\end{itemize}

Proof that \(f\) differentiable \(\implies f \in C^0\):
\begin{align*}
f(x) - f(p) = \frac{f(x)-f(p)}{x-p}(x-p) \stackrel{\tiny\mbox{hypothesis}}{=} f'(p)(x-p) \stackrel{\tiny\mbox{$x\to p$}}\rightrightarrows 0
\end{align*}

\hypertarget{giant-table-of-relations}{%
\subsection{Giant Table of Relations}\label{giant-table-of-relations}}

Bold are assumed hypothesis, regular text is the strongest conclusion
you can reach, strikeout denotes implications that aren't necessarily
true.

\begin{align*}
f' && f && \therefore f && F \\
\hline \\
\cancel{\text{exists}} && \mathbf{continuous} &&  \text{K-integrable} && \text{exists} \\
\cancel{\text{continuous}} && \mathbf{differentiable}  && \text{continuous} && \text{exists} \\
\cancel{\text{exists}} && \mathbf{integrable} && \cancel{\text{continuous}} && \text{differentiable} \\
\end{align*}

Explanation of items in table:

\begin{itemize}
\tightlist
\item
  K-integrable: compactly integrable.
\item
  \(f\) integrable \(\implies F\) differentiable \(\implies F \in C_0\)

  \begin{itemize}
  \tightlist
  \item
    By definition and FTC, and differentiability \(\implies\) continuity
  \end{itemize}
\item
  \(f\) differentiable and \(K\) compact \(\implies f\) integrable on
  \(K\).

  \begin{itemize}
  \tightlist
  \item
    In general, \(f\) differentiable \(\notimplies f\) integrable.
    Necessity of compactness:
    \begin{align*}  
    f(x) = e^x \in C^\infty(\RR)\text{ but }\int_\RR e^x dx \to \infty
    .\end{align*}
  \end{itemize}
\item
  \(f\) integrable \(\notimplies f\) differentiable

  \begin{itemize}
  \tightlist
  \item
    An integrable function that is not differentiable: \(f(x) = |x|\) on
    \(\RR\)
  \end{itemize}
\item
  \(f\) differentiable \(\implies f\) continuous a.e.
\end{itemize}

\hypertarget{integrability}{%
\subsection{Integrability}\label{integrability}}

\begin{itemize}
\tightlist
\item
  Sufficient criteria for Riemann integrability:

  \begin{itemize}
  \tightlist
  \item
    \(f\) continuous
  \item
    \(f\) bounded and continuous almost everywhere, or
  \item
    \(f\) uniformly continuous
  \end{itemize}
\item
  \(f\) integrable \(\iff\) bounded and continuous a.e.
\end{itemize}

\begin{theorem}[FTC for the Riemann Integral]

If \(F\) is a differentiable function on the interval \([a,b]\), and
\(F'\) is bounded and continuous a.e., then \(F' \in L_R([a, b])\) and
\begin{align*}
\forall x\in [a,b]: \int_a^x F'(t)~dt=F(x)-F(a)
\end{align*}

Suppose \(f\) bounded and continuous a.e. on \([a,b]\), and define
\begin{align*}
F(x) \definedas \int_a^x f(t)~dt
\end{align*} Then \(F\) is absolutely continuous on \([a,b]\), and for
\(p \in [a,b]\),
\begin{align*}
f \in C^0(p) \implies F \text{ differentiable at } p,~ F'(p) = f(p), \text{ and } F' \stackrel{\tiny\mbox{a.e}}{=} f.
\end{align*}

\end{theorem}

\begin{proposition}

The Dirichlet function is Lebesgue integrable but not Riemann
integrable:
\begin{align*}
f(x) = \indic{x \in \QQ}
\end{align*}

\end{proposition}

\begin{proof}[?]

\todo[inline]{todo}

\end{proof}

\hypertarget{list-of-free-conclusions}{%
\subsection{List of Free Conclusions:}\label{list-of-free-conclusions}}

\begin{itemize}
\tightlist
\item
  \(f\) integrable on \(U \implies\):

  \begin{itemize}
  \tightlist
  \item
    \(f\) is bounded
  \item
    \(f\) is continuous a.e. (finitely many discontinuities)
  \item
    \(\int f\) is continuous
  \item
    \(\int f\) is differentiable
  \end{itemize}
\item
  \(f\) continuous on \(U\):

  \begin{itemize}
  \tightlist
  \item
    \(f\) is integrable on compact subsets of \(U\)
  \item
    \(f\) is bounded
  \item
    \(f\) is integrable
  \end{itemize}
\item
  \(f\) differentiable at a point \(p\):

  \begin{itemize}
  \tightlist
  \item
    \(f\) is continuous
  \end{itemize}
\item
  \(f\) is differentiable in \(U\)

  \begin{itemize}
  \tightlist
  \item
    \(f\) is continuous a.e.
  \end{itemize}
\item
  Defining the Riemann integral: \#todo
\end{itemize}

\hypertarget{convergence-2}{%
\subsection{Convergence}\label{convergence-2}}

\hypertarget{sequences-and-series-of-functions}{%
\subsubsection{Sequences and Series of
Functions}\label{sequences-and-series-of-functions}}

\begin{definition}[Convergence of an infinite series]

Define
\begin{align*}
s_n(x) \da \sum_{k=1}^n f_k(x)
\end{align*} and
\begin{align*}
\sum_{k=1}^\infty f_k(x) \da \lim_{n\to\infty} s_n(x),
\end{align*} which can converge pointwise, absolutely, uniformly, or not
all.

\end{definition}

\begin{proposition}[?]

If \(\limsup_{k\in \NN} \abs{f_k(x)} \neq 0\) then \(f_k\) is not
convergent.

\end{proposition}

\begin{proposition}[?]

If \(f\) is injective, then \(f'\) is nonzero in some neighborhood of
???

\end{proposition}

\hypertarget{pointwise-convergence}{%
\subsubsection{Pointwise convergence}\label{pointwise-convergence}}

\begin{align*}  
f_n \to f = \lim_{n\to\infty} f_n
.\end{align*} Summary:
\begin{align*}  
\lim_{f_n \to f} \lim_{x_i \to x} f_n(x_i) \neq \lim_{x_i \to x} \lim_{f_n \to f} f_n(x_i)
.\end{align*}

\begin{align*}  
\lim_{f_n \to f} \int_I f_n \neq \int_I \lim_{f_n \to f} f_n
.\end{align*}

\begin{proposition}[?]

Pointwise convergence is strictly weaker than uniform convergence.

\end{proposition}

\begin{proof}[?]

\(f_n(x) = x^n\) on \([0, 1]\) converges pointwise but not uniformly.

\begin{itemize}
\tightlist
\item
  Towards a contradiction let \(\varepsilon = \frac{1}{2}\).
\item
  Let \(n = N\qty{\frac{1}{2} }\) and
  \(x = \left(\frac{3}{4}\right)^\frac{1}{n}\).
\item
  Then \(f(x) = 0\) but
  \begin{align*}
  \abs{f_n(x) - f(x)} = x^n = \frac{3}{4} > \frac{1}{2}
  \end{align*}
\end{itemize}

\end{proof}

\begin{proposition}[A pointwise limit of continuous functions is not necessarily continuous.]

\begin{align*}  
f_n \text{ continuous} \notimplies f\da \lim_n f_n \text{ is continuous}
.\end{align*}

\end{proposition}

\begin{proof}[?]

Take
\begin{align*}  
f_n(x) = x^n,\quad f_n(x) \to \indic[x = 1]
.\end{align*}

\end{proof}

\begin{proposition}[The limit of derivatives need not equal the derivative of the limit]

\begin{align*}  
f_n \text{ differentiable} &\notimplies  f'_n \text{ converges} \\
f'_n \text{ converges} &\not\implies \lim f'_n = f'
.\end{align*}

\end{proposition}

\begin{proof}[?]

Take
\begin{align*}  
f_n(x) = \frac{1}{n}\sin(n^2 x) \to 0,&& \text{but }
f'_n = n\cos(n^2 x) \text{ does not converge}
.\end{align*}

\end{proof}

\begin{proposition}[?]

\begin{align*}  
f_n\in \mathcal{R}_I \notimplies \lim_{f_n \to f} \int_I f_n \neq \int_I \lim_{f_n \to f} f_n
.\end{align*}

\end{proposition}

\begin{proof}[?]

May fail to converge to same value, take
\begin{align*}  
f_n(x) = \frac{2n^2x}{(1+n^2x^2)^2} \to 0 && \text{but }\int_0^1 f_n = 1 - \frac{1}{n^2 + 1} \to 1\neq 0
.\end{align*}

\end{proof}

\hypertarget{uniform-convergence}{%
\subsubsection{Uniform Convergence}\label{uniform-convergence}}

Notation:
\begin{align*}  
f_n \rightrightarrows f= \lim_{n\to\infty} f_n \text{ and } \sum_{n=1}^\infty f_n \rightrightarrows S
.\end{align*}

Summary:
\begin{align*}  
\lim_{x_i \to x} \lim_{f_n \to f} f_n(x_i) = \lim_{f_n \to f} \lim_{x_i \to x} f_n(x_i) = \lim_{f_n \to f} f_n(\lim_{x_i \to x} x_i)
.\end{align*}

\begin{align*}  
\lim_{f_n \to f} \int_I f_n = \int_I \lim_{f_n \to f} f_n
.\end{align*}

\begin{align*}  
\sum_{n=1}^\infty \int_I f_n = \int_I \sum_{n=1}^\infty f_n
.\end{align*}

``The uniform limit of a(n) \(x\) function is \(x\)'', for \(x \in\)
\{continuous, bounded\}

\begin{itemize}
\item
  Equivalent to convergence in the uniform metric on the metric space of
  bounded functions on \(X\):
  \begin{align*}  
  f_n \rightrightarrows f \iff \sup_{x\in X} \abs{f_n(x) - f(x)} \to 0
  .\end{align*}

  \begin{itemize}
  \tightlist
  \item
    \((B(X,Y), \norm{}_\infty)\) is a metric space and
    \(f_n \rightrightarrows f \iff \norm{f_n - f}_\infty \to 0\) (where
    \(B(X,Y)\) are bounded functions from \(X\) to \(Y\) and
    \(\norm{f}_\infty = \sup_{x\in I}\theset{f(x)}\)
  \end{itemize}
\item
  \(f_n \rightrightarrows f \implies f_n \to f\) pointwise
\item
  \(f_n\) continuous \(\implies f\) continuous

  \begin{itemize}
  \tightlist
  \item
    i.e.~``the uniform limit of continuous functions is continuous''
  \end{itemize}
\item
  \(f_n \in C^1\), \(\exists x_0: f_n(x_0) \to f(x_0)\), and
  \(f'_n \rightrightarrows g\) \(\implies f\) differentiable and
  \(f' = g~\) (i.e.~\(f'_n \to f'\))

  \begin{itemize}
  \tightlist
  \item
    Necessity of \(C^1\) -- look at failures of \(f'_n\) to be
    continuous:

    \begin{itemize}
    \tightlist
    \item
      Take
      \(f_n(x) = \sqrt{\frac{1}{n^2} + x^2} \rightrightarrows |x|\), not
      differentiable
    \item
      Take \(f_n(x) = n^{-\frac{1}{2}}\sin(nx) \rightrightarrows 0\) but
      \(f'_n \not\to f' = 0\) and \(f' \neq g\)
    \end{itemize}
  \end{itemize}
\item
  \(f_n\) integrable \(\implies f\) integrable and
  \(\int f_n \to \int f\)
\item
  \(f_n\) bounded \(\implies f\) bounded
\item
  \(f_n \rightrightarrows f_n \notimplies f'_n\) converges

  \begin{itemize}
  \tightlist
  \item
    Says nothing about it general
  \end{itemize}
\item
  \(f_n' \rightrightarrows f' \notimplies f_n \rightrightarrows f\)

  \begin{itemize}
  \tightlist
  \item
    Unless \(f\) converges at one or more points.
  \end{itemize}
\end{itemize}

\begin{proposition}[All subsequences of a convergent sequence share a limit]

\(\theset{x_i} \to p \implies\) every subsequence also converges to
\(p\).

\end{proposition}

\begin{definition}[Cauchy Sequence]

\todo[inline]{todo}

\end{definition}

\begin{proposition}[?]

Every convergent sequence in \(X\) is a Cauchy sequence.

\end{proposition}

\begin{remark}

The converse need not hold in general, but if \(X\) is complete, every
Cauchy sequence converges. An example of a Cauchy sequence that doesn't
converge: take \(X=\QQ\) and set \(x_i = \pi\) truncated to \(i\)
decimal places.

\end{remark}

\begin{remark}

If any subsequence of a Cauchy sequence converges, the entire sequence
converges.

\end{remark}

\begin{definition}[Metric]

\begin{align*}  
d(x,y) &\geq 0 && \text{Positive}\\
d(x,y) &= 0 \iff x = y && \text{Nondegenerate}\\
d(x,y) &= d(y,x) && \text{Symmetric}\\
d(x,y) &\leq d(x,p) + d(p,y) \quad \forall p && \text{Triangle Inequality}
.\end{align*}

\end{definition}

\begin{definition}[Complete]

? \todo[inline]{todo}

\end{definition}

\begin{definition}[Bounded]

? \todo[inline]{todo}

\end{definition}

\hypertarget{topology}{%
\subsection{Topology}\label{topology}}

\begin{definition}[Axioms for a Topology]

\textbf{Open Set Characterization}: Arbitrary unions and finite
intersections of open sets are open.

\textbf{Closed Set Characterization}: Arbitrary intersections and finite
unions of closed sets are closed.

\end{definition}

\begin{remark}

The best source of examples and counterexamples is the open/closed unit
interval in \(\mathbb{R}\). Always test against these first!

\end{remark}

\begin{remark}

If \(f\) is a continuous function. the preimage of every open set is
open and the preimage of every closed set is closed.

\end{remark}

\begin{proposition}[?]

In \(\RR\), singleton sets and finite discrete sets are closed.

\end{proposition}

\begin{proof}[?]

A singleton set can be written
\begin{align*}  
\ts{p_0} = (-\infty, p) \union (p, \infty)
.\end{align*} A finite discrete set \(\ts{p_0}\), which wlog (by
relabeling) can be assumed to satisfy \(p_0 < p_1 < \cdots\), can be
written
\begin{align*}  
\ts{p_0, p_1, \cdots, p_n} = (-\infty, p_0) \union (p_0, p_1) \union \cdots \union (p_n, \infty)
.\end{align*}

\end{proof}

\begin{proposition}[?]

This yields a good way to produce counterexamples to continuity.

\end{proposition}

In \(\mathbb{R}\), singletons are closed. This means any finite subset
is closed, as a finite union of singleton sets!

\begin{proposition}[?]

If \(X\) is a compact metric space, then \(X\) is complete and bounded.

\end{proposition}

\begin{proposition}[?]

If \(X\) complete and \(X \subset Y\), then \(X\) closed in \(Y\).

\end{proposition}

\begin{remark}

The converse generally does not hold, and completeness is a necessary
condition. Counterexample: \(\QQ\subset \QQ\) is closed but
\(\QQ\subset\RR\) is not.

\end{remark}

\begin{proposition}[?]

If \(X\) is compact, then \(Y \subset X \implies Y\) is compact \(\iff\)
\(Y\) closed.

\end{proposition}

\begin{definition}[Sequential Compactness]

A topological space \(X\) is \textbf{sequentially compact} iff every
sequence \(\ts{x_n}\) has a subsequence converging to a point in \(X\).

\end{definition}

\begin{proposition}[Compactness and sequential compactness]

If \(X\) is a metric space, \(X\) is compact iff \(X\) is sequentially
compact.

\end{proposition}

\begin{remark}

Note that in general, neither form of compactness implies the other.

\end{remark}

\hypertarget{counterexamples}{%
\subsection{Counterexamples}\label{counterexamples}}

\begin{proposition}[?]

There are functions differentiable only at a single point. Example:
\begin{align*}  
f(x) = 
\begin{cases}
x^2 & x\in QQ\\
-x^2 & x\in \RR\sm\QQ
\end{cases}
.\end{align*}

This is discontinuous everywhere except for \(x=0\), and you can compute
\begin{align*}  
\lim_{h\to 0} {f(x+h) - f(x) \over h}\evalfrom_{x=0} = 
\lim_{h\to 0}
\begin{cases}
h & x\in \QQ \\
-h & x\in \RR\sm\QQ 
\end{cases}
=0
.\end{align*}

\end{proposition}

\begin{proposition}[?]

The product of two non-differentiable functions can be differentiable:
take \(f(x) = g(x) = \abs{x}\) which are not differentiable at \(x=0\),
then \(fg(x) = \abs{x}^2\) is differentiable at \(x=0\).

\end{proposition}

\begin{proposition}[?]

A continuous function that is zero on a dense set \(A\subset X\) is
identically zero.

\begin{proof}[?]

Since \(A\) is dense, for any \(x\in X\sm A\) take a sequence
\(\ts{x_n}\) in \(A\) converging to \(x\). Then \(0 = f(x_n) \to f(x)\)
implies \(f(x) = 0\).

\end{proof}

\end{proposition}

\hypertarget{point-set-topology}{%
\section{Point-Set Topology}\label{point-set-topology}}

\hypertarget{definitions-1}{%
\subsection{Definitions}\label{definitions-1}}

\todo[inline]{Bring in Rudin's list}

\begin{itemize}
\item
  Epsilon-neighborhood

  \begin{itemize}
  \tightlist
  \item
    \(N_r(p) = \theset{q \mid d_X(p,q) < r}\)
  \end{itemize}
\item
  Limit Point

  \begin{itemize}
  \tightlist
  \item
    \(p\) is a limit point of \(E\) iff
    \(\forall N_r(p),~ \exists q\neq p \mid q \in N_r(p)\)
  \item
    Equivalently, \(\forall N_r(p),~ N_r(p) \cap E \neq \emptyset\)
  \item
    Let \(L(E)\) be the set of limit points of \(E\).
  \item
    Example: \(E = (0,1) \implies 0 \in L(E)\)
  \end{itemize}
\item
  Isolated Point

  \begin{itemize}
  \tightlist
  \item
    \(p\) is an isolated point of \(E\) iff \(p\) is not a limit point
    of \(E\)
  \item
    Equivalently, \(\exists N_r(p) \mid N_r(p) \cap E = \emptyset\)
  \item
    Equivalently, \(E - L(E)\)
  \end{itemize}
\item
  Perfect

  \begin{itemize}
  \tightlist
  \item
    \(E\) is perfect iff \(E\) is closed and \(E \subseteq L(E)\)
  \item
    Equivalently, \(L(E) = E\)
  \end{itemize}
\item
  Interior

  \begin{itemize}
  \tightlist
  \item
    \(p\) is an interior point of \(E\) iff
    \(\exists N_r(p) \mid N_r(p) \subsetneq E\)
  \item
    Denote the interior of \(E\) by \(E^\circ\)
  \end{itemize}
\item
  Exterior
\item
  Closed sets

  \begin{itemize}
  \tightlist
  \item
    \(E\) is closed iff \(p\) a limit point of \(E \implies p \in E\)
  \item
    Equivalently if \(L(E) \subseteq E\)
  \item
    Closed under finite unions, arbitrary intersections
  \end{itemize}
\item
  Open sets

  \begin{itemize}
  \tightlist
  \item
    \(E\) is open iff \(p\in E \implies p \in E^\circ\)
  \item
    Equivalently, if \(E \subseteq E^\circ\)
  \item
    Closed under arbitrary unions, finite intersections
  \end{itemize}
\item
  Boundary
\item
  Closure
\item
  Dense

  \begin{itemize}
  \tightlist
  \item
    \(E\) is dense in \(X\) iff \(X \subseteq E \cup L(E)\)
  \end{itemize}
\item
  Connected

  \begin{itemize}
  \tightlist
  \item
    Space of connected sets closed under union, product, closures
  \item
    Convex \(\implies\) connected
  \end{itemize}
\item
  Disconnected
\item
  Path Connected

  \begin{itemize}
  \tightlist
  \item
    \(\forall x,y \in X \exists f: I \to X \mid f(0) = x, f(1) = y\)
  \item
    Path connected \(\implies\) connected
  \end{itemize}
\item
  Simply Connected
\item
  Totally Disconnected
\item
  Hausdorff
\item
  Compact

  \begin{itemize}
  \tightlist
  \item
    Every covering has a finite subcovering.
  \item
    \(X\) compact and
    \(U \subset X: (U \text{ closed } \implies U \text{ compact })\)

    \begin{itemize}
    \tightlist
    \item
      \(U \text{ compact } \implies U \text{ closed }\) iff \(X\) is
      Hausdorff
    \end{itemize}
  \item
    Closed under products
  \end{itemize}
\end{itemize}

\begin{example}[?]

The space \(\theset{\frac{1}{n}}_{n\in \NN}\).

\end{example}

List of properties preserved by continuous maps:

\begin{itemize}
\tightlist
\item
  Connectedness
\item
  Compactness
\end{itemize}

Checking if a map is homeomorphism:

\begin{itemize}
\tightlist
\item
  \(f\) continuous, \(X\) compact and Hausdorff \(\implies f\) is a
  homeomorphism.
\end{itemize}

\hypertarget{probability}{%
\section{Probability}\label{probability}}

\hypertarget{definitions-2}{%
\subsection{Definitions}\label{definitions-2}}

\begin{align*}
L^2(X) &= \theset{f: X \to \RR: \int_\RR f(x) ~dx < \infty} 
&&\text{square integrable functions}\\
\inner{g}{f}_{2} &= \int_\RR g(x)f(x) ~dx 
&&\text{the } L^2 \text{ inner product}\\
\norm{f}_2^2 &= \inner{f}{f} = \int_\RR f(x)^2 ~dx
&&\text{norm}\\
E[\wait] &= \inner{\wait}{f} 
&&\text{expectation}\\
(\tau_{p}f)(x) &= f(p- x) 
&&\text{translation}\\
(f \ast g)(p) &= \int_\RR f(t)g(p-t)~dt = \int_\RR f(t)(T_{p}g)(t) ~dt = \inner{T_pg}{f} 
&&\text{convolution}\\
\end{align*}

\begin{definition}[Random Variable]

For \((\Sigma, E, \mu)\) a probability space with sample space
\(\Sigma\) and probability measure \(\mu\), a random variable is a
function \(X: \Sigma \to \RR\)

\end{definition}

\begin{definition}[Probability Density Function (PDF)]

For any \(U \subset \RR\), given by the relation
\begin{align*}
P(X \in U) = \int_U f(x) ~dx \\
\implies P(a \leq X \leq b) = \int_a^b f(x) ~dx
\end{align*}

\end{definition}

\begin{definition}[Cumulative Distribution Function (CDF)]

The antiderivative of the PDF
\begin{align*}  
F(x) = P(X \leq x) = \int_{-\infty}^x f(x) ~dx
.\end{align*} Yields \(\dd{F}{x} = f(x)\)

\end{definition}

\begin{definition}[Mean/Expected Value]

\begin{align*}  
E[X] \definedas \inner{\id}{f} = \int_\RR x f(x) ~dx
.\end{align*} Also denoted \(\mu_X\).

\end{definition}

\begin{proposition}[Linearity of Expectation]

\begin{align*}  
E\left[\sum_{i\in\NN} a_i X_i\right] = \sum_{i\in\NN} a_i E[X_i]
.\end{align*} Does not matter whether or not the \(X_i\) are
independent.

\end{proposition}

\begin{definition}[Variance]

\begin{align*}
\mathrm{Var}(X) &= E[(X - E[X])^2] \\
&= \int (x - E[X])^2 f(x) ~dx \\
&= E[X^2] - E[X]^2 \\
&\definedas \sigma^2(X)
\end{align*} where \(\sigma\) is the standard deviation. Can also
defined as \(\inner{(\id - \inner{\id}{f})^2}{f}\) Take the portion of
the id function in the orthogonal complement of \(f\), squared, and
project it back onto \(f\)? \todo[inline]{???}

\end{definition}

\begin{proposition}[Properties of Variance]

\begin{align*}  
\mathrm{Var}(aX + b) &= a^2\mathrm{Var}(X) \\
\mathrm{Var}\qty{ \sum_\NN X_i } &= 
\sum_i \mathrm{Var}(X_i) + 2 \sum_{i < j}\mathrm{Cov}(X_i, X_j)
.\end{align*}

\end{proposition}

\begin{definition}[Covariance]

\begin{align*}
\mathrm{Cov}(X,Y) &= E[(X-\mu_X)(Y-\mu_Y)] \\
 &= E[XY] - E[X]E[Y]
\end{align*}

\end{definition}

\begin{proposition}[Properties of Covariance]

\begin{align*}  
\mathrm{Cov}(X, X) &= \mathrm{Var}(X) \\
\mathrm{Cov}(aX, Y) &= a\mathrm{Cov}(X,Y) \\
\mathrm{Cov}(\sum_{\NN} X_i, \sum_\NN Y_j) 
&= \sum_i \sum_j\mathrm{Cov}(X_i, Y_j) \\
.\end{align*}

\end{proposition}

\begin{proposition}[Stirling's Approximation]

\begin{align*}
k! \sim k^\frac{k+1}{2}e^{-k} \sqrt{2\pi}
.\end{align*}

\end{proposition}

\begin{proposition}[Markov Inequality]

\begin{align*}
P(X \geq a) \leq \frac 1 a E[X]
\end{align*}

One-sided Markov:
\begin{align*}
P(X \in N_\varepsilon(\mu)) = 2\frac{\sigma^2}{\sigma^2 + a^2}
.\end{align*}

\end{proposition}

\begin{proposition}[Chebyshev's Inequality]

\begin{align*}
P(\abs{X - \mu} \geq a) \leq \left( \frac \sigma k \right)^2
\end{align*}

\end{proposition}

\begin{proof}[?]

Apply Markov to the variable \((X-\mu)^2\) and \(a=k^2\)

\end{proof}

\begin{theorem}[Central Limit Theorem]

For \(X_i\) i.i.d.,
\begin{align*}  
\lim_n \frac{\sum_{i=1}^n X_i - n\mu}{\sigma \sqrt n} \sim N(0, 1)
.\end{align*}

\end{theorem}

\begin{theorem}[Strong Large of Large Numbers]

\begin{align*}  
P(\frac{1}{n} \sum X_i \rightarrow \mu) = 1
.\end{align*}

\end{theorem}

\begin{proposition}[Chernoff Bounds]

For all \(t > 0\),
\begin{align*}  
P(X \in N_\varepsilon(a)^c) \leq 2 e^{-at}M_X(t) \\
.\end{align*}

\end{proposition}

\begin{proposition}[Jensen's Inequality]

\begin{align*}
E[f(X)] \geq f(E[X])
\end{align*}

\end{proposition}

\begin{definition}[Entropy]

\begin{align*}
H(X) = - \sum p_i \ln p_i
\end{align*}

\end{definition}

\hypertarget{theory-and-background}{%
\subsection{Theory and Background}\label{theory-and-background}}

\begin{definition}[Axioms for a Probability Space]

Given a sample space \(\Sigma\) with events \(S\), 1.
\(\mu(\Sigma) = 1\) 1. Yields
\(S \in \Sigma \implies 0 \leq P(S) \leq 1\) 2. For mutually exclusive
events, \(P(\union_\NN S_i) = \sum_\NN P(S_i)\) 1. Yields
\(P(\emptyset) = 0\)

\end{definition}

\begin{proposition}[Properties that follow from axioms]

\envlist

\begin{itemize}
\tightlist
\item
  \(P(S^c) = 1 - P(S)\)
\item
  \(E \subseteq F \implies P(E) \leq P(F)\)
\item
  Proof: \(E \subseteq F \implies F = E \union (E^c \intersect F)\),
  which are disjoint, so
  \(P(E) \leq P(E) + P(E^c \intersect F) = P(F)\).
\item
  \(P(E \union F) = P(E) + P(F) - P(E \intersect F)\)
\end{itemize}

\end{proposition}

\begin{definition}[Conditional Probability]

\begin{align*}
P(F)P(E \mid F) = P(E \intersect F) = P(E)P(F \mid E)
\end{align*} Generalization:
\begin{align*}
P(\intersect_\NN E_i) = P(E_1) P(E_2 \mid E_1)P(E_3\mid E_1 \intersect E_2) \cdots
\end{align*}

\end{definition}

\begin{theorem}[Bayes' Rule]

\begin{align*}
P(E) = P(F)P(E \mid F) + P(F^c)P(E \mid F^c) \\
P(E) = \sum_i P(A_i) P(E \mid A_i)
\end{align*} Generalization: for \(\coprod_{i=1}^n A_i = \Sigma\) and
\(A=A_i\) for some \(i\),
\begin{align*}  
P(A \mid B) = \frac{P(A)P(B\mid A)}{\sum_{j = 1}^n P(B \mid A_j)}
.\end{align*} The LHS: the posterior probability, while \(P(A_i)\) are
the priors.

\end{theorem}

\begin{definition}[Odds]

\begin{align*}
P(A) / P(A^c)
\end{align*} Conditional odds:
\begin{align*}
\frac{P(A \mid E)}{P(A^c \mid E)} = \frac{P(A)}{P(A^c)} \frac{P(E \mid A)}{P(E \mid A^c)}
\end{align*}

\end{definition}

\begin{definition}[Independence]

\begin{align*}
P(A \intersect B) = P(A) P(B)
\end{align*}

\end{definition}

\begin{proposition}[Change of Variables for PDFs]

If \(g\) is differentiable and monotonic and \(Y=g(X)\), then
\begin{align*}
f_Y(y) = 
\begin{cases}
(f_X \circ g^{-1})(y) \abs{\dd{}{y} g^{-1}(y)} & y \in \im(g) \\
0 & \text{else} 
\end{cases}
\end{align*}

\end{proposition}

\begin{proposition}[PDF for a sum of independent random variables]

\begin{align*}
f_{X+Y} = (F_X \ast f_y)
\end{align*}

\end{proposition}

\hypertarget{distributions}{%
\subsection{Distributions}\label{distributions}}

Let \(X\) be a random variable, and \(f\) be its probability density
function satisfying \(f(k) = P(X = k)\)

\hypertarget{uniform}{%
\subsubsection{Uniform}\label{uniform}}

\begin{itemize}
\tightlist
\item
  Consider an event with \(n\) mutually exclusive outcomes of equal
  probability, and let \(X \in \theset{1,2,\ldots, n}\) denote which
  outcome occurs. Then,
  \begin{align*}
  f(k)            &= \quad \frac 1 n \\
  \mu             &= \frac n 2 \\
  \sigma^2    &= a
  \end{align*}
\item
  Examples:

  \begin{itemize}
  \tightlist
  \item
    Dice rolls where \(n=6\).
  \item
    Fair coin toss where \(n=2\).
  \end{itemize}
\item
  Continuous: \(\mu = (1/2)(b+a), \sigma^2 = (1/12)(b-a)^2\)
\end{itemize}

\hypertarget{bernoulli}{%
\subsubsection{Bernoulli}\label{bernoulli}}

\begin{itemize}
\tightlist
\item
  Consider a trial with either a positive or negative outcome, and let
  \(X \in\theset{0,1}\) where \(1\) denotes a success with probability
  \(p\). Then,
\end{itemize}

\begin{align*}
f(k)            &= 
\begin{cases}
1-p,        & k = 0 \\
p,          & k = 1
\end{cases} \\
\mu             &= \quad p \\
\sigma^2    &= \quad p(1-p)
\end{align*} - Examples: - A weighted coin with \(P(\text{Heads}) = p\)

\hypertarget{binomial}{%
\subsubsection{Binomial}\label{binomial}}

\begin{itemize}
\tightlist
\item
  Consider a sequence of independent Bernoulli trials, let
  \(X \in \theset{1,\ldots, n}\) denote the number of successes
  occurring in \(n\) trials. Then,
  \begin{align*}
  f(k)            &= \quad {n \choose k} p^k (1-p)^{n-k} \\
  \mu             &= \quad np \\
  \sigma^2    &= \quad np(1-p)
  \end{align*}
\item
  Examples:

  \begin{itemize}
  \tightlist
  \item
    A sequence of coin flips and the numbers of total heads occurring.
  \end{itemize}
\end{itemize}

\hypertarget{poisson}{%
\subsubsection{Poisson}\label{poisson}}

\begin{itemize}
\tightlist
\item
  Given a parameter \(\lambda > 0\) that denotes the rate per unit time
  of an event occurring and \(X\) the number of times the event occurs
  in one unit of time,
  \begin{align*}
  f(k)            &= \quad \frac{\lambda^k}{k!}e^{-\lambda} \\
  \mu             &= \quad \lambda \\
  \sigma^2    &= \quad \lambda^2
  \end{align*}
\item
  Approximates binomial when \(n >> 1\) and \(p << 1\) by using
  \(\lambda = np\)
\end{itemize}

\hypertarget{negative-binomial}{%
\subsubsection{Negative Binomial}\label{negative-binomial}}

\begin{itemize}
\tightlist
\item
  \(B^- (r, p)\): similar to binomial, where \(X\) is the number of
  trials it takes to accumulate \(r\) successes
  \begin{align*}
  f(k) &= \quad {k-1 \choose r-1}p^r(1-p)^{k-r} \\
  \mu &= \quad \frac r p \\
  \sigma^2 &= \quad \frac{r (1-p)}{p^2}
  \end{align*}
\end{itemize}

\hypertarget{geometric}{%
\subsubsection{Geometric}\label{geometric}}

\begin{itemize}
\tightlist
\item
  Consider a sequence of independent Bernoulli trials, let
  \(X \in \theset{1,\ldots, n}\) where \(X=k\) denotes the first success
  happening on the \(k\dash\)th trial. Then,
  \begin{align*}
  f(k)            &= \quad (1-p)^{k-1} p \\
  \mu             &= \quad \frac 1 p \\
  \sigma^2    &= \quad \frac{1-p}{p^2}
  \end{align*}
\item
  Note this is equal to \(B^-(1, p)\)
\item
  Examples:

  \begin{itemize}
  \tightlist
  \item
    A sequence of coin flips and the number of flips before the first
    heads appears.
  \end{itemize}
\end{itemize}

\hypertarget{hypergeometric}{%
\subsubsection{Hypergeometric}\label{hypergeometric}}

\begin{itemize}
\tightlist
\item
  \(H(n, m, s)\) An urn filled with \(n\) balls, where \(m\) are white
  and \(n-m\) are black; pick a sample of size \(s\) and let \(X\)
  denote the number of white balls:
  \begin{align*}
  f(k)            &= \quad {m \choose k} {n-m \choose s-k} {n \choose s}^{-1} \\
  \mu             &= \quad \frac{ms}{n} \\
  \sigma^2    &= \quad \frac{ms}{n}(1- \frac{m}{n})\left( 1 - \frac{s-1}{n-1} \right)
  \end{align*}
\end{itemize}

\hypertarget{normal}{%
\subsubsection{Normal}\label{normal}}

\begin{align*}
f(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{align*}

\begin{longtable}[]{@{}ll@{}}
\toprule
\(z\) & \(\Phi(z)\)\tabularnewline
\midrule
\endhead
\(0\) & \(0.5\)\tabularnewline
\(1\) & \(0.69\)\tabularnewline
\(1.5\) & \(0.84\)\tabularnewline
\(2\) & \(0.93\)\tabularnewline
\(2.5\) & \(0.97\)\tabularnewline
\(>3\) & \(0.99\)\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{table-of-distributions}{%
\subsection{Table of Distributions}\label{table-of-distributions}}

Table: let \(q = 1-p\).

\begin{align*}
\text{Distribution} & f(x)                                                                            &  & \mu              & \sigma^2            & M(t) \\
\hline \\
B(n, p)             & {n\choose x}p^x q^{n-x}                                                         &  & np               & npq                 & (pe^t + q)^n
\\
P(\lambda)          & \frac{\lambda^x}{x!}e^{-\lambda}                                                &  & \lambda          & \lambda             & e^{\lambda(e^t-1)}
\\
G(p)                & q^{x-1}p                                                                        &  & \frac{1}{p}      & \frac{q}{p^2}       & \frac{pe^t}{1-qe^t}
\\
B^-(r, p)           & {n-1 \choose r-1}p^rq^{n-r}                                                     &  & \frac{r}{p}      & \frac{rq}{p^2}      & \left(\frac{pe^t}{1-qe^t}\right)^r
\\
U(a, b)             & \indic{a\leq x\leq b}\frac 1 {b-a}                                          &  & \frac{1}{2}(a+b) & \frac{1}{12}(b-a)^2 & \frac{e^{tb} - e^{ta}}{t(b-a)}
\\
Exp(\lambda)        & \indic{0 \leq x}\lambda e^{-\lambda x}                                      &  & \frac 1 \lambda  & \frac 1 {\lambda^2} & \frac \lambda {\lambda - t}
\\
\Gamma(s, \lambda)  & \indic{0 \leq x} \frac{\lambda e^{-\lambda x} (\lambda x)^{s-1}}{\Gamma(s)} &  & \frac s \lambda  & \frac s {\lambda^2} & \left( \frac{\lambda}{\lambda - t} \right)^s \\
N(\mu, \sigma^2)    & \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}                    &  & \mu              & \sigma^2            & e^{\mu t + \frac{1}{2}\sigma^2 t^2}
\end{align*}

\begin{itemize}
\tightlist
\item
  Why you need the Stieltjes Integral: let
  \(X \sim B(n, \frac 1 2), Y \sim U(0, 1),\) and
  \begin{align*}
  Z = 
  \begin{cases}
  X, & $X = 1$ \\
  Y, & else
  \end{cases}
  \end{align*} then \(\abs{Z} = \abs{\RR}\) so \(Z\) is not discrete,
  but \(P(X = 1) = \frac 1 2 \neq 0\) so \(Z\) is not continuous.
  Definition:
  \begin{align*}  
  \int _ { a } ^ { b } g ( x ) ~d F ( x ) = \lim \sum _ { i = 1 } ^ { n } g \left( x _ { i } \right) \left( F \left( x _ { i } \right) - F \left( x _ { i - 1 } \right) \right)
  .\end{align*}
\end{itemize}

\hypertarget{common-problems}{%
\subsection{Common Problems}\label{common-problems}}

\begin{itemize}
\tightlist
\item
  Birthday Paradox
\item
  Coupon Collectors

  \begin{itemize}
  \tightlist
  \item
    Given \(X = \theset{1, \cdots n}\), what is the expected number of
    draws until all \(n\) outcomes are seen?
  \end{itemize}
\end{itemize}

\hypertarget{notes-shortcuts-misc}{%
\subsection{Notes, Shortcuts, Misc}\label{notes-shortcuts-misc}}

\begin{itemize}
\tightlist
\item
  When computing expected values, variation, etc, just insert a
  parameter \(k\) and compute the moments \(E[X^k]\). Then with a
  solution in terms of \(k\), let \(k=1,2\) etc.
\item
  Neat property of pdfs:
  \(P(X \in N_\varepsilon(a)) \approx \varepsilon f(a)\)
\end{itemize}

\begin{definition}[The Gamma Function]

\begin{align*}  
\Gamma(x+1) = \int_{\RR^{>0}} e^{-t} t^x ~dt
.\end{align*} Integrate by parts to obtain functional relation
\(\Gamma(x+1) = x\Gamma(x)\)

\end{definition}

\begin{proposition}[Boole's Inequality]

\begin{align*}
P(\union_\NN A_i) \leq \sum_\NN P(A_i)
\end{align*}

\end{proposition}

\begin{itemize}
\tightlist
\item
  For any function \(f:X \to \RR\), there is a lower bound:
  \(\max_{x\in X}f(x) \geq E[f(x)]\)
\end{itemize}

\begin{definition}[Moment Generating Functions]

Define
\begin{align*}
M(t) = E[e^{Xt}]
\end{align*}

\begin{itemize}
\tightlist
\item
  Then \(M^{(n)}(0)\) is the \(n\dash\)th moment,
  i.e.~\(M'(0) = E[X], M''(0) = E[X^2]\), etc.
\item
  \(M_{X+Y}(t) = M_X(t)M_Y(t)\) (if independent)
\item
  \(M_{aX + b}(t) = e^{bt} M_X(at)\)
\item
  \(f_X = \mathcal{F}^{-1}(M_X(it))\), denoting the inverse Fourier
  transform,
\end{itemize}

\end{definition}

\hypertarget{combinatorics}{%
\section{Combinatorics}\label{combinatorics}}

\hypertarget{notation-4}{%
\subsection{Notation}\label{notation-4}}

\begin{align*}
S_n                                & = \theset{1,2,\ldots n}                                   &  & \text{the symmetric group} \\
{n\choose k}                       & = \frac{n!}{k!(n-k)!}                                     &  & \text{binomial coefficient}\\
n^{\underline k}                   & = n(n-1) \cdots (n-k+1) = k!{n\choose k}                  &  & \text{falling factorial}\\
n^{\overline k}                    & = n(n+1) \cdots (n+k-1) = k!{n + n - 1 \choose n}         &  & \text{rising factorial}\\
{n \choose {m_1, m_2, \cdots m_k}} & = \frac{n!}{\prod_{i=1}^k m_i!}                           &  & \text{multinomial coefficient}
\end{align*}

Note that the rising and falling factorials always have exactly \(k\)
terms.

Multinomial: A set of \(n\) items divided into \(k\) distinct, disjoint
subsets of sizes \(m_1 \cdots m_k\). Multinomial theorem:
\begin{align*}
(x_1 + x_2 + \cdots x_k )^n 
= \sum_{\substack{m_1, m_2, \cdots, m_k \\ ~~~\sum m_i = n}} 
{n \choose m_1,m_2,\cdots, m_k}
x_1^{m_1}x_2^{m_2}\cdots x_k^{m_k}
\end{align*} which contains \(n + r - 1 \choose r - 1\) terms.

Inclusion-Exclusion:
\begin{align*}
\abs{\union_{i=1}^n A_i} 
= \sum_i \abs A_i - \sum_{i_1 < i_2}^{n\choose 2^2} \abs{A_{i_1} \intersect A_{i_2}} + 
\sum_{i_1 < i_2 < i_3}^{n \choose 2^3} \abs{A_{i_1} \intersect A_{i_2} \intersect A_{i_3}} + 
\cdots +(-1)^{n+1} \abs{\intersect_{i=1}^n A_i} \\ 
= \sum_{k=1}^n ~\sum_{i_1 < \cdots < i_k}  (-1)^{k+1} \abs{\intersect_{j=1}^k A_{i_j}}
\end{align*}

\hypertarget{the-important-numbers}{%
\subsection{The Important Numbers}\label{the-important-numbers}}

\begin{itemize}
\tightlist
\item
  Binomial Coefficients

  \begin{itemize}
  \tightlist
  \item
    The Binomial Theorem:
    \begin{align*}
      (x+y)^n = \sum_{k=0}^n {n\choose k}x^ky^{n-k}
      \end{align*}
  \item
    Choosing:\(n \choose k\)
  \item
    Choosing with repetition allowed: \({n+k-1}\choose k\)
  \end{itemize}
\item
  Signed Stirling Numbers of the First Kind: \(s(n,k)\)

  \begin{itemize}
  \tightlist
  \item
    Count the number of permutations of \(n\) elements with \(k\)
    disjoint cycles, i.e.~the number of elements elements in \(S_n\)
    that are the product of \(k\) disjoint cycles (including trivial
    cycles that fix a point).
  \item
    Recurrence relation:
    \begin{align*}  
      s(n,k) = s(n-1, k-1) + ks(n-1, k)
      .\end{align*}
  \item
    Relation to falling factorial:
    \(x^{\underline n} = \sum_{k=1}^n s(n,k)x^k\)
  \end{itemize}
\item
  Stirling Numbers of the Second Kind: \(\stirling{n}{k}\)

  \begin{itemize}
  \tightlist
  \item
    Counts the number of ways to partition a set \(N\) into \(k\)
    non-empty subsets \(S_i\) (i.e.~such that
    \(S_i \cap S_j = \emptyset,~\coprod_{i=1}^k S_i = N\))
  \item
    Recurrence relation:
    \begin{align*}
      \stirling{n+1}{k} = k\stirling{n}{k} + \stirling{n}{k-1} \\ 
      \stirling 0 0 = 1,
      \quad \stirling n 0 = \stirling 0 n = 0
      \end{align*}
  \item
    Explicit formula:
    \(\stirling n k = \frac 1 {k!} \sum_{i=0}^k (-1)^{k-i} {k \choose i} i^n\)
  \item
    \(B_n = \sum_{i=0}^n \stirling n i\)
  \end{itemize}
\item
  Betti Numbers
\item
  Bell Numbers
\item
  Compositions

  \begin{itemize}
  \tightlist
  \item
    A composition of \(n\) is a way of writing \(n\) as a sum of
    strictly positive integers, ie. \(k_1 + k_2 + \cdots k_i = n\) where
    each \(0 < k_i \leq n\), where order matters (and distinct orders
    count as distinct compositions).
  \item
    Weak compositions: identical, but some terms are allowed to be zero.
  \item
    Number of compositions of \(n\) into \(k\) parts:
    \(n-1 \choose k - 1\)
  \item
    Number of \emph{weak} compositions of \(n\) into \(k\) parts:
    \(n+k-1 \choose n\)
  \item
    Total number of compositions of \(n\) (into any number of parts):
    \(2^{n-1}\)
  \end{itemize}
\item
  Partitions

  \begin{itemize}
  \tightlist
  \item
    A partition of \(n\) is a composition of \(n\) quotiented by
    permutations of the ordering of terms.

    \begin{itemize}
    \tightlist
    \item
      Example: 2 compositions of \(5\) involving \(1\) and \(4\), given
      by \(4+1\) and \(1+4\), whereas there is only one such partition
      of \(5\) given by \(4+1\).
    \end{itemize}
  \item
    Visualize with Young diagrams
  \end{itemize}
\end{itemize}

\hypertarget{common-problems-1}{%
\subsubsection{Common Problems}\label{common-problems-1}}

\begin{itemize}
\tightlist
\item
  Stars and Bars

  \begin{itemize}
  \tightlist
  \item
    No two bars adjacent: \(n-1\choose k-1\)
  \item
    Allowing adjacent bars: \(n+k-1 \choose k-1\)
  \end{itemize}
\end{itemize}

Coupon Collectors Problem

\hypertarget{the-twelvefold-way}{%
\subsubsection{The Twelvefold Way}\label{the-twelvefold-way}}

Consider a function \(f: N \to K\) where \(\abs{N}=n, \abs{K} = k\).

A number of valid interpretations: - \(f\) labels elements of \(N\) by
elements of \(K\) - For each element of \(N\), \(f\) chooses an element
of \(K\) - \(f\) partitions \(N\) into classes that are mapped to the
same element of \(K\) - Throw each of \(N\) balls into some of \(K\)
boxes

Dictionary: - No restrictions: - Assign \(n\) labels, repetition allowed
- Form a multiset of \(K\) of size \(n\) - Injectivity - Assign \(n\)
labels without repetition - Select \(n\) distinct elements from \(K\) -
Number of \(n\dash\)combinations of \(k\) elements - No more than one
ball per box - Surjectivity: - Use every label at least once - Every
element of \(K\) is selected at least once - ``Indistinguishable'' -
Quotient by the action of \(S_n\) or \(S_k\) - \(n\dash\)permutations =
injective functions - \(n\dash\)combinations = injective functions /
\(S_n\) - \(n\dash\)multisets = all functions / \(S^n\) - Partitions of
\(N\) into \(k\) subsets = surjective functions / \(S_k\) - Compositions
of \(n\) into \(k\) parts = surjective functions / \(S_n\)

\begin{align*}
\begin{array}{c|c|c|c}
\text{Permutations \ Restrictions}  & N \mapsvia{f} K & N \injects K & N \surjects K
\\ \hline
f                                                           & k^n & k^{\underline{n}} & k! \stirling{n}{k}  \\
f \circ \sigma_N                                & {n+k-1}\choose n & k\choose n & {n-1}\choose{n-k} \\
\sigma_X \circ f                                & \sum_{i=0}^k \stirling{n}{i} & \indic{n \leq k} & \stirling{n}{k}\\
\sigma_X \circ f \circ \sigma_N & p_k(n+k) & \indic{n \leq k} & p_k(n)
\end{array}
\end{align*}

In words (todo: explain)

\begin{longtable}[]{@{}llll@{}}
\toprule
\begin{minipage}[b]{0.18\columnwidth}\raggedright
Perm. / Rest.\strut
\end{minipage} & \begin{minipage}[b]{0.27\columnwidth}\raggedright
---\strut
\end{minipage} & \begin{minipage}[b]{0.27\columnwidth}\raggedright
Injective\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright
Surjective\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.18\columnwidth}\raggedright
---\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
A sequence of any \(n\) elements of \(K\)\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
Sequences of \(n\) \emph{distinct} elements of \(K\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
Compositions of \(N\) with exactly \(k\) subsets\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.18\columnwidth}\raggedright
Permutations of \(N\)\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
Multisets of \(K\) with \(n\) elements\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
An \(n\dash\)element subset of \(K\)\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
Compositions of \(n\) with \(k\) terms\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.18\columnwidth}\raggedright
Permutations of \(X\)\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
Partitions of \(N\) into \(\leq k\) subsets\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
?\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
Partitions of \(N\) into exactly \(k\) nonempty subsets\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.18\columnwidth}\raggedright
Both\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
Partitions of \(n\) into \(\leq k\) parts\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
?\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright
Partitions of \(n\) into exactly \(k\) parts\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Proofs/Explanations: todo

\begin{itemize}
\tightlist
\item
  Counting non-isomorphic things: Pick a systematic way. Can descend my
  maximum vertex degree, or ascend by adding nodes/leaves.
\end{itemize}

\hypertarget{complex-analysis}{%
\section{Complex Analysis}\label{complex-analysis}}

\begin{itemize}
\tightlist
\item
  \(\lim_{z\rightarrow z_0} f(z) = x_0 + iy_0\) iff the component
  functions limit to \(x_0\) and \(y_0\) respectively. Moreover, both
  ways are equal!
\end{itemize}

Notation: \(z = a + ib, f(z) = u(x, y) + iv(x, y)\)

\hypertarget{useful-equations-and-definitions}{%
\subsection{Useful Equations and
Definitions}\label{useful-equations-and-definitions}}

\begin{align*}
\abs{z} &= \sqrt{a^2 + b^2} \\
\abs{z}^2 = z\conjugate{z} &= a^2 + b^2 \\
\frac{z\conjugate{z}}{\abs{z}^2} &= \frac{(a+ib)(a-ib)}{a^2 + b^2} = 1 \\
\frac{1}{z} &= \frac{\conjugate{z}}{\abs{z}^2} = \frac{a-ib}{a^2+b^2}
\\
e^{zx} = e^{(a+ib)x} &= e^{ax}(\cos(bx) + i\sin(bx)) \\
x^z &\definedas e^{z\ln x} \\
\mathrm{Log}(z) &= \ln\abs{z} + i~\mathrm{Arg}(z) \\
\cos z &= \frac{1}{2}(e^{iz} + e^{-iz}) \\
\sin z &= \frac{1}{2i}(e^{iz} - e^{-iz}) \\
(x-z)(x-\conjugate{z}) &= x^2 - 2\realpart{z}x + (a^2+b^2)
\\
\frac { \partial } { \partial z } &= \frac { 1 } { 2 } \left( \frac { \partial } { \partial x } - i \frac { \partial } { \partial y } \right)
\\
\frac { \partial } { \partial \overline { z } } &= \frac { 1 } { 2 } \left( \frac { \partial } { \partial x } + i \frac { \partial } { \partial y } \right)
\end{align*}

\hypertarget{complex-arithmetic-and-calculus}{%
\subsection{Complex Arithmetic and
Calculus}\label{complex-arithmetic-and-calculus}}

\begin{itemize}
\tightlist
\item
  \(n\dash\)th roots:
  \begin{align*}e^{\frac{ki}{2\pi n}}, \qquad k = 1, 2, \cdots n-1\end{align*}
\end{itemize}

\hypertarget{complex-differentiability}{%
\subsubsection{Complex
Differentiability}\label{complex-differentiability}}

\begin{align*}
z' = \lim_{h\to 0} \frac{f(z+h)-f(z)}{h}
\end{align*} - A complex function that is not differentiable at a point:
\(f(z) = z/\bar{z}\) at \(z=0\)

\begin{itemize}
\tightlist
\item
  Cauchy-Riemann Equations
  \begin{align*}
  u_x = v_y \hspace{4em}u_y = -v_x
  \end{align*}
\item
  Alternatively:

  \begin{itemize}
  \tightlist
  \item
    \(\dd[f]{\bar z} = 0\)
  \item
    \(\inner{\nabla u}{\nabla v} = 0\)
  \item
    \(\Delta u = \Delta v = 0\) (both components are harmonic)
  \end{itemize}
\end{itemize}

\hypertarget{complex-integrals}{%
\subsection{Complex Integrals}\label{complex-integrals}}

The main theorem:
\begin{align*}
\oint_C f(z)~dz = 2\pi i \sum_k \mathrm{Res}(f, z_k)
\end{align*}

Computing residues:
\begin{align*}
\operatorname { Res } ( f , c ) = \frac { 1 } { ( n - 1 ) ! } \lim _ { z \rightarrow c } \frac { d ^ { n - 1 } } { d z ^ { n - 1 } } \left( ( z - c ) ^ { n } f ( z ) \right) \\
f(z) = \frac{g(z)}{h(z)} \implies \operatorname { Res } ( f , c ) = \frac{g(c)}{h'(c)}
\end{align*}

Definitions

\begin{itemize}
\tightlist
\item
  Analytic: differentiable everywhere
\item
  Entire
\item
  Holomorphic
\item
  Meromorphic
\end{itemize}

Complex Analytic \(\implies\) smooth and all derivatives are analytic

\begin{quote}
Not true in real case, take the everywhere differentiable but not
\(C^1\) function
\begin{align*}
f(x) = 
\begin{cases}
-\frac{1}{2}x^2 & x < 0 \\ 
\frac{1}{2}x^2 & x \geq 0
\end{cases}
\end{align*}
\end{quote}

\hypertarget{definitions-3}{%
\subsection{Definitions}\label{definitions-3}}

In these notes, \(C\) generally denotes some closed contour,
\(\mathbb{H}\) is the upper half-plane, \(C_R\) is a semicircle of
radius \(R\) in \(\mathbb{H}\), \(f\) will denote a complex function.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Analytic}

  \(f\) is analytic at \(z_0\) if it can be expanded as a convergent
  power series in some neighborhood of \(z_0\).
\item
  \textbf{Holomorphic}

  A function \(f\) is holomorphic at a point \(z_0\) if \(f'(z_0)\)
  exists in a neighborhood of \(z_0\).

  (Note - this is more than just being differentiable at a single
  point!)

  \emph{Big Theorem}: \(f\) is a holomorphic complex function iff \(f\)
  is analytic.
\item
  \textbf{Meromorphic}

  Holomorphic, except for possibly a finite number of singularities.
\item
  \textbf{Conformal}

  \(f\) is conformal at \(z_0\) if \(f\) is analytic at \(z_0\) and
  \(f'(z_0) \neq 0\).
\item
  \textbf{Harmonic}

  A function \(u(x,y)\) is harmonic if it satisfies Laplace's equation,
  \begin{align*}\Delta u = u_{xx} + u_{yy} = 0\end{align*}
\end{enumerate}

Some other notions to look up:

\begin{itemize}
\tightlist
\item
  Conformal maps
\item
  Analytic
\item
  Theorem: Analytic \(\implies\) conformal
\item
  The Sterographic projection. Is it conformal?
\item
  Branch points and branch cuts
\item
  Loxodromic transformations
\item
  Horocycles
\item
  Analytic Continuation
\item
  The complex logarithm
\item
  Mobius transformations
\item
  Curvature
\item
  Angular Excess
\end{itemize}

\hypertarget{preliminary-notions}{%
\subsection{Preliminary Notions}\label{preliminary-notions}}

\hypertarget{what-is-the-complex-derivative}{%
\subsubsection{What is the Complex
Derivative?}\label{what-is-the-complex-derivative}}

In small neighborhoods, the derivative of a function at a point rotates
it by an angle \(\Delta\theta\) and scales it by a real number
\(\lambda\) according to
\begin{align*}\Delta\theta = \arg f'(z_0), ~\lambda = |f'(z_0)|\end{align*}

\hypertarget{nth-roots-of-a-complex-number}{%
\subsubsection{\texorpdfstring{\(n\)th roots of a complex
number}{nth roots of a complex number}}\label{nth-roots-of-a-complex-number}}

The \(n\)th roots of \(z_0\) are given by writing
\(z_0 = re^{i\theta}\), and are
\begin{align*}
\zeta = \ts{ \sqrt[n]{r} \oldexp\left[{i\left( \frac{\theta}{n} + \frac{2k\pi}{n}\right)}\right] \st k = 0,1,2, \ldots, n-1 }
\end{align*}

or equivalently

\begin{align*}\zeta = \left\{ \sqrt[n]{r}\omega_n^k \mid k = 0,1,2,\ldots, n-1\right\}~\text{where}~\omega_n = e^{\frac{2\pi i}{n}}\end{align*}

This can be derived by looking at
\(\left( re^{i\theta + 2k\pi}\right)^{\frac{1}{n}}\).

It is also useful to immediately recognize that
\(z^2+a = (z-i\sqrt{a})(z+i\sqrt{a})\).

\hypertarget{the-cauchy-riemann-equations}{%
\subsubsection{The Cauchy-Riemann
Equations}\label{the-cauchy-riemann-equations}}

If \(f(x+iy) = u(x,y) + iv(x,y)\) or
\(f(re^{i\theta}) = u(r,\theta) + iv(r,\theta)\), then \(f\) is complex
differentiable if \(u,v\) satisfy

\begin{align*}\begin{aligned}
    u_x &= v_y &\quad u_y &= -v_x \\
    r u_r &= v_\theta &\quad u_\theta &= -r v_r\end{aligned}\end{align*}

In this case,
\begin{align*}f'(x+iy) = u_x(x,y) + iv_x(x,y)\end{align*} or in polar
coordinates,
\begin{align*}f'(re^{i\theta}) = e^{i\theta}(u_r(r,\theta) + iv_r(r,\theta))\end{align*}

\hypertarget{integration}{%
\subsection{Integration}\label{integration}}

\hypertarget{the-residue-theorem}{%
\subsubsection{The Residue Theorem}\label{the-residue-theorem}}

If \(f\) is meromorphic inside of a closed contour \(C\), then
\begin{align*}\oint_C f(z) dz = 2\pi i \sum_{z_k} \underset{z=z_k}{\text{Res}} f(z)\end{align*}

where \(\underset{z=z_k}{\text{Res}} f(z)\) is the coefficient of
\(z^{-1}\) in the Laurent expansion of \(f\).

If \(f\) is analytic everywhere in the interior of \(C\), then
\(\oint_C f(z) dz = 0\).

If \(f\) is meromorphic inside of a contour \(C\) and analytic
everywhere else, one can equivalently calculate the residue at infinity

\begin{align*}\oint_C f(z) dz = 2\pi i \sum_{z_k} \underset{z=0}{\text{Res}} ~z^{-2}f(z^{-1})\end{align*}

\hypertarget{computing-residues}{%
\subsubsection{Computing Residues}\label{computing-residues}}

\hypertarget{simple-poles}{%
\subsubsection{Simple Poles}\label{simple-poles}}

If \(z_0\) is a pole of order \(m\), define \(g(z) := (z-z_0)^m f(z)\).

{If \(g(z)\) is analytic and \(g(z_0) \neq 0\)}, then
\begin{align*}\underset{z=z_0}{\text{Res}} f(z) = \frac{\phi^{(m-1)}(z_0)}{(m-1)!}\end{align*}

In the case where \(m=1\), this reduces to
\begin{align*}\underset{z=z_0}{\text{Res}} f(z) = \phi(z_0)\end{align*}

To compute residues this way, attempt to write \(f\) in the form

\begin{align*}f(z) = \frac{\phi(z)}{(z-z_0)^m}\end{align*}

where \(\phi\) only needs to be analytic at \(z_0\).

\hypertarget{rational-functions}{%
\subsubsection{Rational Functions}\label{rational-functions}}

If \(f(z) = \frac{p(z)}{q(z)}\) where

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(p(z_0) \neq 0\)
\item
  \(q(z_0) = 0\)
\item
  \(q'(z_0) \neq 0\)
\end{enumerate}

then the residue can be computed as

\begin{align*}\underset{z=z_0}{\text{Res}} \frac{p(z)}{q(z)} = \frac{p(z_0)}{q'(z_0)}\end{align*}

\hypertarget{computing-integrals-1}{%
\subsubsection{Computing Integrals}\label{computing-integrals-1}}

When computing real integrals, the following contours can be useful:

One often needs bounds, which can come from the following lemmas

\textbf{The Arc Length Bound} If \(|f(z)| \leq M\) everywhere on \(C\),
then
\begin{align*}|\oint_C f(z) dz | \leq M L_C\end{align*} where \(L_C\) is
the length of \(C\).

\textbf{Jordan's Lemma:} If \(f\) is analytic outside of a semicircle
\(C_R\) and \(|f(z)| \leq M_R\) on \(C_R\) where \(M_R \rightarrow 0\),
then
\begin{align*}\int_{C_R} f(z) e^{iaz} dz \rightarrow 0\end{align*}.

Can also be used for integrals of the form \(\int f(z) \cos az dz\) or
\(\int f(z) \sin az dz\), just take real/imaginary parts of \(e^{iaz}\)
respectively.

\hypertarget{conformal-maps}{%
\subsection{Conformal Maps}\label{conformal-maps}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Linear Fractional Transformations:
\end{enumerate}

\begin{align*}
f(z) = \frac{az+b}{cz+d}\qquad f^{-1}(z) = \frac{-dz+b}{cz-a}
\end{align*}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \([z_1, z_2, z_3] \mapsto [w_1, w_2, w_3]\)

  Every linear fractional transformation is determined by its action on
  three points. Given 3 pairs points \(z_i \mapsto w_i\), construct one
  using the implicit equation
\end{enumerate}

\begin{align*}
\frac{(w-w_1)(w_2-w_3)}{(w-w_3)(w_2-w_1)} = \frac{(z-z_1)(z_2-z_3)}{(z-z_3)(z_2-z_1)}
\end{align*}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  \(z^k: \text{Wedge} \mapsto \mathbb{H}\)

  Just multiplies the angle by \(k\). If a wedge makes angle \(\theta\),
  use \(z^\frac{\pi}{\theta}\).

  It is useful to know that \(z\mapsto z^2\) is equivalent to
  \((x,y) \mapsto (x^2-y^2, 2xy)\).
\item
  \(e^z: \mathbb{C} \mapsto \mathbb{C}\)

  \begin{longtable}[]{@{}lcl@{}}
  \toprule
  \endhead
  Horizontal lines & \(\mapsto\) & rays from origin\tabularnewline
  Vertical lines & \(\mapsto\) & circles at origin\tabularnewline
  Rectangles & \(\mapsto\) & portions of wedges/sectors\tabularnewline
  \bottomrule
  \end{longtable}

  \begin{figure}
  \centering
  \includegraphics{figures/ez-line.png}
  \caption{image}
  \end{figure}

  \begin{figure}
  \centering
  \includegraphics{figures/ez-grid.png}
  \caption{image}
  \end{figure}

  \begin{figure}
  \centering
  \includegraphics{figures/ez-rect.png}
  \caption{image}
  \end{figure}
\item
  \(\log: \mathbb{H} \mapsto \mathbb{R} + i[0, \pi]\)

  Just the inverse of what the exponential map does.

  \begin{longtable}[]{@{}lcl@{}}
  \toprule
  \endhead
  Rays & \(\mapsto\) & Horizontal Lines\tabularnewline
  Wedges & \(\mapsto\) & Horizontal Strips\tabularnewline
  \bottomrule
  \end{longtable}

  \begin{figure}
  \centering
  \includegraphics{figures/log.png}
  \caption{\(z \mapsto \log z\)}
  \end{figure}
\item
  \(\sin: [0, \pi/2] + i\mathbb{R} \mapsto \mathbb{H}_{\mathcal{R}(z)>0}\)

  Maps the infinite strip to the first quadrant.

  \begin{figure}
  \centering
  \includegraphics{figures/sin.png}
  \caption{\(z \mapsfrom \sin w\)}
  \end{figure}
\item
  \(z\mapsto\frac{i-z}{i+z}: \mathbb{H} \mapsto D^\circ\).

  \begin{longtable}[]{@{}lcl@{}}
  \toprule
  \endhead
  \(\mathbb{R}_{>0}\) & \(\mapsto\) & Upper half of
  \(D^\circ\)\tabularnewline
  \(\mathbb{R}_{<0}\) & \(\mapsto\) & Bottom half of
  \(D^\circ\)\tabularnewline
  \bottomrule
  \end{longtable}

  Has inverse \(w \mapsto i\frac{1-w}{1+w}\)
\item
  \(z\mapsto z + z^{-1}: \partial D \mapsto \mathbb{R}\)

  \begin{figure}
  \centering
  \includegraphics{figures/fluid-cylinder.png}
  \caption{\(z \mapsto z+z^{-1}\)}
  \end{figure}

  Maps the boundary of the circle to the real axis, and the plane to
  \(\mathbb{H}\).
\end{enumerate}

\hypertarget{applications}{%
\subsubsection{Applications}\label{applications}}

It is mostly important to know that composing a harmonic function on one
domain with an analytic function produces a new harmonic function on the
new domain.

Similarly, composing the solution to a boundary value problem on a
domain with a conformal map produces a new solution to a new boundary
problem in the new domain, where the new boundary is given by the
conformal image of the old one.

The general technique is use solutions to the boundary value problem on
a simple domain \(D\), and compose one or several conformal maps to map
a given problem into \(D\), then pull back the solution.

\hypertarget{heat-flow-steady-temperatures}{%
\paragraph{Heat Flow: Steady
Temperatures}\label{heat-flow-steady-temperatures}}

Generally interested in finding a harmonic function \(T(x,y)\) which
represents the steady-state temperature at any point. Usually given as a
Dirichlet problem on a domain \(D\) of the form

\begin{figure}
\centering
\includegraphics[width=0.35\textwidth,height=\textheight]{figures/heat-eqn.png}
\caption{image}
\end{figure}

\begin{align*}
\Delta T &= 0 \\
T(\partial D) &= f(\partial D)
\end{align*}

where \(f\) is a given function that prescribes values on
\(\partial D\), the boundary of \(D\).

Embed this in an analytic function with its harmonic conjugate to yield
solutions of the form \(F(x+iy) = T(x,y) + iS(x,y)\).

The \textbf{isotherms} are given by \(T(x,y) = c\).

The \textbf{lines of flow} are given by \(S(x,y) = c\).

\begin{figure}
\centering
\includegraphics[width=0.35\textwidth,height=\textheight]{figures/sin.png}
\caption{image}
\end{figure}

Any easy solution on the domain \(\mathbb{R} \times i[0,\pi]\) in the
\(u,v\) plane, where

\begin{align*}
T(x, 0) &= 0 \\
T(x, \pi) &= 1 
\end{align*} is given by \(T(u,v) = \frac{1}{\pi}v\).

It is harmonic, as the imaginary part of the analytic
\(F(u+iv) = \frac{1}{\pi}(u+iv)\), since every analytic function has
harmonic component functions.

Similar methods work with different domains, just pick a smooth
interpolation between the boundary conditions.

\hypertarget{fluid-flow}{%
\paragraph{Fluid Flow}\label{fluid-flow}}

\begin{figure}
\centering
\includegraphics[width=0.35\textwidth,height=\textheight]{figures/fluid.png}
\caption{image}
\end{figure}

Write \(F(z) = \phi(x,y) + i\psi(x,y)\). Then \(F\) is the complex
potential of the flow, \(\overline{F'}\) is the velocity, and setting
\(\psi(x,y) = c\) yields the streamlines.

A solution in \(\mathbb{H}\) is \(F(z) = Az\) some some velocity \(A\).
Apply conformal mapping appropriately.

\begin{figure}
\centering
\includegraphics[width=0.35\textwidth,height=\textheight]{figures/fluid-cylinder.png}
\caption{image}
\end{figure}

\hypertarget{theorems}{%
\subsubsection{Theorems}\label{theorems}}

\hypertarget{general-theorems}{%
\paragraph{General Theorems}\label{general-theorems}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Liouville's Theorem}:

  If \(f\) is entire and bounded on \(\mathbb{C}\), then \(f\) is
  constant.
\item
  If \(f\) is continuous in a region \(D\), \(f\) is bounded in \(D\).
\item
  If \(f\) is differentiable at \(z_0\), \(f\) is continuous at \(z_0\).

  Note - the converse need not hold!
\item
  If \(f = u + iv\) , where \(u,v\) satisfy the Cauchy-Riemann equations
  \textbf{and} have continuous partials, then \(f\) is differentiable.

  Note - continuous partials are not enough, consider \(f(z) = |z|^2\).
\item
  Rouch's Theorem

  If \(p(z) = f(z) + g(z)\) and \(|g(z)| < |f(z)|\) everywhere on \(C\),
  then \(f\) and \(p\) have the same number of zeros with \(C\).
\item
  \textbf{The Argument Principle}

  If \(f\) is analytic on a closed contour \(C\) and meromorphic within
  \(C\), then
  \begin{align*}
    W \da \frac{1}{2\pi}\Delta_C \arg f(z) = Z - P
    \end{align*}

  \emph{Proof:} Evaluate the integral \(\oint_C \frac{f'(z)}{f(z)} dz\)
  first by parameterizing, changing to polar, and using the FTC, and
  second by using residues directly from the Laurent series.
\item
  \textbf{The Main Story}: The following are equivalent

  \begin{itemize}
  \item
    \(f\) is continuous
  \item
    \(f'\) exists
  \item
    \(f\) is analytic
  \item
    \(f\) is conformal
  \item
    \(f\) satisfies the Cauchy-Riemann equations
  \end{itemize}
\end{enumerate}

\hypertarget{theorems-about-analytic-functions}{%
\paragraph{Theorems About Analytic
Functions}\label{theorems-about-analytic-functions}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  If \(f\) is analytic on \(D\), then \(\oint_C f(z) dz = 0\) for any
  closed contour \(C \subset D\).

  Note: this does not require \(f\) to be \(f'\) to be continuous on
  \(C\).
\item
  \textbf{Maximum Modulus Principle}

  If \(f\) is analytic in a region \(D\) and not constant, then
  \(|f(z)|\) attains its maximum on \(\partial D\).
\item
  If \(f\) is analytic, then \(f^{(n)}\) is analytic for every \(n\). If
  \(f = u(x,y) + iv(x,y)\), then all partials of \(u,v\) are continuous.
\item
  If \(f\) is analytic at \(z_0\) and \(f'(z_0) \neq 0\), then \(f\) is
  conformal at \(z_0\).
\item
  If \(f = u+iv\) is analytic, then \(u,v\) are harmonic conjugates.
\item
  If \(f\) is holomorphic, \(f\) is \(C_\infty\) (smooth).
\item
  If \(f\) is analytic, \(f\) is holomorphic.

  \emph{Proof:} Since \(f\) has a power series expansion at \(z_0\), its
  derivative is given by the term-by-term differentiation of this
  series.
\end{enumerate}

\hypertarget{some-useful-formulae}{%
\subsubsection{Some Useful Formulae}\label{some-useful-formulae}}

\begin{align*}
  f_{x_0}(x) = f(x_0) + f'(x_0)(x-x_0) + \frac{1}{2!}f''(x_0)(x-x_0)^2 + \ldots
  \end{align*}

\begin{align*}
\frac{1}{1-z} = \sum_k z^k
\end{align*}

\begin{align*}
  e^z = \sum_k \frac{1}{k!} z^k
  \end{align*}

\begin{align*}
    \left(\sum_i a_i z^i \right) \left( \sum_j b_j z^j\right) = \sum_n \left( \sum\limits_{i+j=n}a_ib_j \right) z^n
    \end{align*}

\begin{align*}\begin{aligned}
%   
\cos z 
&= \frac{1}{2}(e^{iz} + e^{-iz})
&
&= 1 - \frac{z^2}{2!} + \frac{z^4}{4!} - \ldots \\
%
\cosh z 
&= \frac{1}{2}(e^{z} + e^{-z}) 
&= \cos iz 
&= 1 + \frac{z^2}{2!} + \frac{z^4}{4!} + \ldots \\
%
\sin z 
&= \frac{1}{2i}(e^{iz} - e^{-iz}) 
&
&= z - \frac{z^3}{3!} + \frac{z^4}{4!} - \ldots \\
%
\sinh z 
&= \frac{1}{2}(e^{z} - e^{-z}) 
&= -i\sin iz 
&= z + \frac{z^3}{3!} + \frac{z^4}{4!} + \ldots \\\end{aligned}\end{align*}
Mnemonic: just remember that cosine is an even function, and that the
even terms of \(e^z\) are kept. Similarly, sine is an odd function, so
keep the odd terms of \(e^z\).

\textbf{Harmonic Conjugate}
\begin{align*}v(x,y) = \int_{(0,0)}^{(x,y)} -u_t(s,t)ds + u_s(s,t)dt\end{align*}

\textbf{The Gamma Function}
\begin{align*}\Gamma(z) = \int_0^\infty x^{z-1} e^{-x} dx\end{align*}

Useful to know: \(\Gamma(\frac{1}{2}) = \sqrt\pi\).

\hypertarget{questions}{%
\subsection{Questions}\label{questions}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  True or False: If \(f\) is analytic and bounded in \(\mathbb{H}\),
  then \(f\) is constant on \(\mathbb{H}\).

  False: Take \(f(z) = e^{-z}\), where \(|f(z)| \leq 1\) in
  \(\mathbb{H}\).
\item
  Compute \(\int_{-\infty}^{\infty} \frac{\sin x}{x(x^2+a^2)}dx\)

  Two semicircles needed to avoid singularity at zero. Limit equals the
  residue at zero, solution is
  \(\pi (\frac{1}{a^2} - \frac{e^{-a}}{a^2})\).
\item
  Compute \(\int_0^{2\pi} \frac{1}{2+\cos\theta}d\theta\)

  Cosine sub, solution is \(\frac{2\pi}{\sqrt{3}}\)
\item
  Find the first three terms of the Laurent expansion of
  \(\frac{e^z+1}{e^z-1}\).

  Equals \(2z^{-1} + 0 + 6^{-1}z + \ldots\)
\item
  Compute \(\int_{S_1} \frac{1}{z^2+z-1}dz\)

  Equals \(i\frac{2\pi}{5}\)
\item
  True or false: If f is analytic on the unit disk
  \(E = \{z : |z| < 1\}\), then there exists an \(a \in E\) such that
  \(|f (a)| \geq |f (0)|\).

  True, by the maximum modulus principal. Suppose otherwise. Then
  \(f(0)\) is a maximum of \(f\) inside \(S_1\). But by the MMP, \(f\)
  must attain its maximum on \(\partial S_1\).
\item
  Prove that if \(f(z)\) and \(f (\bar{z})\) are both analytic on a
  domain D, then f is constant on D

  Analytic \(\implies\) Cauchy-Riemann equations are satisfied. Also
  have the identity \(f' = u_x + iv_x\), and \(f' = 0\) \(\implies\)
  \(f\) is constant.
\end{enumerate}

\hypertarget{common-mistakes}{%
\section{Common Mistakes}\label{common-mistakes}}

\begin{align*}
-x^{-2} &\neq \int x^{-1} = \int \frac{1}{x} = \ln x \\ 
\frac{1}{x} &\neq \int \ln x = x\ln x - x \\
\int x^{-k} = \frac{1}{-k+1}x^{-k+1} &\neq \frac{1}{-(k+1)}x^{-(k+1)} \\
\text{ e.g. } \int x^{-2} = -x^{-1} &\neq -\frac{1}{3}x^{-3}
\lim_{n\to\infty} \frac{n}{n+1} = 1 \neq 0\\
\frac{\partial}{\partial x}a^x = \frac{\partial}{\partial x}e^{x\ln a} = e^{x\ln a} \ln a = a^x \ln a.
\end{align*}

Exponentials: when in doubt, write \(a^b = e^{b\ln a}\)

\begin{align*}
\frac{\partial}{\partial x} x^{f(x)} = ?
\end{align*}

\begin{align*}
\sum x^k = \frac{1}{1-x} \neq \frac{1}{1+x} = \sum (-1)^k x^k
\end{align*}

\hypertarget{appendix-1}{%
\section{Appendix 1}\label{appendix-1}}

\hypertarget{neat-tricks}{%
\subsection{Neat Tricks}\label{neat-tricks}}

\begin{itemize}
\tightlist
\item
  Commuting differentials and integrals:
\end{itemize}

\begin{align*}
\frac{d}{dx} \int_{a(x)}^{b(x)} f(x,t) dt = f(x, b(x))\frac{d}{dx}b(x) - f(x, a(x))\frac{d}{dx}a(x) + \int_{a(x)}^{b(x)} \frac{\partial}{\partial x} f(x, t) dt
\end{align*}

\begin{itemize}
\tightlist
\item
  Need \(f,dfdxf, \frac{df}{dx}\) to be continuous in both variables.
  Also need \(a(x),b(x)C1a(x),b(x) \in C_1\).
\item
  If \(a,b\) are constant, boundary terms vanish.
\item
  Recover the fundamental theorem with
  \(a(x)=a,b(x)=ba(x) = a, b(x) = b\), and \(f(x,t)=f(t)f(x,t) = f(t)\).
\end{itemize}

\hypertarget{big-derivative-integral-table}{%
\subsection{Big Derivative / Integral
Table}\label{big-derivative-integral-table}}

\begin{align*}
\frac{\partial f}{\partial{x}}\Leftarrow && f                           && \Rightarrow\int f dx   \\
\hline \\
\frac{1}{2\sqrt{x}}                      && \sqrt{x}                    && \frac{2}{3}x^{\frac{3}{2}} \\
nx^{n-1}                                 && x^n, n \neq -1              && \frac{1}{n+1}x^{n+1} \\
-nx^{-(n+1)}                             && \frac{1}{x^n}, n \neq 1     && -\frac{1}{n-1}x^{-(n-1)} \\
\frac{1}{x}                              && {\ln(x)}       && x\ln(x) - x \\
a^x\ln(a)                                && a^x                         && \frac{a^x}{\ln a} \\
\cos(x)                                  && \sin(x)                     && -\cos(x) \\
-\csc(x)\cot(x)                          && \csc(x)                     && \ln\abs{\csc(x)-\cot(x)} \\
-\sin(x)                                 && \cos(x)                     && \sin(x) \\
\sec(x)\tan(x)                           && \sec(x)                     && \ln\abs{\sec(x) + \tan(x)} \\
\sec^2(x)                                && \tan(x)                     && \ln\abs{\frac{1}{\cos x}} \\
-\csc^2(x)                               && \cot(x)                     && \ln \abs{\sin x} \\
\frac{1}{1+x^2}                          && {\tan^{-1}(x)} && x\tan^{-1}x - \frac{1}{2}\ln(1+x^2) \\
\frac{1}{\sqrt{1-x^2}}                   && {\sin^{-1}(x)} && x\sin^{-1}x+ \sqrt{1-x^2} \\
-\frac{1}{\sqrt{1-x^2}}                  && {\cos^{-1}(x)} && x\cos^{-1}x -\sqrt{1-x^2} \\
\frac{1}{\sqrt{x^2+a}}                   && \ln\abs{x+\sqrt{x^2+a}}     && \cdot\\
2\sin x\cos x                            && \sin^2(x)                   && \frac{1}{2}(x - \sin x \cos x) \\
-2\sin x\cos x                           && \cos^2(x)                   && \frac{1}{2}(x + \sin x \cos x) \\
2\csc^2(x)\cot(x)                        && \csc^2(x)                   && -\cot(x) \\
2\sec^2(x)\tan(x)                        && \sec^2(x)                   && \tan(x) \\
?                                        && ?                           && ? \\
?                                        && ?                           && ? \\
?                                        && ?                           && ? \\
?                                        && ?                           && ? \\
?                                        && ?                           && ? \\
?                                        && ?                           && ? \\
?                                        && ?                           && ? \\
(ax+1)e^{ax}                             && xe^{ax}                     && \frac { 1 } { a ^ { 2 } } ( a x - 1 ) e ^ { a x } \\
?                                        && e^{ax}\sin(bx)              && \frac { 1 } { a ^ { 2 } + b ^ { 2 } } e ^ { a x } ( a \sin b x - b \cos b x ) \\
?                                        && e^{ax}\cos(bx)              && \frac { 1 } { a ^ { 2 } + b ^ { 2 } } e ^ { a x } ( a \sin b x + b \cos b x ) \\
?                                        && ?                           && ? \\
\end{align*}

\hypertarget{useful-series-and-sequences}{%
\subsection{Useful Series and
Sequences}\label{useful-series-and-sequences}}

Notation: \(\uparrow, \downarrow\): monotonically converges from
below/above.

\begin{itemize}
\tightlist
\item
  Taylor Series:
\end{itemize}

\begin{align*}
f ( x ) = \sum _ { n = 0 } ^ { \infty } \frac { f ^ { ( n ) } \left( x _ { 0 } \right) } { n ! } \left( x - x _ { 0 } \right) ^ { n }
\end{align*}

\begin{itemize}
\tightlist
\item
  Cauchy Product:
\end{itemize}

\begin{align*}
\left( \sum_{k=0}^\infty a_k x^k \right)\left( \sum_{k=0}^\infty b_i x^n \right) = \sum_{k=0}^\infty \left( \sum_{i=0}^k a_{n} b_{n} \right)x^k
\end{align*}

\begin{itemize}
\tightlist
\item
  Differentiation:
\end{itemize}

\begin{align*}
\frac{\partial}{\partial x} \sum_{k=i}^\infty a_kx^k = \sum_{k=i+1}^\infty k\,a_k x^{k-1}
\end{align*}

\begin{itemize}
\tightlist
\item
  Common Series
  \begin{align*}
  &\sum_{k=0}^{N} x^k                                                                                                                        &=  \frac{1-x^{N+1}}{1-x} &\\
  &\sum_{k=1}^\infty x^k                                                                                                                     &=  \frac{1}{1-x}& \quad\text{ for } \abs{x} < 1 \\
  &\sum_{k=1}^{\infty } k x ^ {k - 1 }                                                                                                       &=  \frac{1}{( 1 - x ) ^ { 2 } }& \quad \text { for } | x | < 1 \\
  &\sum_{k=2}^{\infty } k ( k - 1 ) x ^ {k - 2 }                                                                                             &=  \frac{2}{( 1 - x ) ^ { 3 } } & \quad \text { for } | x | < 1  \\
  &\sum_{k=3}^{\infty } k ( k - 1 ) ( k - 2 ) x ^ {k - 3 }                                                                                   &=  \frac{6}{( 1 - x ) ^ { 4 } } & \quad \text { for } | x | < 1   \\
  &\sum_{k=1}^\infty {n\choose k} x^k y^{n-k}                                                                                                &=  (x+y)^n& \\
  &\sum_{k=1}^{\infty } \frac{x ^ {k } } {k }                                                                                                &=  -\log ( 1 - x )& \\
  &\sum_{k=0}^{\infty } \frac{x ^ {k } } {k ! }                                                                                              &=  e^x & \\
  &\sum_{ n = 0}^{\infty } \frac{( - 1 ) ^ {k }}{( 2 n + 1 ) ! } x ^ { 2 k + 1 } \quad = x - \frac{x ^ { 3 }}{3 ! } + \frac{x ^ { 5 }}{5 ! } &=  \sin(x) & \\
  &\sum_{k=0}^{\infty } \frac{( - 1 ) ^ {k }}{( 2 n ) ! } x ^ { 2 k } \quad = 1 - \frac{x ^ { 2 }}{2 ! } + \frac{x ^ { 4 }}{4 ! }            &=  \cos(x)& \\
  &\sum_{k=0}^{\infty } \frac{( - 1 ) ^ {k }}{2 n + 1 } x ^ { 2 k + 1 } \quad = x - \frac{x ^ { 3 }}{3 } + \frac{x ^ { 5 }}{5 }              &=  \arctan(x) & \\
  &\sum_{k=0}^{\infty } \frac{1}{( 2 k + 1 ) ! }x ^ { 2 n + 1 } \quad = x + \frac{x ^ { 3 }}{3 ! } + \frac{x ^ { 5 }}{5 ! } + \cdots         &=  \sinh(x) & \\
  &\sum_{k=0}^{\infty } \frac{1}{( 2 k ) ! }x ^ { 2 k } \quad = 1 + \frac{x ^ { 2 }}{2 ! } + \frac{x ^ { 4 }}{4 ! } + \cdots                 &=  \cosh(x) & \\
  &\sum_{k=0}^{\infty } \frac{x ^ { 2 k + 1 }}{2 k + 1 }                                                                                     &=  \operatorname { arctanh } x  & \\
  &\sum_{k=1}^\infty \frac{1}{k}                                                                                                             &=  \infty &\\
  &\sum_{k=1}^\infty (-1)^k \frac{1}{k}                                                                                                      &=  \ln (2) & \\
  &\sum_{k=1}^N \frac{1}{k}                                                                                                                  &= _\approx \ln(N) + \gamma + \frac{1}{2N} & \\
  &\sum_{k=1}^{\infty } \frac{1 } {k ^ { 2 } }                                                                                               &=  \frac{\pi ^ { 2 }}{6 }& \\
  \end{align*}
\end{itemize}

\hypertarget{partial-fraction-decomposition}{%
\subsection{Partial Fraction
Decomposition}\label{partial-fraction-decomposition}}

Given \(R(x) = \frac{p(x)}{q(x)}\), factor \(q(x)\) into
\(\prod q_i(x)\).

\begin{itemize}
\item
  Linear factors of the form \(q_i(x) = (ax+b)^n\) contribute
  \begin{align*}
  r_i(x) = \sum_{k=1}^n \frac{A_k}{(ax+b)^k} = \frac{A_1}{ax+b} + \frac{A_2}{(ax+b)^2} + \cdots
  \end{align*}
\item
  Irreducible quadratics of the form \(q_i(x) = (ax^2+bx+c)^n\)
  contribute
  \begin{align*}
  r_i(x) = \sum_{k=1}^n \frac{A_k x + B_k}{(ax^2+bx+c)^k} = \frac{A_1x+B_1}{ax^2+bx+c} + \frac{A_2x+B_2}{(ax^2+bx+c)^2} + \cdots
  \end{align*}

  \begin{itemize}
  \tightlist
  \item
    Note: \(ax^2+bx+c\) is irreducible \(\iff b^2 < 4ac\)
  \end{itemize}
\item
  Write \(R(x) = \frac{p(x)}{\prod q_i(x)} = \sum r_i(x)\), then solve
  for the unknown coefficients \(A_k, B_k\).

  \begin{itemize}
  \item
    IMPORTANT SHORTCUT: don't try to solve the resulting linear system:
    for each \(q_i(x)\), multiply through by that factor and evaluate at
    its root to zero out many terms!
  \item
    For linear terms \(q_i(x) = (ax+b)^n\), define
    \(P(x) = (ax+b)^nR(x)\); then
    \begin{align*}
    A_{k} =  \frac{1}{(n-k)!}P^{(n-k)}(a), \quad k = 1,2,\cdots n \\ \implies A_n= P(a),~ A_{n-1} = P'(a),~ \cdots,~ A_1 = \frac{1}{(n-1)!}P^{(n-1)}(A)
    \end{align*}
  \item
    Note: \#todo check, might need to evaluate at \(-b/a\) instead,
    extend to quadratics.
  \end{itemize}
\end{itemize}

\hypertarget{properties-of-norms}{%
\subsection{Properties of Norms}\label{properties-of-norms}}

\begin{align*}
\norm{t\mathbf x}                  & = \abs{t} \norm{\mathbf x} \\
\abs{\inner{\mathbf x}{\mathbf y}} & \leq \norm{\mathbf x} \norm{\mathbf y} \\
\norm{\mathbf x+\mathbf y}         & \leq \norm{\mathbf x} + \norm{\mathbf y} \\
\norm{\mathbf x-\mathbf z}         & \leq \norm{\mathbf x-\mathbf y} + \norm{\mathbf y-\mathbf z}
\end{align*}

\hypertarget{logic-identities}{%
\subsection{Logic Identities}\label{logic-identities}}

\begin{itemize}
\tightlist
\item
  \(P \implies Q \iff Q \orr \lnot P\)
\item
  \(P \implies Q \iff \lnot Q \implies \lnot P\)
\item
  \(P \orr(Q \annd S) \iff (P \orr Q) \annd (P \orr S)\)
\item
  \(P \annd(Q \orr S) \iff (P \annd Q) \orr (P \annd S)\)
\item
  \(\lnot (P \annd Q) \iff \lnot P \orr \lnot Q\)
\item
  \(\lnot (P \orr Q) \iff \lnot P \annd \lnot Q\)
\end{itemize}

\hypertarget{set-identities}{%
\subsection{Set Identities}\label{set-identities}}

\begin{align*}
A \union B                          && = && A \union (A^c \intersect B) \\
A                                   && = && (B\intersect A) \union (B^c \intersect A) \\
(\union_\NN A_i)^c                  && = && \intersect_\NN A_i^c \\
(\intersect_\NN A_i)^c              && = && \union_\NN A_i^c \\
A - B                               && = && A \cap B^c \\
(A-B)^c                             && = && A^c \cup B \\
(A\cup B) - C                       && = && (A-C) \cup (B-C) \\
(A\cap B) - C                       && = && (A-C) \cap (B-C) \\
A - (B \cup C)                      && = && (A - B) \cap (A - C) \\
A - (B \cap C)                      && = && (A-B) \cup (A-C) \\
A - (B - C)                         && = && (A-B) \cup (A \cap C) \\
(A-B) \cap C                        && = && (A \cap C) - B && = && A \cap (C-B) \\
(A-B) \cup C                        && = && (A \cup C) - (B-C) \\
A\cup(B\cap C)                      && = && (A\cup B) \cap (A\cup C) \\
A\cap(B\cup C)                      && = && (A\cap B) \cup (A \cap C) \\
A \subseteq C \annd B \subseteq C   &&\implies && A \cup B \subseteq C \\
C \subseteq A \annd C \subseteq B   &&\implies && C \subseteq A \cup B \\
A_k ~\text{countable}               &&\implies && \prod_{k=1}^n A_k, ~ \union_{k=1}^\infty A_k \quad\text{countable}
\end{align*}

\hypertarget{preimage-identities}{%
\subsection{Preimage Identities}\label{preimage-identities}}

Summary

\begin{itemize}
\tightlist
\item
  Injectivity: left cancellation
\item
  Surjectivity: right cancellation
\item
  Everything commutes with unions
\item
  Preimage commutes with everything
\item
  Image generally only results in an inequality
\end{itemize}

Preimage Equations

\begin{itemize}
\tightlist
\item
  \(A \subseteq B \implies f(A) \subseteq f(B) \orr f^{-1}(A) \subseteq f^{-1}(B)\)
\item
  \(f^{-1}(\union_{i\in I}A_i) = \union_{i\in I} f^{-1}(A_i)\)

  \begin{itemize}
  \tightlist
  \item
    Also holds for \(f(\union_{i\in I}A_i) = \union_{i\in I} f(A_i)\)
  \end{itemize}
\item
  \(f^{-1}(\intersect_{i\in I}A_i) = \intersect_{i\in I} f^{-1}(A_i)\)

  \begin{itemize}
  \tightlist
  \item
    Also holds for
    \(f(\intersect_{i\in I}A_i) = \intersect_{i\in I} f(A_i)\)
  \end{itemize}
\item
  \(f^{-1}(A) - f^{-1}(B) = f^{-1}(A-B)\)

  \begin{itemize}
  \tightlist
  \item
    BUT \(f(A) - f(B) \subseteq f(A-B)\)
  \end{itemize}
\item
  For \(X\subset A, Y \subset B\):

  \begin{itemize}
  \tightlist
  \item
    \((\restrictionof{f}{X})^{-1} = X \intersect f^{-1}(Y)\)
  \item
    \((f\circ f^{-1})(Y) = Y \intersect f(A)\)
  \end{itemize}
\item
  Summary: preimage commutes with:

  \begin{itemize}
  \tightlist
  \item
    Union
  \item
    Intersection
  \item
    Complements
  \item
    Difference
  \item
    Symmetric Difference
  \end{itemize}
\end{itemize}

Image Equations

\begin{itemize}
\tightlist
\item
  \(A \subset B \implies f(A) \subset f(B)\)
\item
  \(f(\union A_i) = \union f(A_i)\)
\item
  \(f(\intersect A_i) \subset \intersect f(A_i)\)
\item
  \(f(A-B) \supset f(A) - f(B)\)
\item
  \(f(A^c) = \im(f) - f(A)\)
\end{itemize}

Equations Involving Both

\begin{itemize}
\tightlist
\item
  \(A \subseteq f^{-1}(f(A))\)

  \begin{itemize}
  \tightlist
  \item
    Equal \(\iff f\) is injective
  \end{itemize}
\item
  \(f(f^{-1}(A)) \subseteq A\)

  \begin{itemize}
  \tightlist
  \item
    Equal \(\iff f\) is surjective
  \end{itemize}
\end{itemize}

\hypertarget{pascals-triangle}{%
\subsection{Pascal's Triangle:}\label{pascals-triangle}}

\begin{longtable}[]{@{}ll@{}}
\toprule
\(n\) & Sequence\tabularnewline
\midrule
\endhead
3 & \(1,2,1\)\tabularnewline
4 & \(1,3,3,1\)\tabularnewline
5 & \(1,4,6,4,1\)\tabularnewline
6 & \(1,5,10,10,5,1\)\tabularnewline
7 & \(1,6,15,20,15,16,1\)\tabularnewline
8 & \(1,7,21,35,35,21,7,1\)\tabularnewline
\bottomrule
\end{longtable}

Obtain new entries by adding in \(L\) pattern rotated by \(\pi\) (e.g.~7
= 1+6, 12 = 6 + 15, etc). Note that \(n\choose i\) is given by the entry
in the \(n\dash\)th row, \(i\dash\)th column.

\hypertarget{table-of-small-factorials}{%
\subsection{Table of Small Factorials}\label{table-of-small-factorials}}

\begin{longtable}[]{@{}ll@{}}
\toprule
\(n\) & \(n!\)\tabularnewline
\midrule
\endhead
2 & \(2\)\tabularnewline
3 & \(6\)\tabularnewline
4 & \(24\)\tabularnewline
5 & \(120\)\tabularnewline
6 & \(720\)\tabularnewline
7 & \(5040\)\tabularnewline
8 & \(40320\)\tabularnewline
9 & \(362880\)\tabularnewline
10 & \(3628800\)\tabularnewline
\bottomrule
\end{longtable}

\(\pi \approx 3.1415926535\) \(e \approx 2.71828\)
\(\sqrt{2} \approx 1.4142135\)

\hypertarget{primes-under-100}{%
\subsection{Primes Under 100:}\label{primes-under-100}}

\begin{align*}
& 2, 3, 5, 7, \\
& 11, 13, 17, 19, \\
& 23, 29, \\
& 31, 37, \\
& 41, 43, 47, \\
& 53, 59, \\
& 61, 67, \\
& 71, 73, 79, \\
& 83, 89, \\
& 97, \\
& 101
\end{align*}

\hypertarget{checking-divisibility-by-small-numbers}{%
\subsection{Checking Divisibility by Small
Numbers}\label{checking-divisibility-by-small-numbers}}

Note that \(n\mod 10^k\) yields the last \(k\) digits. Let \(d_i\)
denote the \(i\dash\)th digit of \(n\).

The recursive prime procedure (RPP): for each prime \(p\), there exists
a \(k\) such recursive application of this procedure to \(n\) yields the
same remainder mod \(p\) as \(n\) itself.

\begin{itemize}
\tightlist
\item
  Write \(n_0 = 10x + y\) where \(y = 0 \ldots 9\)
\item
  Let \(n_1 = x + ky\), repeat until \(n_i < 10\).
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[b]{0.30\columnwidth}\raggedright
\(p\)\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
\(p \mid n \iff\)\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Mnemonic\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.30\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(n \equiv 2, 4, 6, 8 \mod 10\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Last digit is even\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
3\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(\sum d_i \equiv 0 \mod 3\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
3 divides the sum of digits (apply recursively)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
4\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(n \equiv 4k \mod 10^2\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Last two digits are divisible by 4\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
5\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(n \equiv 0, 5 \mod 10\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Last digit is 0 or 5\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
6\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(n \equiv 0 \mod 2 \text{ and } n \equiv 0 \mod 3\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Reduce to 2, 3 case\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
7\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
RPP, \(k=-2\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(-20 \equiv 1 \mod 7 \implies 10x+y \equiv 10(x-2y) \mod 7\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
8\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(n \equiv 8k \mod 10^3\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Manually divide the last 3 digits by 8 (or peel off factors of 2)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
9\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(\sum d_i \equiv 0 \mod 9\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
9 divides the sum of digits (apply recursively)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
10\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(n \equiv 0 \mod 10\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
Last digit is 0\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
11\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(\sum (-1)^i d_i \equiv 0 \mod 11\) or\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
11 divides alternating sum\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
13\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
RPP, \(k=4\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(40 \equiv 1 \mod 13 \implies 10x + y \equiv 10(x + 4y) \mod 13\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
17\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
RPP, \(k=-5\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(-50 \equiv 1 \mod 17 \implies 10x + y \equiv 10(x - 5y) \mod 19\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
19\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
RPP, \(k=2\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(20 \equiv 1 \mod 19 \implies 10x + y \equiv 10(x + 2y) \mod 19\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{hyperbolic-functions}{%
\subsection{Hyperbolic Functions}\label{hyperbolic-functions}}

\begin{align*}
\cosh(x)    & = \frac{1}{2}(e^x + e^{-x}) \\
\sinh(x)    & = \frac{1}{2}(e^x - e^{-x}) \\
\cos(iz)    & = \cosh z \\
\cosh(iz)   & = \cos z \\
\sin(iz)    & = \sinh z \\
\sinh(iz)   & = \sin z \\
\sinh^{-1}x & = ? \quad = \ln(x + \sqrt{x^2+1}) \\
\cosh^{-1}x & = ? \quad = \ln(x + \sqrt{x^2-1}) \\
\tanh^{-1}x & = \frac{1}{2}\ln(\frac{1+x}{1-x}) \\
\end{align*}

\hypertarget{integral-tables}{%
\subsection{Integral Tables}\label{integral-tables}}

\begin{align*}
\frac{\partial f}{\partial{x}}\Leftarrow &  & f                       &  & \Rightarrow\int f dx   \\
\hline \\
\frac{1}{2\sqrt{x}}                      &  & \sqrt{x}                &  & \frac{2}{3}x^{\frac{3}{2}} \\
nx^{n-1}                                 &  & x^n, n \neq -1          &  & \frac{1}{n+1}x^{n+1} \\
\frac{1}{x}                              &  & {\ln(x)}                &  & x\ln(x) - x \\
a^x\ln(a)                                &  & a^x                     &  & \frac{a^x}{\ln a} \\
\cos(x)                                  &  & \sin(x)                 &  & -\cos(x) \\
-\sin(x)                                 &  & \cos(x)                 &  & \sin(x) \\
2\sec^2(x)\tan(x)                        &  & \sec^2(x)               &  & \tan(x) \\
2\csc^2(x)\cot(x)                        &  & \csc^2(x)               &  & -\cot(x) \\
\sec^2(x)                                &  & \tan(x)                 &  & \ln\abs{\sec(x)} \\
\sec(x)\tan(x)                           &  & \sec(x)                 &  & \ln\abs{\sec(x) + \tan(x)} \\
-\csc(x)\cot(x)                          &  & \csc(x)                 &  & \ln\abs{\csc(x)-\cot(x)} \\
\frac{1}{1+x^2}                          &  & {\tan^{-1}(x)}          &  & x\tan^{-1}x - \frac{1}{2}\ln(1+x^2) \\
\frac{1}{\sqrt{1-x^2}}                   &  & {\sin^{-1}(x)}          &  & x\sin^{-1}x+ \sqrt{1-x^2} \\
-\frac{1}{\sqrt{1-x^2}}                  &  & {\cos^{-1}(x)}          &  & x\cos^{-1}x -\sqrt{1-x^2} \\
\frac{1}{\sqrt{x^2+a}}                   &  & \ln\abs{x+\sqrt{x^2+a}} &  & \cdot\\
-\csc^2(x)                               &  & \cot(x)                 &  & ? \\
?                                        &  & \cos^2(x)               &  & ? \\
?                                        &  & \sin^2(x)               &  & ? \\
?                                        &  & xe^{ax}                 &  & \frac { 1 } { a ^ { 2 } } ( a x - 1 ) e ^ { a x } \\
?                                        &  & e^{ax}\sin(bx)          &  & \frac { 1 } { a ^ { 2 } + b ^ { 2 } } e ^ { a x } ( a \sin b x - b \cos b x ) \\
?                                        &  & e^{ax}\cos(bx)          &  & \frac { 1 } { a ^ { 2 } + b ^ { 2 } } e ^ { a x } ( a \sin b x + b \cos b x ) \\
?                                        &  & ?                       &  & ?
\end{align*}

\hypertarget{definitions-4}{%
\section{Definitions}\label{definitions-4}}

\begin{align*}  
e^x = \lim_{n \to \infty} \left(1 + \frac{1}{n}\right)^n = \lim_{n \to \infty} \left( \frac{n+1}{n} \right)^n
.\end{align*}

\hypertarget{set-theory}{%
\subsection{Set Theory}\label{set-theory}}

\begin{itemize}
\item
  Injectivity
  \begin{align*}  
  f:X \to Y \text{ injective } \iff \forall x_1,x_2 \in X,\quad  f(x_1) = f(x_2) \implies x_1 = x_2 \\ \iff \forall x_1,x_2 \in X,\quad x_1 \neq x_2 \implies f(x_1) \neq f(x_2)
  .\end{align*}
\item
  Surjectivity
  \begin{align*}  
  f:X \to Y \text{ surjective } \iff \forall y\in Y,~ \exists x\in X : f(x) = y
  .\end{align*}
\item
  Preimage
  \begin{align*}  
  f:X \to Y, U \subseteq Y \implies f^{-1}(U) = \theset{x \in X : f(x) \in U}
  .\end{align*}
\end{itemize}

\hypertarget{calculus}{%
\subsection{Calculus}\label{calculus}}

\begin{itemize}
\item
  Limit
  \begin{align*}
  \lim_{x \to p} f(x) = L \iff \forall\varepsilon,~\exists\delta:\\
  d(x, p) < \delta\implies d(f(x), L) < \varepsilon
  \end{align*}
\item
  Continuity

  \begin{itemize}
  \item
    Epsilon-delta definition:
    \begin{align*}
    f:X \to Y \text{ continuous at } p \iff
    \forall \varepsilon,~ \exists\delta:\\
    d_X(x, p) < \delta \implies d_Y(f(x), f(p)) < \varepsilon
    \end{align*}
  \item
    Limit/Sequential definition:
    \begin{align*}
    f:X \to Y \text{ continuous at } p \iff
    \forall \theset{x_i}_{i\in \NN}\subseteq X: \theset{x_i}\to p,\\
    \lim_{i\to\infty}f(x_i) = f(\lim_{i\to\infty}x_i) = f(p)
    \end{align*}
  \item
    Topological Definition:
    \begin{align*}  
    f:X \to Y \text{ continuous } \iff U \text{ open in } \im(f) \subseteq Y \implies f^{-1}(U) \text{ open in } X  
    .\end{align*}
  \end{itemize}
\item
  Differentiability and the Derivative

  \begin{itemize}
  \item
    For single variable functions:
    \begin{align*}
    f:\RR \to \RR ~\text{differentiable at } p \iff
    \forall \theset{x_i}_{i\in \NN}\to p,\\
    f'(p) \definedas \lim_{i\to\infty} \frac{f(x_i) - f(p)}{x_i - p}< \infty
    \end{align*}
  \item
    For multivariable functions:
    \begin{align*}
    \mathbf{f}:\RR^n \to \RR^m ~\text{differentiable at } \mathbf{p} \iff \exists \text{ a linear map } \mathbf{J}:\RR^n \to \RR^m \text{ such that: } \\ \lim _ { \mathbf{h} \rightarrow 0 } \frac { \left\| \mathbf { f } \left( \mathbf{p} + \mathbf { h } \right) - \mathbf { f } \left( \mathbf { p } \right) - \mathbf { J } ( \mathbf { h } ) \right\|_ {\RR^n}} { \| \mathbf { h } \|_ {\RR^m} } = 0
    \end{align*}
  \end{itemize}
\item
  Gradient
  \begin{align*}  
  \nabla f = [f_x, f_y, f_z]
  .\end{align*}
\item
  Divergence
\item
  Curl
\item
  Taylor Series (at a point \(a\))

  \begin{itemize}
  \tightlist
  \item
    Single Variable \(\RR \to \RR\)
  \end{itemize}

  \begin{align*}  
    T_a(x) = f ( a ) + \frac { f ^ { \prime } ( a ) } { 1 ! } ( x - a ) + \frac { f ^ { \prime \prime } ( a ) } { 2 ! } ( x - a ) ^ { 2 } + \frac { f ^ { \prime \prime \prime } ( a ) } { 3 ! } ( x - a ) ^ { 3 } + \cdots \\ \implies T_a(x) = \sum _ { n = 0 } ^ { \infty } \frac { f ^ { ( n ) } ( a ) } { n ! } ( x - a ) ^ { n }
  .\end{align*}

  \begin{itemize}
  \tightlist
  \item
    Multivariable \(\RR^n \to \RR\):
  \end{itemize}

  \begin{align*}  
    T_a(\mathbf x) = f(\mathbf a) + (\mathbf x - \mathbf a)^T\nabla f(\mathbf a)
  .\end{align*}

  \begin{itemize}
  \tightlist
  \item
    Multivariable \(\RR^n \to \RR^m\):
  \end{itemize}

  \begin{align*}  
    T_{(a,b)} ( x , y ) = f ( a , b ) + ( x - a ) f _ { x } ( a , b ) + ( y - b ) f _ { y } ( a , b ) + \\ \frac { 1 } { 2 ! } \left( ( x - a ) ^ { 2 } f _ { x x } ( a , b ) + 2 ( x - a ) ( y - b ) f _ { y y  } ( a , b ) + ( y - b ) ^ { 2 } f _ { y x } ( a , b ) \right) + \cdots
  .\end{align*}

  \begin{align*}
    T_a ( \mathbf { x } ) = f ( \mathbf { a } ) + ( \mathbf { x } - \mathbf { a } ) ^ { \mathrm { T } } \mathbf{J} ( \mathbf { a } ) + \frac { 1 } { 2 ! } ( \mathbf { x } - \mathbf { a } ) ^ { \mathrm { T } } \mathbf{H} ( \mathbf { a } ) ( \mathbf { x } - \mathbf { a } ) + \cdots \\
    \implies T_a ( \mathbf { x } ) = \sum _ { | \alpha | \geq 0 } \frac { ( \mathbf { x } - \mathbf { a } ) ^ { \alpha } } { \alpha ! } \left( \partial ^ { \alpha } f \right) ( \mathbf { a } )
    \end{align*}
\end{itemize}

\hypertarget{analysis}{%
\subsection{Analysis}\label{analysis}}

\begin{itemize}
\item
  Archimedean Property:
  \(x \in \RR \implies \exists n\in \NN:~ x < n \annd x > 0 \implies \exists n:~ \frac{1}{n} < x\)
\item
  Upper Bound (for S \(\subseteq \RR\))
  \begin{align*}  
  \alpha\text{ is an upper bound for } S \iff s \in S \implies s < \alpha
  .\end{align*}
\item
  Triangle Inequality

  \begin{itemize}
  \tightlist
  \item
    \({ | a + b | \leq | a | + | b | }\)
  \item
    \({ | a - b | \leq | a | + | b | }\)
  \end{itemize}
\item
  Reverse Triangle Inequality

  \begin{itemize}
  \tightlist
  \item
    \({\abs{| a | - | b |} \leq | a - b | }\)
  \end{itemize}
\item
  Least Upper Bound / Supremum (for S \(\subseteq \RR\))
  \begin{align*}  
  \alpha\text{ is a LUB for } S \iff s \in S \implies s < \alpha \annd \forall t : (s \in S \implies s < t),~ \alpha < t
  .\end{align*}
\item
  Greatest Lower Bound / Infimum (for S \(\subseteq \RR\))
  \begin{align*}  
  \alpha\text{ is a GLB for } S \iff s \in S \implies \alpha < s \annd \forall t : (s \in S \implies t < s),~ t < \alpha
  .\end{align*}
\item
  Open Set
\item
  Closed Set
\item
  Limit Point
\item
  Interior Point
\item
  Closure of a Set
\item
  Boundary
\item
  Metric
\item
  Cauchy Sequence:
  \begin{align*}  
  \theset{a_i} \text{ is a cauchy sequence } \iff \forall \varepsilon~\exists N\in\NN: \quad m,n > N \implies d(x_m, x_n) < \varepsilon
  .\end{align*}
\item
  Connected: \(S\) is connected \(\iff\) \(\not\exists U,V\subset S\)
  nonempty, open, disjoint such that \(S = U \cup V\)
\item
  Compact: Every open cover has a finite subcover:
  \begin{align*}  
  X \subseteq \union_{j\in J} V_j \implies \exists I \subseteq J: \abs{I} < \infty \annd X\subseteq \union_{i\in I} V_i
  .\end{align*}
\item
  Sequential Compactness Every sequence has a convergent subsequence:
  \begin{align*}  
  \theset{x_i}_{i\in I}\subseteq X \implies \exists J\subseteq I,~ \exists p\in X: \quad \theset{x_j}_{j\in J} \to p
  .\end{align*}
\item
  Bounded (sequences, subsets, metric spaces)
  \begin{align*}  
  U \subseteq X \text{ is bounded } \iff \exists x\in X, \exists M \in \RR :\quad u\in U \implies d(x, u) < M
  .\end{align*}
\item
  Totally Bounded \todo[inline]{todo}
\item
  Pointwise Convergence
  \begin{align*}
  \text{For}~\{f_n: X \to Y\}_{n\in\NN},\\ f_n \to f \iff
  \forall \varepsilon > 0,~\forall x\in X,~ \exists N(x, \varepsilon)\in\NN :
  \quad n > N \implies d_Y\left(f_n(x),f(x)\right) < \varepsilon
  \end{align*}
\item
  Uniform Convergence
  \begin{align*}
  \text{For}~\{f_n: X \to Y\}_{n\in\NN}, \\
  f_n \rightrightarrows f \iff \forall \varepsilon > 0,~ \exists N(\varepsilon)\in\NN :
  \quad \forall x\in X,~ n > N \implies d_Y(f_n(x), f(x)) < \varepsilon
  \end{align*}
\item
  Generalized Mean Value Theorem
  \begin{align*}  
  (f ( b ) - f ( a ) ) g' ( c ) = (g ( b ) - g ( a )) f' ( c )
  .\end{align*}
\end{itemize}

\hypertarget{linear-algebra-1}{%
\subsection{Linear Algebra}\label{linear-algebra-1}}

Convention: always over a field \(k\), and \(T: k^n \to k^m\) is a
generic linear map (or \(m\times n\) matrix).

\begin{itemize}
\item
  Consistent

  A system of linear equations is \emph{consistent} when it has at least
  one solution.
\item
  Inconsistent

  A system of linear equations is \emph{inconsistent} when it has no
  solutions.
\item
  Rank

  The number of nonzero rows in RREF
\item
  Elementary Matrix
\item
  Row Equivalent
\item
  Pivot
\item
  Cofactor
\end{itemize}

\begin{align*}  
\mathrm{cofactor}(A)_{i,j} = (-1)^{i+j} M_{i, j}
\end{align*} where \(M_{i, j}\) is the minor obtained by deleting the
\(i\dash\)th row and \(j\dash\)th column of \(A\).

\begin{itemize}
\item
  Adjugate
  \begin{align*}  
  \mathrm{adjugate}(A) = \mathrm{cofactor}(A)^T = (-1)^{i+j} M_{j, i}
  .\end{align*}
\item
  Vector Space Axioms

  \begin{itemize}
  \tightlist
  \item
    Let \(k\) be a field and \(\vector{u},\vector{v},\vector{w} \in V\)
    and \(r,s,t\in k\). A vector space \(V\) over \(k\) satisfies:

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \tightlist
    \item
      Closure under addition: \(\vector{v} + \vector{w} \in V\)
    \item
      Closure under scalar multiplication: \(r\vector{v} \in V\)
    \item
      Commutativity of addition:
      \(\vector{v} + \vector{w} = \vector{w} + \vector{v}\)
    \item
      Associativity of addition:
      \(\vector{u} + (\vector{v} + \vector{w}) = (\vector{u}+\vector{v}) + \vector{w}\)
    \item
      Existence of an additive zero \(\vector{0}\) satisfying
      \(\vector{v} + 0 = 0 + \vector{v} = \vector{v}\)
    \item
      Existence of additive inverse \(-\vector{v}\) satisfying
      \(v + (-\vector{v}) = 0\)
    \item
      Unit property: \(1\vector{v} = \vector{v}\)
    \item
      Associativity of scalar multiplication:
      \((rs)\vector{v} = r(s\vector{v})\)
    \item
      Distribution of scalars multiplication over vector addition:
      \(r(\vector{v} + \vector{w}) = r\vector{v} + r\vector{w}\)
    \item
      Distribution of scalar multiplication over scalar addition:
      \((r+s)\vector{v} = r\vector{v} + s\vector{v}\)
    \end{enumerate}
  \end{itemize}
\item
  Subspace

  \begin{itemize}
  \item
    A nonempty subset \(W \subseteq V\) that is a vector space and
    satisfies
    \begin{align*}  
    \theset{ \sum_i c_i \vector x_i \mid c_i \in \FF,~ x_i \in W} \subseteq W
    .\end{align*}
  \item
    Quick counter-check: find \(\vector x, \vector y\) such that
    \(a\vector x + b\vector y \not\in W\)
  \end{itemize}
\item
  Span Given a set of \(n\) vectors
  \(S = \theset{\vector x_i}_{i=1}^n\), defined as
  \begin{align*}  
  \mathrm{Span}(S) = \theset{\sum_{i=1}^nc_i \vector x_i \mid c_i \in k}
  .\end{align*}
\item
  Row Space

  \begin{itemize}
  \item
    The range of the linear map \(T\).
  \item
    Given
    \(T = \begin{bmatrix} \vector x_1 \rightarrow \\ \vector x_2 \rightarrow \\ \vdots \\ \vector x_m \rightarrow \end{bmatrix}\),
    defined as
    \begin{align*}  
    \mathrm{Span}(\theset{\vector x_i}_{i=1}^m) \subseteq k^m
    .\end{align*}
  \item
    \(\mathrm{rowspace}(T)\perp = \mathrm{null}(T)\)
  \item
    \(\abs{\mathrm{rowspace}(T)} = \mathrm{Rank}(T)\)
  \end{itemize}
\item
  Column Space
\item
  Null Space

  \begin{itemize}
  \tightlist
  \item
    Defined as
    \(\mathrm{null}(T) = \theset{\vector{x} \in k^n \mid T(\vector{x}) = 0 \in k^m}\)
  \item
    \(\mathrm{null}(T)\perp = \mathrm{rowspace}(T)\)
  \end{itemize}
\item
  Eigenvalue

  \begin{itemize}
  \tightlist
  \item
    A value \(\lambda\) such that \(Ax = \lambda x\)
  \item
    Invariant under similarity.
  \end{itemize}
\item
  Eigenspace

  \begin{itemize}
  \tightlist
  \item
    For a linear map \(T\) with eigenvalue \(\lambda\), defined as
    \(E_\lambda = \theset{\vector x \in k^n \mid T(\vector x) = \lambda \vector x}\)
  \end{itemize}
\item
  Dimension

  \begin{itemize}
  \tightlist
  \item
    The cardinality of a basis of \(V\)
  \end{itemize}
\item
  Basis

  \begin{itemize}
  \tightlist
  \item
    A linearly independent set of vectors
    \(S = \theset{\vector x_i} \subset V\) such that
    \(\mathrm{Span}(S) = V\)
  \end{itemize}
\item
  Linear independence

  \begin{itemize}
  \tightlist
  \item
    A set of vectors \(\theset{\vector x_i}_{i=1}^n\) is linearly
    independent
    \(\iff \sum_{i=1}^n c_i \vector x_i = 0 \implies c_i = 0\) for all
    \(i\).
  \item
    Can be detected by considering the matrix
    \begin{align*}  
      T = [\vector x_1, \vector x_2, \cdots, \vector x_n]^T
    .\end{align*} (linearly independent iff \(T\) is singular)
  \end{itemize}
\item
  Rank

  \begin{itemize}
  \tightlist
  \item
    Dimension of rowspace
  \end{itemize}
\item
  Rank-Nullity Theorem

  \begin{itemize}
  \tightlist
  \item
    \(\abs{\mathrm{Nullspace}(A)} + \abs{\mathrm{Rank}(A)} = \abs{\mathrm{Codomain}(A)}\)
  \end{itemize}
\item
  Nullspace

  \begin{itemize}
  \tightlist
  \item
    \(\mathrm{nullspace}(A) = \theset{\vector{x}\in \RR^n : A \vector{x} = \vector 0}\)
  \end{itemize}
\item
  Singular

  \begin{itemize}
  \tightlist
  \item
    A square \(n\times n\) matrix \(T\) is singular iff
    \(\mathrm{Rank}(T) < n\)
  \end{itemize}
\item
  Similarity

  \begin{itemize}
  \tightlist
  \item
    Two matrices \(A, B\) are similar iff there exists an invertible
    matrix \(S\) such that \(B = SAS^{-1}\)
  \end{itemize}
\item
  Diagonalizable

  \begin{itemize}
  \tightlist
  \item
    A matrix \(X\) is diagonalizable if it can be written
    \(X = EDE^{-1}\) where \(D\) is diagonal.
  \item
    If \(X\) is \(n\times n\) and has \(n\) linearly independent
    eigenvectors \(\vector\lambda_i\), then
    \(D_{ii} = \vector\lambda_i\), and
    \(E = \begin{bmatrix} \vector\lambda_1 & \vector\lambda_2 & \cdots & \vector \lambda_n \\ \downarrow & \downarrow &\cdots & \downarrow \end{bmatrix}\)
  \end{itemize}
\item
  Positive Definite

  \begin{itemize}
  \tightlist
  \item
    A matrix \(A\) is positive definite iff
    \(\forall \vector x \in k^n,\) we have the scalar inequality
    \(\vector x^T A \vector x > 0\)
  \end{itemize}
\item
  Projection

  \begin{itemize}
  \tightlist
  \item
    The projection of a vector \(\vector v\) onto \(\vector u\) is given
    by
    \(P_{\vector u}(\vector v) = \left< \vector u, \vector v \right> \hat u\)
  \item
    The projection of a vector \(\vector v\) onto a space
    \(U = \mathrm{Span}(\theset{\vector u_i})\) is given by
    \begin{align*}  
      P_U(\vector v) = \sum_i P_{\vector u_i}(\vector v) = \sum_i \left< \vector u_i, \vector v\right> \hat u_i
    .\end{align*}
  \end{itemize}
\item
  Orthogonal Complement

  \begin{itemize}
  \tightlist
  \item
    Given a subspace \(U \subseteq V\), defined as
    \(U\perp = \theset{\vector v \in V \mid \forall \vector u \in U, \left<\vector u, \vector v\right> = 0}\)
  \end{itemize}
\item
  Determinant
  \begin{align*}  
  \det(A) = \sum_{\tau \in S^n}\prod_{i=1}^n \sigma(\tau) a_{i, \tau(i)}
  .\end{align*}
\item
  Trace
  \begin{align*}  
  \mathrm{Tr}(A) = \sum_{i=1}^n A_{ii}
  .\end{align*}
\item
  Characteristic Polynomial

  \begin{itemize}
  \tightlist
  \item
    \(p_A(x) = \det(xI - A)\)
  \item
    Roots of \(p_A\) are eigenvalues of \(A\)
  \end{itemize}
\item
  Symmetric: \(A = A^T\)
\item
  Skew-Symmetric: \(A = -A^T\)
\item
  Inner Product

  \begin{itemize}
  \tightlist
  \item
    \(\inner{\vector x}{\vector x} \geq 0\)
  \item
    \(\inner{\vector x}{\vector x} = 0 \iff \vector x = \vector 0\)
  \item
    \(\inner{\vector x}{\vector y} = \conjugate{\inner {\vector y}{\vector x}}\)
  \item
    \(\inner[k\vector x]{\vector y} = k\inner{\vector x}{\vector y} = \inner{\vector x}{k\vector y}\)
  \item
    \(\inner{\vector x + \vector y}{\vector z} = \inner{\vector x}{\vector z} + \inner[y]{\vector z}\)
  \item
    \(\inner[a\vector x]{b\vector y} = \inner{\vector x}{\vector x} + \inner{a\vector x}{y} + \inner{\vector x}{b\vector y} + \inner{\vector y}{\vector y}\)
  \item
    Defines a norm:
    \(\norm{\vector x} = \sqrt{\inner{\vector x}{\vector x}} \implies \norm{\vector x}^2 = \inner{\vector x}{\vector x}\)
  \end{itemize}
\item
  Cauchy-Schwarz Inequality:
  \(\abs{\inner{\vector x}{\vector y}} \leq \norm{\vector x}\norm{\vector y}\)
\item
  Orthogonality:

  \begin{itemize}
  \tightlist
  \item
    For vectors:
    \(\vector x \perp \vector y \iff \inner{\vector x}{\vector y} = 0\)
  \item
    For matrices: \(A\) is orthogonal \(\iff A^{-1} = A^T\)
  \end{itemize}
\item
  Orthogonal Projection of \(\vector x\) onto \(\vector y\):
  \begin{align*}  
  P(\vector x, \vector y) = \inner{\vector x}{\vector y} \hat y = \inner{\vector x}{\vector y} \frac{\vector y}{\norm{\vector y}^2}
  .\end{align*}

  \begin{itemize}
  \tightlist
  \item
    Note
    \(\norm{P(\vector x, \vector y)} = \norm{\vector x}\cos\theta_{x,y}\)
  \end{itemize}
\item
  Defective: An \(n\times n\) matrix \(A\) is defective \(\iff\) the
  number of linearly independent eigenvectors of \(A\) is less than
  \(n\).
\end{itemize}

\hypertarget{differential-equations}{%
\subsection{Differential Equations}\label{differential-equations}}

\begin{itemize}
\item
  Homogeneous
  \begin{align*}  
  f(x, y) \text{ homogeneous of degree } n \iff \exists n\in\NN : f(tx, ty) = t^nf(x, y).
  .\end{align*}
\item
  Separable
  \begin{align*}  
  p(y)\frac{dy}{dx} - q(x) = 0
  .\end{align*}
\item
  Wronskian:
  \begin{align*}
  W \left[ f _ { 1 } , f _ { 2 } , \ldots , f _ { k } \right] ( x ) = \left| \begin{array} { c c c c } { f _ { 1 } ( x ) } & { f _ { 2 } ( x ) } & { \dots } & { f _ { k } ( x ) } \\ { f _ { 1 } ^ { \prime } ( x ) } & { f _ { 2 } ^ { \prime } ( x ) } & { \dots } & { f _ { k } ^ { \prime } ( x ) } \\ { \vdots } & { \vdots } & { } & { \vdots } \\ { f _ { 1 } ^ { ( k - 1 ) } ( x ) } & { f _ { 2 } ^ { ( k - 1 ) } ( x ) } & { \dots } & { f _ { k } ^ { ( k - 1 ) } ( x ) } \end{array} \right|
  \end{align*}
\item
  Laplace Transform:
  \begin{align*}  
  L_f(s) = \int_0^\infty e^{-st} f(t) ~dt
  .\end{align*}
\end{itemize}

\hypertarget{algebra-1}{%
\subsection{Algebra}\label{algebra-1}}

\begin{itemize}
\tightlist
\item
  Ring
\item
  Group
\item
  Subgroup

  \begin{itemize}
  \tightlist
  \item
    Two step subgroups test:
  \end{itemize}
\item
  Integral Domain
\item
  Division Ring
\item
  Principal Ideal Domain
\item
  Tensor Product: \#todo insert construction
\end{itemize}

\hypertarget{complex-analysis-1}{%
\subsection{Complex Analysis}\label{complex-analysis-1}}

\begin{itemize}
\tightlist
\item
  Analytic
\item
  Harmonic
\item
  Cauchy-Euler Equations
\item
  Holomorphic
\item
  The Complex Derivative
\item
  Meromorphic
\item
  The Gamma Function: Satisfies \(\Gamma(p+1) + p\Gamma(p)\) and
  \(\Gamma(1) = 1\), defined as
  \begin{align*}  
  \Gamma ( p ) = \int _ { 0 } ^ { \infty } t ^ { p - 1 } e ^ { - t } d t , \quad p > 0
  .\end{align*}
\end{itemize}

\hypertarget{algebra-2}{%
\subsection{Algebra}\label{algebra-2}}

\begin{itemize}
\item
  Looking at real roots:

  \begin{itemize}
  \tightlist
  \item
    Let \(p\) be number of sign changes in \(f(x)\);
  \item
    Let \(q\) be number of sign changes in \(f(-x)\);
  \item
    Let \(n\) be the degree of \(f\).
  \item
    Then \(p\) gives the maximum number of positive real roots, \(q\)
    gives the maximum number of negative real roots, and \(n-p-q\) gives
    the \emph{minimum} number of complex roots.
  \item
    Rational Roots Theorem: If \(p(x) = ax^n +\cdots + c\) and
    \(r = \frac{p}{q}\) where \(p(r) = 0\), then \(p \mid c\) and
    \(q \mid a\).
  \end{itemize}
\item
  Properties of logs:

  \begin{itemize}
  \tightlist
  \item
    \(\ln(\prod) = \sum \ln\) but \(\prod \ln \neq \ln \sum\)
  \item
    \(\log_b x = \frac{\ln x}{\ln b}\)
  \end{itemize}
\end{itemize}

Be careful! \(\frac{\ln x}{\ln y} \neq \ln\frac{x}{y} = \ln x - \ln y\)

\begin{itemize}
\tightlist
\item
  Completing the square:

  \begin{itemize}
  \tightlist
  \item
    \(p(x) = ax^2 + bx + c \implies p(x) = a(x+\frac{b}{2a})^2 + -\frac{1}{2}\left(\frac{b^2-4ac}{2a}\right)\)
  \end{itemize}
\end{itemize}

\hypertarget{geometry}{%
\subsection{Geometry}\label{geometry}}

\begin{itemize}
\item
  Generic Conic Sections

  \begin{align*}A x ^ { 2 } + B x y + C y ^ { 2 } + D x + E y + F = 0\end{align*}

  \begin{align*}\frac{(x-x_0)^2}{w_0} \pm \frac{(y-y_0)^2}{h_0} = c\end{align*}
\item
  Circles:
  \begin{align*} Ax^2 + By^2 + C = 0 \hspace{5em} (x-x_0)^2 + (y-y_0)^2 = r^2\end{align*}

  \begin{itemize}
  \tightlist
  \item
    Defining trait: locus of points at a constant distance from the
    \textbf{center}
  \item
    \textbf{Center} at \((x_0, y_0)\)
  \end{itemize}
\item
  Parabolas:
  \begin{align*}Ax^2 + Bx + Cy + D = 0 \hspace{5em} y = ax^2\end{align*}

  \begin{itemize}
  \tightlist
  \item
    Defining Trait:

    \begin{itemize}
    \tightlist
    \item
      Locus of points equidistant from the \textbf{focus} (a point) and
      the \textbf{directrix} (a line)
    \item
      \#todo add image
    \end{itemize}
  \item
    \textbf{Focus} at \((0, \frac{1}{4a})\)
  \item
    \textbf{Directrix} at the line \(y = -\frac{1}{4a}\)

    \begin{itemize}
    \tightlist
    \item
      For an arbitrary quadratic: complete the square to write in the
      form \(y = a(x - w_0)^2 + h_0\), and translate points of interest
      by by \((x+w_0, y+h_0)\)
    \end{itemize}
  \end{itemize}
\item
  Ellipses:
  \begin{align*}\frac{x^2}{w^2} + \frac{y^2}{h^2} = 1\end{align*}

  \begin{itemize}
  \tightlist
  \item
    Defining trait:

    \begin{itemize}
    \tightlist
    \item
      The locus of points where the \emph{sum} of distances to two
      \textbf{focii} are constant.
    \end{itemize}
  \item
    \textbf{Center} at \((0,0)\) (can translate easily)
  \item
    \textbf{Vertices} at \((\pm w, 0)\) and \((0, \pm h)\)
  \item
    \textbf{Focii} at
    \(F_1 = (\sqrt{w^2-h^2}, 0), F_2 = (-\sqrt{w^2-h^2}, 0)\)
  \item
    Another useful shortcut form:
  \end{itemize}
\item
  Hyperbolas:
  \begin{align*}\frac{x^2}{w^2}-\frac{y^2}{h^2} = 1\end{align*}

  \begin{itemize}
  \tightlist
  \item
    Defining trait:

    \begin{itemize}
    \tightlist
    \item
      Locus of points where the \emph{difference} between the distances
      to two \textbf{focii} are constant.
    \end{itemize}
  \item
    \textbf{Vertices} at \((0, \pm h)\) and \((\pm w, 0)\)
  \item
    \textbf{Focii} at
    \(F_1 = (\sqrt{w^2+h^2}, 0), F_2 = (-\sqrt{w^2+h^2}, 0)\)
  \end{itemize}
\item
  Summary of Traits:

  \begin{itemize}
  \tightlist
  \item
    One point \(p\):

    \begin{itemize}
    \tightlist
    \item
      Distance to \(p\) is constant: circle
    \end{itemize}
  \item
    Two points \(a,b\):

    \begin{itemize}
    \tightlist
    \item
      Distance to \(a\) equal to distance to \(b\) equals a constant: a
      line bisecting the midpoint of the line connecting them
    \item
      Difference of distances constant: ellipse
    \item
      Sum of differences constant: hyperbola
    \end{itemize}
  \item
    Point \(p\) and a line \(l\):

    \begin{itemize}
    \tightlist
    \item
      Distance to \(p\) equals distance to \(l\) equals a constant:
      parabola
    \end{itemize}
  \end{itemize}
\item
  Areas of certain figures:
\end{itemize}

\newpage

\newpage
\section{Indices}
\listoftodos[List of Todos]

% Hook into amsthm environments to list them.
\renewcommand{\listtheoremname}{Definitions}
\listoftheorems[ignoreall,show={definition}, numwidth=3.5em]

\renewcommand{\listtheoremname}{Theorems}
\listoftheorems[ignoreall,show={theorem,proposition}, numwidth=3.5em]

\renewcommand{\listtheoremname}{Exercises}
\listoftheorems[ignoreall,show={exercise}, numwidth=3.5em]

\listoffigures


\printbibliography[title=Bibliography]


\end{document}
