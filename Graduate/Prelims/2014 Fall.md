# Fall 2014

1. Parts
   1. $f:X \to Y$ is surjective $\iff \forall y\in Y,~\exists x\in X \suchthat y=f(x)$.
   2. $f:X \to Y$ is not injective $\iff \exists x_1\neq x_2 \in X \suchthat f(x_1) = f(x_2).$
   3. $f: (X, d_x) \to (Y, d_y)$ is uniformly continuous $\iff \forall \varepsilon \exists \delta(\varepsilon) \suchthat \forall x_1,x_2\in X,~ \quad d_x(x_1, x_2) \leq \delta \implies d_y(f(x_1), f(x_2)) \leq \varepsilon.$

      *(Note that $\delta$ can only depend on $\varepsilon$ and must work for all $x_1, x_2$ simultaneously.)*
   4. $f$ is not uniformly continuous $\iff \exists \varepsilon \suchthat \forall \delta, \exists x_1, x_2\in X \suchthat \quad d_x(x_1, x_2) \leq \delta ~\&~ d_y(f(x_1), f(x_2)) > \varepsilon.$

2. Base case: for $n=1$, we have $a_1 = 1 \leq a_2 = \frac{16} 3 \leq 10.$ Suppose this holds for $k < n$, then 
$$
a_{n-1} \leq a_n = \frac{a_{n-1}}{3} + 5 \implies 3a_{n-1} \leq a_{n-1} + 15 \implies a_{n-1} \leq \frac{15}{2}
$$

   and thus we have
$$
a_{n+1} = \frac{a_n}{3} + 5 = \frac{1}{3}(a_n + 15) \\ 
= \frac{1}{3}((\frac{a_{n-1}}{3} + 5) + 15) \\
= \frac{a_{n-1} + 60}{9} \\
\leq \frac{\frac{15}{2} + 60}{9} \\
= \frac{150}{18} \\
< \frac{180}{18} = 10,
$$

and $a_{n+1} \leq 10$. Moreover, note that the relation $a_{n+1} = \frac{a_n}{3} + 5$ can be rewritten as 
$$
a_n = 3a_{n+1} - 15, \\ 
a_{n-1} = 3a_n - 15.
$$ Using the inductive hypothesis $a_{n-1} \leq a_n$, we can thus write
$$
3a_n - 15 = a_{n-1} \leq a_n = 3a_{n+1} - 15,
$$

from which we get $3a_{n} - 15 \leq 3a_{n+1} - 15$ and thus $a_{n} \leq a_{n+1}$.

To compute $\lim_{n\to\infty}a_n$, perhaps there are easier ways, but we can just use generating functions. Note that the limit exists by the **Monotone Convergence Theorem**. Let $A(x) = \sum_{n=0}^\infty a_n x^n$ where $a_0 = 0$. Then applying the magic sauce, we have
$$\begin{align*}
a_n = \frac{1}{3}a_{n-1} + 5 &\implies \sum_{n=1}^\infty a_nx^n = \frac{1}{3}\sum_{n=1}^\infty a_{n-1}x^n + 5\sum_{n=1}^\infty x^n \\
&\implies A(x) - a_0 = \frac 1 3 xA(x) + 5\left( \frac 1 {1-x} - 1\right) \\
&\implies A(x)\left(1 - \frac x 3\right) = 5\left( \frac x {1-x}\right) \\
&\implies A(x) = 15\left(\frac 1 {3-x} \right)\left(\frac x {1-x} \right) \\
&\implies A(x) = \frac{15x}{(3-x)(1-x)} \\
&\implies A(x) = \frac{-\frac{45}{2}}{3-x} + \frac{\frac{15}{2}}{1-x} \\
&\implies A(x) = \frac 3 2 \left(-5 \left( \frac{1}{1-\frac x 3} \right) + 5\left( \frac 1 {1-x}\right) \right) \\
&\implies A(x) = \frac{15}{2} \sum_{n=0}^\infty \left(1 - \left( \frac 1 3\right)^n\right)x^n \\
&\implies a_n = \frac {15} 2 \left(1 - \left( \frac 1 3\right)^n\right)
\end{align*}$$

and so we find 
$$
\lim_{n\to\infty}a_n = \lim_{n\to\infty}\frac {15} 2 \left(1 - \left( \frac 1 3\right)^n\right) = \frac{15}{2}. \qed
$$

1. Parts
   1. Suppose $\exists M_g \suchthat \forall x,~ g(x) < M$. Then let $\varepsilon > 0$ be arbitrarily chosen; we want to show that there exists a $\delta$ such that $\abs{x} \leq \delta \implies \abs{f(x)g(x)} \leq \varepsilon$. Since $\lim_{x\to 0} f(x) = 0$, there exists a $\delta_f$ such that $\abs{x} \leq \delta_f \implies \abs{f(x)} \leq \frac{\varepsilon}{M_g}$. So letting $\delta = \delta_f$, we have
$$
\abs{x} \leq \delta \implies \abs{f(x)g(x)} \leq \abs{\frac{\varepsilon}{M_g}g(x)} \leq \abs{\frac{\varepsilon}{M_g}M_g} = \varepsilon. \qed 
$$ 
     1. Let $f(x) = x$ and $g(x) = \frac{1}{x}$. Note that $g(x)$ is not bounded in any neighborhood of zero, and $f(x)g(x) = 1 \not\to 0$.

1. Let $\vector w_i$ be the proposed new basis elements -- we can write down a linear map that sends $\vector v_i$ to $\vector w_i$, which is given by the matrix 
$$
A = \left[ \begin{array} { l l l } { 1 } & { 0 } & { 0 } \\ { 1 } & { 1 } & { 1 } \\ { 0 } & { - 1 } & { 2 } \end{array} \right].
$$

    Then, if $\mathrm{colspace}~A = \RR^3$, then the columns of $A$ will span $\RR^3$ and thus form a basis for it. One can compute $\det A = 3 \neq 0$, and so $\mathrm{nullity}~A = 0$. Then by rank-nullity, $\rank A = 3$ and thus the columns span $\RR^3$ as desired.

    Expanding on why this works, let $\vector x \in \RR^3$ be an arbitrary vector. Since $\vector v_i$ are a basis, we have $\vector x = a_1\vector v_1 + a_2 \vector v_2 + a_3\vector v_3$. If the $\vector w_i$ are to be a basis, we should be able to write 
  $$
  \vector x = b_1\vector w_1 + b_2 \vector w_2 + b_3\vector w_3 \\ 
  = b_1(\vector v_1 + \vector v_2) + b_2 (\vector v_2 - \vector v_3) + b_3(\vector v_2 + 2\vector v_3) \\
  = (b_1)\vector v_1 + (b_2 + b_3)\vector v_2 + (-b_2 + 2b_3)\vector v_3
  $$

    which yields a linear system of equations
  $$
  a_1 = b_1 \\
  a_2 = b_2 + b_3 \\
  a_3 = -b_2 + 2b_3
  $$

    which is can be written as the matrix equation
  $$
  \left[ \begin{array} { l } { a _ { 1 } } \\ { a _ { 2 } } \\ { a _ { 3 } } \end{array} \right] = \left[ \begin{array} { l l l } { 1 } & { 0 } & { 0 } \\ { 1 } & { 1 } & { 1 } \\ { 0 } & { - 1 } & { 2 } \end{array} \right] \left[ \begin{array} { l } { b_1 } \\ { b _ { 2 } } \\ { b _ { 3 } } \end{array} \right] \iff \vector a = A \vector b
  $$

    which has a solution iff $A$ is invertible iff $A$ is full rank. $\qed$.

1. We first note that we can rewrite the equation of the region to obtain something more familiar: $x^2 + y^2 = 2x \implies (x-1)^2 + y^2 = 1$, which is a translated circle. Integrating over this region will be easy compared to the line integral, so we apply Green's theorem:
$$
\int_C xe^x ~dx + ye^y +x^2 ~dy = \iint_D 2x ~dA.
$$

  We can parameterize this region as $D = \theset{(r(1+ \cos\theta), r\sin\theta) \suchthat \theta \in [0, 2\pi), r\in [0, 1]}$. Noting that $dA = r~dr~d\theta$, we can then integrate
$$
\iint_D 2x ~dA = 2\int_0^{2\pi} \int_0^1 1 + \cos\theta ~dr ~d\theta = 4\pi. \qed
$$

1. Parts
   1. $A$ is symmetric, so by the spectral theorem $A$ will be diagonalizable by its matrix of eigenvectors. We can compute the characteristic polynomial
$$
p_\chi(x) = x^2 - (\Tr A)x + \det A = x^2 - 7x + 6 = (x-6)(x-1),
$$

      and so $\spec(A) = \theset{6,1}$. Computing the kernel of $A-\lambda I$ for each of these yields
$$
\vector v_1 = \left[ \begin{array} { c } { 1 } \\ { - 2 } \end{array} \right],
\vector v_2 = \left[ \begin{array} { c } { 2 } \\ { 1 } \end{array} \right],
$$

       To ensure that the matrix we get is orthogonal, we have to do Gram-Schmidt. Since all eigenvectors were distinct, the $\vector v_i$ are already orthogonal, so we just need to normalize. This yields
$$
\vector w_1 = \widehat{\vector v_1} = \frac 1 {\sqrt{5}} \left[ \begin{array} { c } { 1 } \\ { - 2 } \end{array} \right],\\
\vector w_2 = \frac 1 {\sqrt{5}} \left[ \begin{array} { c } { 2 } \\ { 1 } \end{array} \right],
$$
  
       We can then check
$$
\implies P = \frac 1 {\sqrt{5}} \left[ \begin{array} { l l } { 1 } & { 2 } \\ { - 2 } & { 1 } \end{array} \right], \quad PP^T + I
$$

        and
$$
 P^{-1} = \frac{\sqrt{5}}{5}\left[ \begin{array} { l l } { 1 } & { -2 } \\ { 2 } & { 1 } \end{array} \right], \quad PAP^{-1} = \left[\begin{array}{rr}
6 & 0 \\
0 & 1
\end{array}\right].
$$

    1. We want to compute $\min\theset{\inner{A\vector x}{\vector x}\suchthat \vector x \in S^2}$. I don't know how to do this from the given information, we can use the fact that the operator norm is defined as $\norm{A}_{\mathrm{op}} = \max\theset{\sqrt \lambda \suchthat \lambda \in \spec (A)}$ along with the fact that $\lambda \in \spec(A) \implies \lambda\inv \in \spec(A\inv)$. 
    
        We know that $\spec A = \theset{1,6}$ from above. This gives $\norm{A\inv}_{\mathrm{op}} = \min\theset{\sqrt{\lambda} \suchthat \lambda \in \spec (A)}$. So in this case, we get $\sqrt 1 = 1$. $\qed$

        Perhaps a second way to do this is to use the fact that 
$$
\inner{A\vector x}{\vector x} = \vector x^T A^T \vector x = \vector x^T (PDP^T)^T \vector x \\
= (P^T \vector x)^T D (P^T\vector x) = \vector y^T D \vector y = \inner{D\vector y}{ \vector y},
$$

        for some vector $\vector y = P \vector x$, and since $P$ is orthogonal, $\norm{\vector y} = 1$ as well. So we're reduced to the case of minimizing his inner product instead. Letting $\vector y = \thevector{y_1, y_2}$, we can multiply this inner product out to formulate this as a constrained optimization problem
$$
f(y_1, y_2) = 6y_1^2 + y_2^2 \\
g(y_1, y_2) = y_1^2 + y_2^2 = 1
$$

        which can be approached with Lagrange multipliers, i.e. looking at where $\nabla f = \lambda \nabla g$. This forces the conclusion that either $y_1 =0$ or $y_2=0$, yielding 4 combinations for $\vector y$ that can exhaustively tested, where it turns out that the minimum inner product is equal to 1. $\qed$

        A third way uses the Cauchy-Schwarz inequality, i.e. $\abs{\inner{\vector u}{\vector v}} \leq \norm{\vector u} \norm{\vector v}$, which is saturated when $\vector u = \lambda \vector v$. Letting $A\vector x = \vector u$ and $\vector x = \vector v$, this amounts to asking when $A\vector x = \lambda \vector x$, i.e. $\lambda$ is an eigenvalue of $A$. Moreover, $\norm{A\vector x} = \norm{\vector x}$ since $A$ is orthogonal, and $\norm{\vector x} = 1$ since $\vector x \in S^2$. Thus we have 

        $$
        \abs{\inner{A\vector x}{\vector x}} \leq \norm{A\vector x} \norm{\vector x} = \lambda\\
        $$,

        and the RHS is minimized by the smallest eigenvalue, i.e. $\lambda = 1$. $\qed$

1. We need to show that $R$ is reflexive, transitive, and symmetric.
   1. Reflexive: this would say that $x\sim x \iff x^2-4x = x^2-4x$, which is true.
   2. Transitive: suppose $x\sim y$ and $y\sim z$, we want to show $x\sim z$. But we have
   $$
   x^2 - 4x = y^2-4y ~\&~ y^2-4y = z^2-4z \implies x^2-4x = y^2-4y = z^2-4z
   $$
   1. We want to show $x\sim y \implies y \sim x$, which follows because $x^2-4x = y^2-4y \iff y^2-4y = x^2-4x$.
   2. The equivalence classes:
   $$\begin{align*}
   x^2-4x &=  0:  &\theset{0, 4}\\
   x^2-4x &= -3: &\theset{1,3} \\
   x^2-4x &= -4: &\theset{2} \\
   x^2-4x &=  5: &\theset{5}
   \end{align*}$$

2. $f: \RR^2 \to \RR$ is totally differentiable at $\vector x$ iff there is an open neighborhood of $\vector 0$ containing $\vector h$ and there exists a linear map $D: \RR^2 \to \RR$ such that $f(\vector x + \vector h)- f(\vector x) = \vector D(\vector x)\vector h + \vector \varepsilon(\vector h)$, where $\frac{\varepsilon(\vector h)}{\norm{\vector h}} \to 0$.

    Equivalently, it is when the following limit exists and satisfies
    $$
    \lim_{\vector h \to \vector 0} \abs{\frac{(f(\vector x + \vector h) - f(\vector x)) - D(\vector x)\vector h}{\norm{\vector h}}} = 0,
    $$

    in which case we denote $f'(\vector x) = D(\vector x)$.

    Substituting in $\vector x = \vector 0$, this says that 
    $$
    f(\vector h) - f(\vector 0) = D(\vector 0)\vector h + \varepsilon(h), \text{ or } \\
    \lim_{\vector h \to \vector 0} \abs{\frac{f(\vector h) - f(\vector 0) - D(\vector 0)\vector h}{\norm{\vector h}}} = 0.
    $$

    or more explicitly,
    $$
    \lim_{\thevector{h_1, h_2} \to 0} \abs{\frac{f(h_1, h_2) - f(0, 0) -D(0, 0)\thevector{h_1, h_2}^T}{\norm{\thevector{h_1, h_2}}}}
    $$

    We'll use the following definition:
    $$
    f_x(x, y) = \lim_{t \to 0} \frac{f(x + t, y) - f(x, y)}{t} \implies t  f_x(x, y) = f(x + t, y) - f(x, y) \\
    f_y(x, y) = \lim_{t \to 0} \frac{f(x, y + t) - f(x, y)}{t} \implies t 
    f_y(x, y) = f(x, y + t) - f(x, y)
    $$

    These are the partial derivatives of $f$; by assumption $f_x$ exists and $f_y \in C^1$. 

    We'll also use the MVT:
    $$
    f_x \text{ exists and continuous on } (0, t) 
    \implies \exists A \in [0, t] \suchthat \\ 
    f(t, y) - f(0, y) = f_x(A, y)(t - 0) = tf_x(A, y) \\
    \implies \color{purple} f(t, y) - f(0, y) = tf_x(A, y)
    $$
    where moreover $\lim_{t\to 0} f_x(A, y) = f_x(\lim_{t\to 0} A, y) = f(0, y)$ since $f_x$ is continuous.

    We'll also use the "best linear approximation" definition of differentiability:
    $$
    f_y \text{ exists on } (0, t) \\ 
    \implies f(x, t) - f(x, 0) = f_y(x, 0)(t - 0) + o(t) \\
    \implies \color{green}f(x, t) - f(x, 0) = tf_y(x,0) + o(t).
    $$

    where $t\inv o(t)\to 0$.

    So define 
    $$D(0, 0) =\thevector{f_x(0, 0), f_y(0, 0)};$$ 
    
    which is well-defined only because both partial derivatives *exist* at $(0,0)$. We will show that this operator satisfies the necessary properties for $f$ to be differentiable with derivative $f' = D$.

    This follows because
    $$\begin{align*}
    f(h_1, h_2) - f(0, 0) - D(0, 0)\thevector{h_1, h_2}^T 
    &= f(h_1, h_2) - f(0, 0) - h_1 f_x(0, 0) -h_2 f_y(0,0) \\ \\
    &= {\color{green}f(h_1, h_2) - f(h_1, 0)} + {\color{purple}f(h_1, 0) - f(0, 0)} - h_1 f_x(0, 0) -h_2 f_y(0,0) \\ \\
    &= {\color{green}h_2f_y(h_1, 0) + o(h_2)} + {\color{purple}h_1f_x(A, 0)}- h_1 f_x(0, 0) -h_2 f_y(0,0) \\ 
    & \quad \quad \textit{for some } A \in [0, h_1]\textit{ by MVT }\\ \\
    &= h_1({\color{purple}f_x(A,0)} - f_x(0, 0)) + h_2( {\color{green}f_y(h_1, 0) + o(h_2)} - f_y(0, 0))
    \end{align*}$$

    We are now in a position where we can take $\lim_{(h_1, h_2) \to \vector 0}$. By the continuity of $f_x$, as $h_1 \to 0$ we have $h_1 \abs{({\color{purple}f_x(A,0)} - f_x(0, 0))} \to 0$.

    We can then write
    $$\begin{align*}
    f(h_1, h_2) - f(0, 0)
    &= {\color{green}f(h_1, h_2) - f(h_1, 0)} + {\color{purple}f(h_1, 0) - f(0, 0)} \\
    &= {\color{green}h_2f_y(h_1, 0) + o(h_2)} + {\color{purple}h_1f_x(A, 0)} \\
    &= asdsa
    \end{align*}$$
