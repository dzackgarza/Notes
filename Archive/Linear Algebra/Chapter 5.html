<!DOCTYPE html><html><head>
      <title>Chapter 5</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      
        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({"extensions":["tex2jax.js"],"jax":["input/TeX","output/HTML-CSS"],"messageStyle":"none","tex2jax":{"displayMath":[["$$","$$"],["\\[","\\]"]],"inlineMath":[["$","$"],["\\(","\\)"]],"processEscapes":true,"processEnvironments":false},"TeX":{"extensions":["noUndefined.js","autoload-all.js","AMSmath.js","AMSsymbols.js"],"equationNumbers":{"autoNumber":"AMS"},"Macros":{"NN":"{\\mathbb{N}}","RR":"{\\mathbb{R}}","ZZ":"{\\mathbb{Z}}","CC":"{\\mathbb{C}}","QQ":"{\\mathbb{Q}}","RP":"{\\mathbb{RP}}","CP":"{\\mathbb{CP}}","HP":"{\\mathbb{HP}}","OP":"{\\mathbb{OP}}","FF":"{\\mathbb{F}}","PP":"{\\mathbb{P}}","AA":"{\\mathbb{A}}","MM":"{\\mathbb{M}}","TT":"{\\mathbb{T}}","SS":"{\\mathbb{S}}","KK":"{\\mathbb{K}}","Gr":"{\\text{Gr}}","GL":"{\\text{GL}}","SL":"{\\text{SL}}","char":"{\\mathbb{1}}","vector":["{\\mathbf{ {#1} }}",1],"abs":["{\\left\\lvert #2 \\right\\rvert_{\\text{#1}}}",2,""],"Aut":"{\\text{Aut}}","del":"{\\partial}","too":["{\\xrightarrow{#1}}",1,""],"im":"{\\text{im}~}","homotopic":"\\simeq","into":"\\to","theset":["\\{{#1}\\}",1],"norm":["{{\\lVert}{#1}{\\rVert}}",1],"cross":"\\times","definedas":"\\mathrel{\\vcenter{:}}=","surjects":"\\twoheadrightarrow","onto":"\\twoheadrightarrow","injects":"\\hookrightarrow","id":"\\text{id}","restrictionof":["{\\left.{#1}\\right|_{#2}}",2],"intersect":"\\bigcap","union":"\\bigcup","mapsvia":["{\\xrightarrow{#1}}",1],"coker":"\\operatorname{coker}","rank":"\\operatorname{rank}","tensor":"\\otimes","semidirect":"\\rtimes","pt":"\\{\\text{pt}\\}","and":"{\\text{ and }}","or":"{\\text{ or }}","bd":"{\\del}","wait":"{\\,\\cdot\\,}","selfmap":"{\\circlearrowleft}","tor":"{\\text{Tor}}","ext":"{\\text{Ext}}","hom":"{\\text{Hom}}","actson":"{\\curvearrowright}","actsonl":"{\\curvearrowleft}","disjoint":"{\\coprod}","qed":"{\\tag*{$\\blacksquare$}}","endef":"{\\tag*{$\\triangleleft$}}","dash":"{\\hbox{-}}","bigast":"{\\mathop{\\Large \\ast}}","from":"{\\leftarrow}","covers":"{\\twoheadrightarrow_p}","inner":["{\\langle {#1}, {#2} \\rangle}",2],"indicator":["{\\unicode{x1D7D9}\\left[#1\\right]}",1],"equalsbecause":["{\\stackrel{\\mbox{$\\tiny{\\text{ #1 }}$}}{=}}",1],"conjugate":["{\\overline{{#1}}}",1],"strike":["{\\enclose{horizontalstrike}{#1}}",1],"realpart":["{\\mathcal{Re}({#1})}",1],"imaginarypart":["{\\mathcal{Im}({#1})}",1],"dd":["{\\frac{\\partial #1}{\\partial #2}}",2],"normal":"{~\\trianglelefteq~}","rotate":["{\\style{display: inline-block; transform: rotate(#1deg)}{#2}}",2,"90"],"stirling":["\\genfrac\\{\\}{0pt}{}{#1}{#2}",2],"thevector":["{\\left[ {#1} \\right]}",1],"th":["{\\text{th}}",0],"spec":["{\\mathrm{Spec}}",0],"suchthat":["{~\\backepsilon ~}",0],"divides":["{~\\mid ~}",0]}},"HTML-CSS":{"availableFonts":["TeX"]}});
        </script>
        <script type="text/javascript" async src="file:////home/zack/.vscode-oss/extensions/shd101wyy.markdown-preview-enhanced-0.3.11/node_modules/@shd101wyy/mume/dependencies/mathjax/MathJax.js" charset="UTF-8"></script>
        
      
      
      
      
      
      
      
      
      

      <style> 
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview .pagebreak,.markdown-preview .newpage{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center !important}.markdown-preview:not([for="preview"]) .code-chunk .btn-group{display:none}.markdown-preview:not([for="preview"]) .code-chunk .status{display:none}.markdown-preview:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0}@media screen and (min-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px)}}@media screen and (max-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{font-size:14px !important;padding:1em}}@media print{html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,0.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{padding:0 1.6em;margin-top:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc li{margin-bottom:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{list-style-type:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% -  300px);padding:2em calc(50% - 457px -  150px);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */
.markdown-preview.markdown-preview {
  /*.reveal .slides {*/
  /*position: absolute;*/
  /*top: 0;*/
  /*left: 0;*/
  /*// overflow-y: auto !important;*/
  /*}*/
}
 
      </style>
    </head>
    <body for="html-export">
      <div class="mume markdown-preview   ">
      <h1 class="mume-header" id="chapter-5-orthogonality">Chapter 5: Orthogonality</h1>

<h2 class="mume-header" id="inner-products-and-norms">Inner Products and Norms</h2>

<p>The point of this chapter is to show how an inner product can induce a notion of &#x201C;angle&#x201D;, which agrees with our intuition in Euclidean spaces such as <span class="mathjax-exps">$\RR^n$</span>, but can be extended to much less intuitive things, like spaces of functions.</p>
<p>Given an inner product<br>
</p><div class="mathjax-exps">$$\inner{\wait}{\wait}: \RR^n \cross \RR^n \to \RR,$$</div><p></p>
<p>we can define a <strong>norm</strong><br>
</p><div class="mathjax-exps">$$\norm {\vector x} = \sqrt{\inner{\vector x}{\vector x}},$$</div><p></p>
<p>which has the often useful alternative formulation</p>
<p></p><div class="mathjax-exps">$$\inner{\vector x}{\vector x} = \norm{\vector x}^2$$</div><p></p>
<p>We can define a notion of angle:<br>
</p><div class="mathjax-exps">$$\inner{\vector x}{\vector y} = \norm{\vector x} \norm{\vector y} \cos\theta_{x,y} \implies \cos \theta_{x,y} \definedas \frac{\inner{\vector x}{\vector y}}{\norm{\vector x} \norm{\vector y}} = \inner{\hat{\vector x}}{\hat{\vector y}}$$</div><p></p>
<p>where <span class="mathjax-exps">$\theta_{x,y}$</span> denotes the angle between the vectors <span class="mathjax-exps">$\vector x$</span> and <span class="mathjax-exps">$\vector y$</span>.</p>
<p>Since <span class="mathjax-exps">$\cos \theta=0$</span> exactly when <span class="mathjax-exps">$\theta = \pm \frac \pi 2$</span>, we can can declare two vectors to be <strong>orthogonal</strong> exactly in this case:<br>
</p><div class="mathjax-exps">$$\vector x \perp \vector y \iff \inner{\vector x}{\vector y} = 0.$$</div><p></p>
<p>Note that this makes the zero vector orthogonal to everything.</p>
<p>Given a notion of orthogonality for vectors, we can extend this to matrices. A square matrix is said to be orthogonal iff <span class="mathjax-exps">$QQ^T = Q^TQ = I$</span>. For rectangular matrices, we have the following characterizations:<br>
</p><div class="mathjax-exps">$$\begin{align*} QQ^T = I \implies &amp;\text{The columns of } Q \text { are orthogonal,} \\ Q^TQ = I \implies &amp;\text{The rows of } Q \text{ are orthogonal.} \end{align*}$$</div><p></p>
<p>Another useful formula is<br>
</p><div class="mathjax-exps">$$\norm{\vector x + \vector y}^2 = \norm{\vector x}^2 + 2\inner{\vector x}{\vector y} + \norm{\vector y}^2,$$</div><p></p>
<p>and so when <span class="mathjax-exps">$\vector x \perp \vector y$</span>, we have <span class="mathjax-exps">$\inner{\vector x}{\vector y} = 0$</span>. So the norm commutes with squaring, yielding<br>
</p><div class="mathjax-exps">$$\norm{\vector x + \vector y}^2 = \norm{\vector x}^2 + \norm{\vector y}^2$$</div><p></p>
<h2 class="mume-header" id="projections">Projections</h2>

<p>With an inner product in hand and a notion of orthogonality, we can define a notion of <strong>orthogonal projection</strong> of one vector onto another, and more generally of a vector onto a subspace spanned by multiple vectors.</p>
<p><strong>Projection Onto a Vector</strong><br>
Say we have two vectors <span class="mathjax-exps">$\vector x$</span> and <span class="mathjax-exps">$\vector y$</span>, and we want to define &quot;the component of <span class="mathjax-exps">$\vector x$</span> that lies along <span class="mathjax-exps">$\vector y$</span>&quot;, which we&#x2019;ll call <span class="mathjax-exps">$\vector p$</span>. We can work out what the formula should be using a simple model:</p>
<p><strong>Figure Here</strong></p>
<p>We notice that whatever <span class="mathjax-exps">$p$</span> is, it will in the direction of <span class="mathjax-exps">$\vector y$</span>, and thus <span class="mathjax-exps">$\vector p = \lambda \hat {\vector y}$</span> for some scalar <span class="mathjax-exps">$\lambda$</span>, where in fact <span class="mathjax-exps">$\lambda = \norm {\vector p}$</span> since <span class="mathjax-exps">$\norm{\hat{\vector y}} = 1$</span>. We will find that \lambda = <span class="mathjax-exps">$\inner{\vector x}{\hat{\vector y}}$</span>, and so<br>
</p><div class="mathjax-exps">$$\vector p = \inner{\vector x}{\hat{\vector y}}\hat{\vector y} = \frac{\inner{\vector x}{\vector y}}{\inner{\vector y}{\vector y}} \vector y$$</div><p></p>
<p>Notice that we can then form a &#x201C;residual&#x201D; vector <span class="mathjax-exps">$\vector r = \vector x - \vector p$</span>, which should satisfy <span class="mathjax-exps">$\vector r \perp \vector p$</span>. If we were to let <span class="mathjax-exps">$\lambda$</span> vary as a function of a parameter <span class="mathjax-exps">$t$</span> (making <span class="mathjax-exps">$\vector r$</span> a function of <span class="mathjax-exps">$t$</span> as well) we would find that this particular choice minimizes <span class="mathjax-exps">$\norm{\vector r (t)}$</span>.</p>
<p><strong>Projection Onto a Subspace</strong><br>
In general, supposing one has a subspace <span class="mathjax-exps">$S = \mathrm{span}\theset{\vector y_1, \vector y_2, \cdots, \vector y_n}$</span> and (importantly!) the <span class="mathjax-exps">$\vector y_i$</span> are orthogonal, then the projection of <span class="mathjax-exps">$\vector p$</span> of <span class="mathjax-exps">$x$</span> onto <span class="mathjax-exps">$S$</span> is given by the sum of the projections onto each basis vector, yielding<br>
</p><div class="mathjax-exps">$$\begin{equation}\label{projection_equation} \vector p = \sum_{i=1}^n \frac{\inner{\vector x}{\vector y_i}}{\inner{\vector y_i}{\vector y_i}} \vector y_i = \sum_{i=1}^n \inner{\vector x}{\vector y_i} \hat{\vector y_i}\end{equation}.$$</div><p></p>
<blockquote>
<p>Note: this is part of why having an orthogonal basis is desirable!</p>
</blockquote>
<p>Letting <span class="mathjax-exps">$A = [\vector y_1, \vector y_2, \cdots]$</span>, then the following matrix projects vectors onto <span class="mathjax-exps">$S$</span>, expressing them in terms of the basis <span class="mathjax-exps">$\vector y_i$</span><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>:<br>
</p><div class="mathjax-exps">$$\tilde P_A = (AA^T)^{-1}A^T,$$</div><p></p>
<p>while this matrix performs the projection and expresses it in terms of the standard basis:<br>
</p><div class="mathjax-exps">$$P_A = A(AA^T)^{-1}A^T.$$</div><p></p>
<h2 class="mume-header" id="subspaces">Subspaces</h2>

<p>Equation of a plane: given a point <span class="mathjax-exps">$\vector p_0$</span> on a plane and a normal vector <span class="mathjax-exps">$\vector n$</span>, any vector <span class="mathjax-exps">$\vector x$</span> on the plane satisfies<br>
</p><div class="mathjax-exps">$$\inner{\vector x - \vector p_0}{\vector n} = 0$$</div><p></p>
<p>To find the distance between a point <span class="mathjax-exps">$\vector a$</span> and a plane, we need only project <span class="mathjax-exps">$\vector a$</span> onto the subspace spanned by the normal <span class="mathjax-exps">$\vector n$</span>:<br>
</p><div class="mathjax-exps">$$d = \inner{\vector a}{\vector n}$$</div><p></p>
<p>Given a subspace <span class="mathjax-exps">$S \subseteq V$</span>, we define its <strong>orthogonal complement</strong><br>
</p><div class="mathjax-exps">$$S^\perp = \theset{\vector v\in V \suchthat \forall \vector s\in S,~ \inner{\vector v}{\vector s} = 0}.$$</div><p></p>
<p>Given two subspace <span class="mathjax-exps">$S_1$</span> and <span class="mathjax-exps">$S_2$</span> where <span class="mathjax-exps">$S_1^\perp = S_2$</span>, we occasionally write <span class="mathjax-exps">$S_1 \perp S_2$</span>.</p>
<p>An important property is that any choice of subspace <span class="mathjax-exps">$S\subseteq V$</span> yields a decomposition <span class="mathjax-exps">$V = S \oplus S^\perp$</span>.</p>
<p>One important property of projections is that for any vector <span class="mathjax-exps">$\vector v$</span> and for any subspace <span class="mathjax-exps">$S$</span>, we have <span class="mathjax-exps">$\vector v - P_S(\vector v) \in S^\perp$</span>. Moreover, if <span class="mathjax-exps">$\vector v \in S^\perp$</span>, then <span class="mathjax-exps">$P_s(\vector v)$</span> must be zero. This follows by noting that in equation <span class="mathjax-exps">$\ref{projection_equation}$</span>, every inner product appearing in the sum vanishes, by definition of <span class="mathjax-exps">$\vector v \in S^\perp$</span>, and so the projection is zero.</p>
<p><strong>The Fundamental Subspaces Theorem</strong></p>
<p>Given a matrix <span class="mathjax-exps">$A \in \mathrm{Mat}(m, n)$</span>, and noting that<br>
</p><div class="mathjax-exps">$$\begin{align*} A &amp;: \RR^n \to \RR^m,\\ A^T &amp;:  \RR^m \to \RR^n \end{align*}$$</div><p></p>
<p>We have the following decompositions:<br>
</p><div class="mathjax-exps">$$\begin{align*} &amp;\RR^n &amp;\cong  \ker A &amp;\oplus \im A^T &amp;\cong \mathrm{nullspace}(A) &amp;\oplus~ \mathrm{colspace}(A^T) \\ &amp;\RR^m &amp;\cong  \im A &amp;\oplus \ker A^T &amp;\cong \mathrm{colspace}(A) &amp;\oplus~ \mathrm{nullspace}(A^T) \end{align*}$$</div><p></p>
<h2 class="mume-header" id="least-squares">Least Squares</h2>

<p>Figure here</p>
<p>Figure here</p>
<p>The general setup here is that we would like to solve <span class="mathjax-exps">$A\vector x = \vector b$</span> for <span class="mathjax-exps">$\vector x$</span>, where <span class="mathjax-exps">$\vector b$</span> is not in fact in the range of <span class="mathjax-exps">$A$</span>. We thus settle for a unique &quot;best&quot; solution <span class="mathjax-exps">$\tilde{\vector x}$</span> such that the error <span class="mathjax-exps">$\norm{A\tilde{\vector x} - \vector b}$</span> is minimized.</p>
<p>Geometrically, the solution is given by projecting <span class="mathjax-exps">$\vector b$</span> onto the column space of <span class="mathjax-exps">$A$</span>. To see why this is the case, define the residual vector <span class="mathjax-exps">$\vector r = A\tilde{\vector x} - \vector b$</span>. We then seek to minimize <span class="mathjax-exps">$\norm{\vector r}$</span>, which happens exactly when <span class="mathjax-exps">$\vector r \perp \im A$</span>. But this happens exactly when <span class="mathjax-exps">$\vector r \in (\im A)^\perp$</span>, which by the fundamental subspaces theorem, is equivalent to <span class="mathjax-exps">$\vector r \in \ker A^T$</span>.</p>
<p>From this, we get the equation<br>
</p><div class="mathjax-exps">$$A^T \vector r = \vector 0 \\ \implies A^T(A \tilde{\vector x} - \vector b) = \vector 0\\ \implies A^TA\tilde{\vector x} = A^T \vector b,$$</div><p></p>
<p>where the last line is described as the <strong>normal equations</strong>.</p>
<p>If <span class="mathjax-exps">$A$</span> is an <span class="mathjax-exps">$m\times n$</span> matrix and is of full rank, so it has <span class="mathjax-exps">$n$</span> linearly independent columns, then one can show that <span class="mathjax-exps">$A^T A$</span> is nonsingular, and we thus arrive at the least-squares solution<br>
</p><div class="mathjax-exps">$$\tilde{\vector x} = (A^TA)^{-1}A^T \vector b \qed$$</div><p></p>
<p>These equations can also be derived explicitly using Calculus applied to matrices, vectors, and inner products. This requires the use of the following formulas:<br>
</p><div class="mathjax-exps">$$\begin{align*} \dd{}{\vector x} \inner{\vector x}{\vector a} &amp;= \vector a \\ \dd{}{\vector x} \inner{\vector x}{\vector A\vector x} &amp;= (A+A^T)\vector x \end{align*}$$</div><p></p>
<p>as well as the adjoint formula<br>
</p><div class="mathjax-exps">$$\inner{A\vector x}{\vector x} = \inner{\vector x}{A^T \vector x}.$$</div><p></p>
<p>From these, by letting <span class="mathjax-exps">$A=I$</span> we can derive<br>
</p><div class="mathjax-exps">$$\dd{}{\vector x} \norm{\vector x}^2 = \dd{}{\vector x} \inner{\vector x}{\vector x} = 2\vector x\\$$</div><p></p>
<p>The derivation proceeds by solving the equation<br>
</p><div class="mathjax-exps">$$\dd{}{\vector x} \norm{\vector b - A\vector x}^2 = \vector 0.$$</div><p></p>
<h2 class="mume-header" id="gram-schmidt">Gram-Schmidt</h2>

<p>The general setup here is that we are given an orthogonal basis <span class="mathjax-exps">$\theset{\vector x_i}_{i=1}^n$</span> and we want to produce an <strong>orthonormal</strong> basis from them.</p>
<p>Why would we want such a thing? Recall that we often wanted to change from the standard basis <span class="mathjax-exps">$\mathcal{E}$</span> to some different basis <span class="mathjax-exps">$\mathcal{B} = \theset{\vector b_1, \vector b_2, \cdots}$</span>. We could form the change of basis matrix <span class="mathjax-exps">$B = [\vector b_1, \vector b_2, \cdots]$</span> acts on vectors in the <span class="mathjax-exps">$\mathcal{B}$</span> basis according to<br>
</p><div class="mathjax-exps">$$B[\vector x]_\mathcal{B} = [\vector x]_{\mathcal{E}}$$</div><p></p>
<p>But to change from <span class="mathjax-exps">$\mathcal{E}$</span> to <span class="mathjax-exps">$\mathcal{B}$</span> requires computing <span class="mathjax-exps">$B^{-1}$</span>, which acts on vectors in the standard basis according to<br>
</p><div class="mathjax-exps">$$B^{-1}[\vector x]_\mathcal{E} = [\vector x]_{\mathcal{B}}$$</div><p></p>
<p>If, on the other hand, the <span class="mathjax-exps">$\vector b_i$</span> are orthonormal, then <span class="mathjax-exps">$B{-1} = B^T$</span>, which is much easier to compute. We also obtain a rather simple formula for the coordinates of <span class="mathjax-exps">$\vector x$</span> with respect to <span class="mathjax-exps">$\mathcal B$</span>. This follows because we can write<br>
</p><div class="mathjax-exps">$$\vector x = \sum_{i=1}^n \inner{\vector x}{\vector b_i} \vector b_i \definedas \sum_{i=1}^n c_i \vector b_i,$$</div><p></p>
<p>and we find that<br>
</p><div class="mathjax-exps">$$[\vector x]_\mathcal{B} = \vector c \definedas [c_1, c_2, \cdots, c_n]^T.$$</div><p></p>
<p>This also allows us to simplify projection matrices. Supposing that <span class="mathjax-exps">$A$</span> has orthonormal columns and letting <span class="mathjax-exps">$S$</span> be the column space of <span class="mathjax-exps">$A$</span>, recall that the projection onto <span class="mathjax-exps">$S$</span> is defined by<br>
</p><div class="mathjax-exps">$$P_S = Q(Q^TQ)^{-1}Q^T.$$</div><p></p>
<p>Since <span class="mathjax-exps">$Q$</span> has orthogonal columns and satisfies <span class="mathjax-exps">$QQ^T = I$</span>, this simplifies to<br>
</p><div class="mathjax-exps">$$P_S = QQ^T.$$</div><p></p>
<h3 class="mume-header" id="the-algorithm">The Algorithm</h3>

<p>Given the orthogonal basis <span class="mathjax-exps">$\theset{\vector x_i}$</span>, we form an orthonormal basis <span class="mathjax-exps">$\theset{\vector u_i}$</span> iteratively as follows.</p>
<p>First define<br>
</p><div class="mathjax-exps">$$\begin{align*} N: \RR^n &amp;\to S^{n-1} \\ \vector x &amp;\mapsto \hat{\vector x} \definedas \frac {\vector x} {\norm{\vector  x}} \end{align*}$$</div><p></p>
<p>which projects a vector onto the unit sphere in <span class="mathjax-exps">$\RR^n$</span> by normalizing. Then,</p>
<p></p><div class="mathjax-exps">$$\begin{align*} \vector u_1 &amp;= N(\vector{x_1}) \\ \vector u_2 &amp;= N(\vector x_2 - \inner{\vector x_2}{\vector u_1}\vector u_1)\\ \vector u_3 &amp;= N(\vector x_3 - \inner{\vector x_3}{\vector u_1}\vector u_1 - \inner{\vector x_3}{\vector u_2}\vector u_2 ) \\ \vdots &amp; \qquad \vdots \\ \vector u_k &amp;= N(\vector x_k - \sum_{i=1}^{k-1} \inner{\vector x_k}{\vector u_i}\vector u_i) \end{align*}$$</div><p></p>
<p>In words, at each stage, we take one of the original vectors <span class="mathjax-exps">$\vector x_i$</span>, then subtract off its projections onto all of the <span class="mathjax-exps">$\vector u_i$</span> we&#x2019;ve created up until that point. This leaves us with only the component of <span class="mathjax-exps">$\vector x_i$</span> that is orthogonal to the span of the previous <span class="mathjax-exps">$\vector u_i$</span> we already have, and we then normalize each <span class="mathjax-exps">$\vector u_i$</span> we obtain this way.</p>
<h3 class="mume-header" id="the-qr-decomposition">The QR Decomposition</h3>

<p>Gram-Schmidt is often computed to find an orthonormal basis for, say, the range of some matrix <span class="mathjax-exps">$A$</span>. With a small modification to this algorithm, we can write <span class="mathjax-exps">$A = QR$</span> where <span class="mathjax-exps">$R$</span> is upper triangular and <span class="mathjax-exps">$Q$</span> has orthogonal columns.</p>
<p>Why is this useful? One reason is that this also allows for a particularly simple expression of least-squares solutions. If <span class="mathjax-exps">$A=QR$</span>, then <span class="mathjax-exps">$R$</span> will be invertible, and a bit of algebraic manipulation will show that<br>
</p><div class="mathjax-exps">$$\tilde{\vector x} = R^{-1}Q^T\vector b.$$</div><p></p>
<p>How does it work? You simply perform Gram-Schmidt to obtain <span class="mathjax-exps">$\theset{\vector u_i}$</span>, then </p><div class="mathjax-exps">$$Q = [\vector u_1, \vector u_2, \cdots ].$$</div><p></p>
<p>The matrix <span class="mathjax-exps">$R$</span> can then be written as</p>
<p></p><div class="mathjax-exps">$$r_{ij} = \begin{cases} \inner{\vector u_i}{\vector x_j}, &amp; i\leq j, \\ 0, &amp; \text{else}. \end{cases}$$</div><p></p>
<p>Explicitly, this yields the matrix<br>
</p><div class="mathjax-exps">$$R =  \begin{bmatrix} \inner{\vector u_1}{\vector x_1} &amp; \inner{\vector u_1}{\vector x_2} &amp; \inner{\vector u_1}{\vector x_3} &amp; \cdots &amp; \\ 0 &amp; \inner{\vector u_2}{\vector x_2} &amp; \inner{\vector u_2}{\vector x_3} &amp; \cdots &amp; \\ 0 &amp; 0 &amp; \inner{\vector u_3}{\vector x_3} &amp; \cdots &amp; \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots \\ \end{bmatrix}$$</div><p></p>
<p>Todo: explain shortcut for diagonals.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>For a derivation of this formula, see the section on least-squares approximations. <a href="#fnref1" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
</ol>
</section>

      </div>
      <div class="md-sidebar-toc"><ul>
<li><a href="#chapter-5-orthogonality">Chapter 5: Orthogonality</a>
<ul>
<li><a href="#inner-products-and-norms">Inner Products and Norms</a></li>
<li><a href="#projections">Projections</a></li>
<li><a href="#subspaces">Subspaces</a></li>
<li><a href="#least-squares">Least Squares</a></li>
<li><a href="#gram-schmidt">Gram-Schmidt</a>
<ul>
<li><a href="#the-algorithm">The Algorithm</a></li>
<li><a href="#the-qr-decomposition">The QR Decomposition</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
      <a id="sidebar-toc-btn">&#x2261;</a>
    
    
    
    
    
    
    
    
<script>

var sidebarTOCBtn = document.getElementById('sidebar-toc-btn')
sidebarTOCBtn.addEventListener('click', function(event) {
  event.stopPropagation()
  if (document.body.hasAttribute('html-show-sidebar-toc')) {
    document.body.removeAttribute('html-show-sidebar-toc')
  } else {
    document.body.setAttribute('html-show-sidebar-toc', true)
  }
})
</script>
      
  
    </body></html>